[{"title":"Hello World","path":"/2023/01/06/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","tags":["年后","法法","发顺丰三"],"categories":["新文章"]},{"path":"/intro/index.html","content":"冬夜读书示子聿陆游古人学问无遗力，少壮工夫老始成。纸上得来终觉浅，绝知此事要躬行。2023年1月1日随想录分类卷标签集归档册言堂序 关于本站关于博主站点说明你好，欢迎参观我的学习博客“学圃堂” ，也许你已经发现了这个博客非常简洁只记录了我科研学习的笔记，这也是我的博客名称由来，因此我建立这个网站的初衷就是存放学习笔记、疑难点以及一些小技巧，这也就意味着这个博客更新面向我自己，文章更多的是用我自己看得懂的方式书写，如果你也对内容感兴趣并且在阅读过程中有任何疑惑欢迎在下方进行留言探讨！当然如果你想更加深入的了解我还请前往我的主站!来自于七里台男子职业技术学院的软件工程专业在读大四学生😆 热爱cg的科研小白，立志于成为冉冉升起的科研新星😂 在逃社恐人员，害怕工作的家里蹲一枚😨 学习能力堪忧，除了八股文啥也不会，未能保研的菜菜🥺 正在为研究生学位焦虑奋斗的考研人(等待来自于2.21的审判版)😳本站使用百度统计记录访问者的ip、浏览页面、浏览时间等信息。 本站部分资源来源网络，如因传播、转载、商用等导致纠纷，与本站无关； 本站文章引用或转载会严格遵循转载协议，尊重原作者的创作成果，如发现侵权等问题，及时联系处理； 本站提供非实名制评论功能，站长有权在未告知用户的前提下，修改、删除任何评论内容，所以请和谐留言； 本站非特别注明，均采用 CC BY-NC-SA 4.0 许可协议，转载请注明出处。 最终解释权归本站所有。"},{"title":"科研学习","path":"/notes/index.html","content":"技术加油站 W3schoolhttps://www.w3school.com.cn/C语言中文网http://c.biancheng.netcs-Noteshttp://www.cyc2018.xyzRoad2Codinghttps://r2coding.com/OI WIKIhttps://oi-wiki.org/廖雪峰的技术网站https://www.liaoxuefeng.com/Quick Referencehttps://quickref.cn/图形学与混合现实平台https://games-cn.org/git bookhttps://git-scm.com/book/zh/v2leetcode bookhttps://books.halfrost.com/leetcode/GAMES101课件https://sites.cs.ucsb.edu/~lingqi/teaching/games101.html南瓜书https://datawhalechina.github.io/pumpkin-book/ 闭关修炼室 LeetCodehttps://leetcode.cn/?utm_source=LCUS&utm_medium=ip_redirect&utm_campaign=transfer2china拼题Ahttps://pintia.cn/homeCodeForceshttps://codeforces.com/KuangBIN刷题https://vjudge.net/article/371 情报收集处 中国知网https://www.cnki.net/ARXIVhttps://arxiv.org/CHATGPThttps://chat.openai.com/chatStackOverFlowhttps://stackoverflow.com/ 秘传功法阁 搬书匠http://www.banshujiang.cn/鸠摩搜书https://www.jiumodiary.com/谷粉想学术http://chongbuluo.glgooo.top/muchong.html艾思科蓝https://www.ais.cn/ 上古神器室 CNKI学术翻译https://dict.cnki.net/index有道文档翻译https://pdf.youdao.com/?src=fanyiwebHighCharts演示图https://www.hcharts.cn/demo/highchartsLatex公式编辑器https://www.latexlive.com/Slager Latex模板https://www.slager.link/#/home妙写论文排版https://www.miaowrite.com/latex表格生成器https://www.tablesgenerator.com/亿图图示https://www.edrawmax.cn/Latex公式转图片https://latex.vimsky.com/论文查重http://www.papertime.cn/矩阵计算器https://zh.numberempire.com/matrixcalculator.php3D计算器https://www.geogebra.org/calculator"},{"title":"建站必备","path":"/notes/建站必备/index.html","content":"云数据存储 七牛云KODO存储https://www.qiniu.com/products/kodo阿里云ecshttps://www.aliyun.com/product/ecs?spm=5176.19720258.J_3207526240.33.542176f4vcfDVPvercelhttps://vercel.com/leancloudhttps://leancloud.app/ 网站自维护 百度流量统计https://tongji.baidu.com/web5/welcome/login域名解析https://cloud.tencent.com/product/cns网站测速https://tool.chinaz.com/speedtest/github.com"},{"title":"处理工具","path":"/notes/处理工具/index.html","content":"在线文件处理 ConvertIO文件转换器https://convertio.co/zh/aConvert图片转换https://www.aconvert.com/cn/image/webp-to-gif/RemoveBG抠图https://www.remove.bg/zh?from=thosefree.com改图宝压缩裁剪https://www.gaitubao.com/#图片生成pdfhttps://smallpdf.com/cn/jpg-to-pdftinypnghttps://tinypng.com/ 设计素材模板 Canvas可画https://www.canva.cn/IconFonthttps://www.iconfont.cn/fontAwesomehttps://fontawesomelib.cn/FlatUIColorhttps://flatuicolors.com/ 代码文档共享 代码高亮https://tool.oschina.net/highlight代码生成图片https://carbon.now.sh/代码分享https://paste.org.cn/金山文档https://www.kdocs.cn/latest 有用但没大用 emoji文档https://www.emojiall.com/zh-hans在线画板https://www.suxieban.com/index.html美寄词云https://www.moage.cn/wordartMonkeyTypehttps://monkeytype.com/login键帽设计https://www.zfrontier.com/mykb/editor缩略图设计http://xsdggw.cn/t/tool/wylst/mockuphonehttps://mockuphone.com/device?type=ios#iphone12"},{"title":"这是分页标题","path":"/wiki/GAMES191笔记/index.html","content":"fff ggg"},{"title":"什么是操作系统","path":"/wiki/操作系统笔记/什么是操作系统/index.html","content":"操作系统的概念功能和目标 操作系统的结构分布 OS系统就是连接用户和应用程序与硬件之间的中间载体，他来实现两层结构间的数据交换，因此一个程序运行一定要在操作系统上才可以运行。 操作系统的定义 操作系统（Operating System,OS)是指控制和管理整个计算机系统的硬件和软件资源，并合理地组织调度计算机的工作和资源的分配，以提供给用户和其他软件方便的接口和环境。它是计算机系统中最基本的系统软件。 从上面的定义我们可以看出操作系统的几个根本功能： 操作系统是系统资源的管理者，任务是分配资源 为上层应用提供易用的服务和环境 是最接近硬件的一层软件（因此还是由算法编程实现的） 任务管理器中，左侧是对应的用户程序，而右侧就是硬件的信息。 思考：内存的作用？ 执行一个程序前需要先将该程序（包裹执行代码，数据块）等放入内存中才可以在CPU上面执行。 以打开QQ和朋友聊天为例： 在各个文件夹中找到QQ安装的位置 第一步是找到程序的存放位置，当然一般上左面的快捷方式直接映射到了执行程序的存放位置 双击打开QQ.exe 将程序的执行代码和数据等放入内存中，做好准备后等待CPU执行此程序 QQ程序开始运行 执行程序开始在CPU上被执行 开始和朋友视频聊天 需要将摄像头设备分配给QQ进程来使用 在上面得到一系列过程中存入内存，分配进程等工作就是OS系统的任务。 操作系统的功能–为上层提供方便易用的服务 向上层提供方便易用的服务 将各个硬件进行封装为多个功能接口并分配给进程程序使用。如下图： 美丽就好像一个接口他需要多个硬件提供支持，而操作系统就是提供接口的服务者，之所以软件层和硬件层不能直接运行而需要OS系统提供媒介的主要原因就是因为语言障碍，软件层是高级语言如“服务A”，而硬件层并不能理解，因此需要OS系统进行指令翻译为提供“美丽”接口同时传达给硬件层能够理解的二进制指令。 GUI：图形用户接口 封装思想：操作系统把一些丑陋的硬件功能封装为简单易用的服务，使用户能够更加方便的使用计算机，用户无需关心底层硬件的原理，只需要对操作系统发出命令即可。 类似于前后台，用户和程序就像前台，只关注于页面设计和数据呈现，而具体的数据提供与数据分析就是由硬件系统这一&quot;后台&quot;来完成，而他们之间的联系者就是OS，提供对应的接口。 很多现代的操作系统都提供了GUI，即图形化用户接口，用户可以使用形象的图形界面进行操作，而不再需要记忆复杂的命令，参数。例如下图： 因此像古老的计算机就是拥有的没有GUI的操作系统，无论是打开文件或者开关机都是需要输入相对应的指令的，而现在的计算机和手机等设备就是拥有GUI的现代操作系统，例如菜单栏，管理文档的文档区等都属于GUI，类似的应用程序也有有无GUI的区分，像vim等就是没有GUI的编辑器，他们需要手动输入指令来打开文件，退出文件等行动，而VSCODE就是拥有GUI的应用程序。 联机命令接口 联机命令接口是交互式命令接口。如win系统计算机必备的cmd窗口就是一个没有GUI的命令窗口。特点就是用户说一句系统就做一步，打开方式： win+R 输入cmd,回车，打开命令解释器 尝试使用time指令，ipconfig指令 类似像git命令窗口也是这种特点的接口，用户输入一条指令，就执行一步，然后在等待用户输入指令在执行。 脱机命令接口 脱机命令接口是批处理命令接口，例如win系统自带的文档搜索功能，他是在用户输入一条指令后进行多步处理即用户说一堆指令，系统跟着做一堆指令，这是与交互式命令接口的根本区别。 程序接口 可以再程序中进行系统调用来使用程序接口。普通用户不能直接使用程序借口，只能通过程序代码间接使用。例如程序猿编写程序时调用的C语言库就是一种程序接口的调用过程。 这种系统调用（或者叫做广义指令）类似于函数调用，使用用程序请求操作系统服务的唯一方式。例如C语言中的输出指令printf函数就是调用的C库函数中的一个原函数，相对应的就会在底层调用操作系统提供的显式相关的&quot;系统调用&quot;,而一般平时情况用户是无法通过输入指令来通过操作系统调用这个接口的。 总结： 所有的有关软硬层直接的信息传输和功能调度都是一定要经过OS处理的，对于命令接口和程序接口了解即可。 操作系统的目标–作为最接近硬件得层次 简而言之，操作系统就是实现对硬件机器的扩展，只有一个裸机安装上了操作系统才能够提供方便用户的服务功能，从而大幅提高了机器的便捷性和功能使用性。我们通常把覆盖了软件的机器称为扩充机器或者虚拟机。 如果将一台设备比喻成一个汽车，那么硬件就像是发动机（只会转），轮胎（只会滚），在原始的硬件机器上覆盖一层系统才能够让发动机有目的性得带着轮子转，即操作系统是连接各个硬件的关键者，他使得各个独立工作的硬件之间产生了协调配合，互相合作的联系从而才能够使用简单的硬件设备之间的合作来实现复杂的功能。 总结 操作系统的四个特征 特征1–并发性 并发是指两个或多个事件在同一时间间隔内发生。这些事件宏观上看是同时发生的，但是实际上微观上看是交替发生的。即cpu频繁的不断切换为两个多个进程服务，因为进程并不是一直不间断的连续需要cpu时刻在旁边提供服务，而是在完成某个阶段的任务后向cpu发起一个服务需求，然后cpu在提供计算服务。这就类似于餐厅内的顾客和服务员，应用程序就像顾客，cpu就像服务员（因为这里讨论的是单核，所以就是一名服务员），服务员需要同时为这几桌的的顾客提供上菜等服务，所以整体上看可以同时为多个顾客提供服务，而微观上服务员一次只能为一个顾客提供服务，所以叫做同一时间间隔内发生。 思考：并行性与并发性的区别？ 并行性就是指两个或多个事件在同一时间内发生，这个才是真正的微观上的同时发生，类似于每一个顾客都有一名服务员，他们之间如果同时需要服务时，则不需要等待或者频繁切换服务员走动，可以真正做到一人配一个实时服务的情况。因此也不难推测到，并行性一般对应是多核CPU，才可以做到同时执行多个程序，实现并行执行。但是注意并行性可不是操作系统的四大特征之一。 思考：并法与并行是互斥事件吗（即操作系统不能同时兼具）？ 并不是，首先毋庸置疑，单核CPU的操作系统肯定就是只有并发性了，毕竟就一个可以提供服务的cpu，但是对于多核的计算机，一般是同时兼具并发性和并行性，原因很好理解，对一个n核cpu处理器，当需要同时服务n+k(k&gt;0)个事件时，也需要并发执行，当然同时也是具有并行性的毕竟一次可以同步执行n个事件。 特征2–共享性 共享是指资源共享，是指系统中的资源可供内存中多个并发执行的进程共同使用。 所以资源共享所说的也是宏观上的同时“共享”，在微观上来看，有可能是交替的对资源进行访问的（分时共享）。 互斥共享 例如QQ和微信同时使用视频聊天，但是同一段时间内摄像头只能分配给其中的一个进程使用。 同时共享 QQ发送文件A，同时微信发送文件B，两边都在同时读取发送硬盘里的文件资源，从读取数据来看他们宏观上在同时共享硬盘资源，但是微观上来看，两个进程还是交替着访问硬盘的。 思考：互斥共享和同时共享的本质区别？ 首先需要声明，无论是这两种共享方式的哪一种本质上都不是真正的并行性共享，即都是分时共享，互斥共享指的仅仅是物理设备位置上的共享，时间上是根本不共享的，即这一时间段内就是只能有一个程序使用。而同时共享在物理设备位置上的共享基础上，在时间间隔内还有类似于并发性的特点，即这一段时间间隔内切换着同时为两个进程资源服务。至于真正做到的并行性共享即同一时间内两个进程或多个进程同时同地对于一个资源进行操作是尽量避免的，互斥锁也正是由这个应运而生的，因为会造成重大的Bug，应该尽量避免此类共享的发生。 思考：并发和共享的关系？ 并发性是指计算机系统中同时存在着多个运行着的程序。而并发性是指计算机系统中的资源科供内存中多个并发执行的进程同时使用。 对于上面QQ和微信同时发送文件A和B的例子来看，并发性体现在两个进程在同一个时间间隔内共同执行，共享性体现在在同一段时间间隔内两个并发执行的程序同时共享的访问硬盘资源。 特质3–虚拟性 虚拟是指把一个物理上的实体变为若干个逻辑上的对应物，物理实体是实际存在的，而逻辑对应物是用户感受到的，但是并不是真实存在的。 举个例子，例如一个程序需要在第7个地址空间存储一个数据，然后再在第50000个地址空间存储一个数据，那么中间没有进行存储的8-49999地址空间不可能真正的留白空出，而是操作系统进行了虚拟化，比如物理地址上的1对应着虚拟地址7，物理地址上的3对应50000，这样就可以有效提高空间的利用率同时压缩了真实物理空间的大小。在实际生活中我们时常发现一个大型主机游戏需要4-20GB的运行内存，而我们的电脑一般只有4-8G的内存，理论上如果按照物理地址1:1对应映射存储的话，根本就不可能将程序放入到内存中（这里先不讨论内外存动态存储，就默认必须直接一次性全部放入才可执行），更别提运行程序了，这时候虚拟的特征就解决了这个问题。再比如，一个程序放入到内存后会分配给cpu执行，但是单核cpu的计算机在频繁切换执行时却可以给用户造成一种同时运行对个程序的感觉（当然实际上是并发运行的），这也能体现虚拟性，这种方法就体现了“分时复用”的一个特点。 思考：虚拟性和并发性的特点？ 虚拟就是一种用逻辑上的映射来加快物理上的执行效率，从而产生了一种同时执行的错觉。所以虚拟是建立在并发的基础之上的，如果每次就运行一个程序，那么也就没必要实现虚拟了。 特征4–异步性 异步是指，在多道程序环境下，允许多个程序并发执行，但是由于资源有限（即使是用来虚拟，实际上空间也是远远不够的），进程的执行不是一贯到底的，而是走走停停，以不可预知的速度向前推进，这就是异步性。 思考：异步性和并发性的关系？ 因为异步也是在多程序同时进行才能体现出来的特征，所以如果失去了并发性，即系统只能串行的运行各个程序，那么每个程序的执行就会一贯到底，也就没有异步性了，所以只有系统拥有并发性，才有可能导致异步性。 总结 我们通过上面的四大特征的定义以及思考对比，发现虚拟和异步都是建立在异步的基础上才会有可能体现出来的，而共享和并发则是相互体现，互为存在的条件，因此并发和共享才是一个操作系统的最基本额两个特征。 操作系统的发展与分类 这部分了解即可，主要是关注和理解各类不同的操作系统主要想解决的问题是什么，以及各自的优缺点。 手工操作阶段 最早的阶段，基本上等同于没有操作系统的阶段，就是人机交互，用户需要和硬件之间进行交互操作导致消耗大量时间，效率极低。即使机器的运行效率很快，但是运行时间的上限非常受人机交互操作的限制。 批阶段处理阶段 单道批处理系统 引入脱机输入、输出技术，（用外围机和磁道完成），并且由监督员负责控制作业的输入和输出），即输入和输出时不再是人机交互，而是引入一些类似磁带等外部接入设备加快输入与输出速度，如下图： 这样没有了超慢和慢两个环节，速度大幅提升： 但是缺点为内存中仅能有一道程序执行，运行结束后才可以调入下一个程序，即没有并发性，cpu利用率太低，空闲时间长。 多道批处理系统 既然读入的程序太少，那就多读，每次都输入多个程序存入内存中并发执行可以进一步加快速度： 多个程序并发进行，共享计算机的资源，资源利用率大幅提升，cpu和其他资源能够长时间保持“忙碌”状态，系统的吞吐量提升： 可以看到这种多条线有交集的就是并发性图的特点，现在流水线加工也一般是这种并发执行。 但是多道批处理系统仍然有缺点，即用户相应时间过长，从输入数据开始后用户就一直得等到输出完成，中间没有人机交互功能，即一旦将作业提交给机器就只能等待机器执行完，这期间用户无权在控制访问自己的作业，这种情况导致程序无法在机器执行时被用户调试或者用户在运行过程中添加其他的参数或选项（当然这种问题手工操作和单道批处理系统也是拥有此缺陷的，但是但是速度都上不来，就更没有考虑此问题，现在速度提上来以后又发现了新的缺陷）。当然多道批系统还是象征着操作系统开始出现，毕竟操作系统主要是为了提供人机随时交互的便捷服务，提升运行速度并不是其根本任务，当然也是重要任务之一。 分时操作系统 计算机以时间片为单位轮流的为各个用户/服务，各个用户可以通过终端（终端概念在此时出现）与计算机进行交互。 优点：用户请求可以及时响应了，及用户可以实时对作业进行暂停，加入新参数或者中途终止任务等，并且这种并发性执行也允许了多个用户同时使用一台计算机进行任务调度，并且用户对计算机的操作相互独立，感受不到其他用户的存在，即各个用户之间操作相互独立，互不打扰。 缺点：不能优先处理一些紧急任务，操作系统是对各个用户/作业绝对公平的，循环的为每一个作业服务一个时间片，不区分任务的紧急性。 实时操作系统 其实就是在分时操作系统的基础上解决了缺点，可以能够优先相应一些紧急任务，某些紧急任务不需要时间片排队，而是优先一直执行完。在实时操作系统的控制下，计算机系统接收到外部信号后及时进行处理，并且要在严格的时限内处理完事件（可以认为根据不同的事件的时限对每一个事件增加了优先级），这种操作系统主要特点为及时性和可靠性。 当然也有特例，例如软实时系统允许偶尔超时。 其他几种操作系统 网络操作系统 伴随着网络的产生应运而生的，能把网络中各个计算机有机集合，实现数据传送等，这种操作系统一般应用于后台大型服务器，主要是实现网络中各种资源的共享（如文件共享）和各台计算机之间的通信。 分布式操作系统 主要特点就是分布性和并发性，系统中的各台计算机地位相同，任何工作够可以分布在这些计算机上，由他们并行、协作完成。 个人计算机操作系统 个人使用，win XP,MacOS等，以上介绍的几种通常都是用于大型服务器的。 总结"},{"title":"什么是线程","path":"/wiki/操作系统笔记/什么是线程/index.html","content":"线程的概念和特点 线程的概念 考虑一个问题，如下图 很明显这是三个进程并发进行。那么在实际上的并发运行中操作系统会频繁的切换进程以达到同时以某种未知的速度进行（异步性的体现）。那么上节我们知道每次切换进程都要进行PCB更新来记录离开时的运行环境，毋庸置疑，这需要很大的时间开销。 并且从上图我们也可以看出进程是调度（即任务分配）和资源分配的基本单位，那么有没有一种而更好的模型可以减少时间开销的同时还能够保证功能的实现？ 这时我们就引入了一个新的概念–线程，其实线程和进程很相似，如下图： 线程的优点 对比之前的模型，我们发现三个进程合并在了一个进程里，此时合并成为了一个QQ进程，而此时进程依然是资源分配的基本单位，他是分配资源的最下单位，但是此时三个合并的进程更名为线程，此时他们不再是资源分配的基本单位，三者共同共享QQ进程这一进程的共享资源，但是线程仍然是cpu调度的基本单位。此时进程成为了资源分配的基本单位，线程是调度的基本单位，这样，当切换线程时就没有必要频繁更新PCB的环境信息了也就减少了切换的时间开销。 对比两种模型，线程的引入只是减小了线程间并发的时间开销，而当时切换不同进程下的线程时，在时间开销并未得到优化，开销还是很大。当然此时，从属于不通进程的线程间的通信仍然必须请求操作系统的系统服务，而统一进程下的线程间可以直接在共享资源空间下进行读写的操作，所以此时同一个进程下的线程间通信不在需要操作系统的服务。 思考：引入线程的概念后进程发生了哪些变化？ 引入线程前，进程同时是cpu调度和资源分配的基本单位，而当引入线程后，进程只是资源分配的基本单位，而线程成为了cpu调度的基本单位。所以可以理解为线程是一个寄存在进程下的小进程，小进程之间拥有直接通信，切换不用更新PCB，共用共享资源的特殊点而已。但是由于线程不是资源分配的单位，所以线程基本上不拥有独属于自己的资源空间，大部分都是共用的一个进程下的共享空间。当然在多cpu的环境下，各个线程也可以分配到不同的cpu上并行地执行且时间开销还很小。但是这里我们只讨论单核，所以线程之间也只会并发执行。 引入了线程的概念后，进程不仅仅是只能串行的执行某一个任务了，从宏观视角来看，此时cpu上的进程可以并发进行，同时某一个进程内的线程也在并发进行，所以并发性显著提高，此时进程只作为除cpu之外的系统资源的分配单元（如打印机，内存地址空间等都是分配给进程的），而线程则成为了处理机的分配单元。 线程的实现方式 用户级线程 用户级线程（User-Level Thread,ULT),用户级线程由应用程序通过线程库实现。所有的线程管理工作都由应用程序负责（包括线程切换），用户级线程中，线程切换在目态即可完成，无需操作系统的干预。在用户看来是有多个线程，但是在操作系统的视角来看，操作系统并意识不到线程存在（用户级线程对用户不透明，对操作系统透明），即用户级线程只是从用户的视角能够看到线程。 在早期的操作系统（如早期的UNIX），只支持进程，不支持线程。当时的“线程”是由线程库实现。以操作系统视角来看，根本就没有线程，而是就是三个进程。即在操作系统看来： 还是三个进程。。如果我们从代码的角度来看，线程其实就是一段代码逻辑，上述三段代码逻辑上可以看做是三个“线程”，而while循环就是“线程库”，线程库完成了对线程的管理工作（如调度，当然while循环就是通过If-else判断管理线程的）。 123456789int main()&#123;int i=0; while(true)&#123; if(i==0)&#123;//处理视频聊天的代码&#125; if(i==1)&#123;//处理文字聊天的代码&#125; if(i==2)&#123;//处理文件传输的代码&#125; i=(i+1)%3; &#125;&#125; 很多变成语言提供了强大的线程库，可以实现线程的创建，销毁和调度等功能。因为用户级线程是由应用程序通过线程库实现的，所有的线程管理工作都是由应用程序负责（包括线程切换），所以在用户级线程中，线程切换在目态即可完成，无需操作系统的干预，当然进程切换还是得有操作系统完成的，不同进程下的线程间信息交流也需要操作系统的服务。由于用户级线程是只是在用户视角下有体现线程，但是在操作系统看来还是几个进程并发进行，所以一旦一个用户级线程被阻塞，整个进程就都被阻塞了，即其他的几个线程也会阻塞，并发度并不高。此时的多个线程不可在多核处理机上并行运行，因为只有在操作系统中的线程才可以在多核cpu上并行运行，而现在虽然有多个线程，但是在系统看来并没有意识到线程。 内核级线程 内核级线程（Kernel-Level Thread,KLT)又称“内核支持的线程”，内核级线程中用户所看到的线程都和操作系统中某一个线程对应（注意不一定是一一对应），所以此时的线程管理工作是由操作系统内核完成，线程调度、切换工作也都是由内核负责，所以也就不需要线程库了，因此内核级线程的切换需要在管态下才能完成。可以简单地理解为此时从操作系统的视角看内核可以看到线程。大多数的现代操作系统都实现了内核级线程，如windos,linux。 以上这些都属于内核级线程，一定要特别注意内核级线程和用户级线程的本质区别就是内核有没有内核级线程的概念，至于所说的多线程模型（下面会将）都是针对内核级线程而讨论的。并且一定要注意，因为操作系统只能看得到内核级线程，所以只有内核级线程才是处理机分配的单位。 操作系统为每一个内核级线程都建立了TCP（Thread Control Block,线程控制块，线程是Thread,进程是Process，所以进程控制块叫做PCB)来对内核级线程进行管理。优点是此时当一个线程在被阻塞后别的线程可以继续并发执行。且因为此时操作系统可以看到线程，所以此时的多线程可以在多核处理机上并行执行。缺点是一个用户进程会有许多的内核级线程，又因为此时的线程是由操作系统内核完成，所以需要频繁的变态，因为管理成本高，开销大。并且对比思考，PCB有不同的组织方式，那么TCB应该也有不同的组织方式。 思考：用户级线程和内核级线程的根本区别？ 就是在操作系统内核看来能否意识到线程的存在即有无内核级线程的概念。 多线程模型 首先我们需要注意的是，当说到多线程模型时，操作系统首先是一定有了线程的概念，即此时肯定是可以意识到线程的存在的，所以用户级线程就没有多线程模型的以下几个分类的概念，即用户级线程不属于下面的任意一种。在支持内核级线程的系统中，根据用户级线程和内核级线程的映射关系，可以划分为以下几类。 一对一模型 一个用户级线程就对应与一个内核级线程，每个用户进程有与用户级线程同数量的内核级线程，优点是当一个线程被阻塞后，别的线程还可以继续执行，并发能力强，且此时多线程可在多核处理机上并行执行。缺点是一个用户进程就会占用多个内核级线程，线程切换由内核完成成本高。 多对一模型 优点是用户级线程的切换在目态下切换即可，线程管理的系统开销小，效率高，但是当一个用户级线程阻塞时，整个进程都会被阻塞，并发度不高。多个线程不可以在多核处理器上并行运行。（和用户级线程很想，但不是用户级线程的模型，因为此时系统能够意识到线程即内核级线程的存在）。 多对多模型 n个用户线程映射到m个(m&lt;=n)内核级线程，每个用户进程对应着m个内核线程。克服了多对一模型的并发度不高的缺点的同时又克服了一对一模型中开销太大的缺点，所以性能较为稳定不极端。还记得前面强调的内核级线程才是cpu调度的基本单位吗，所以此时这个用户进程虽然有3个用户级线程，但是一次性只能个有两个内核级线程获得cpu的调度。 因此我们可以理解为用户级线程是“代码逻辑”的载体，而内核级进程是“运行机会&quot;的载体，内核级线程才是处理机分配的单位，例如：多核cpu环境下，上面这个进程最多被分配到两个和并行执行。一段”代码逻辑“只有获得了“运行机会”才能被cpu执行。内核级线程可以运行任意一个有映射关系的用户级线程的代码，所以只有两个内核级线程中逻辑都被阻塞时，这个进程在会被阻塞。 总结"},{"title":"什么是进程","path":"/wiki/操作系统笔记/什么是进程/index.html","content":"进程的概念，组成，特征和组织方式 进程的概念 程序是静态的，就是存放在某个磁盘里的可执行文件，是一系列指令的集合。而进程是动态的，是程序的一次执行过程，所以同一个程序会对应多个进程。 如上图，QQ是一个程序，而图中的三个框分别对应着QQ程序三次执行过程，因此各为一个进程。仔细回想，你会注意到并发性所描述的是进程在同一个时间间隔内同时进行。 进程的组成 那么操作系统该如何区分每一个进程呢，毕竟有些进程在我们看来是一模一样的根本不好区分，操作系统则是根据PID来区分不同的进程。当一个进程被创建时，操作系统就会为这个进程分配一个唯一的且不重复的“身份证号”–PID。 而操作系统要记录PID和进程所属用户ID（UID)，这个是进程最基本的描述信息，可以使操作系统区分各个进程。例如上图中的3个QQ登录进程实际上在操作系统看来并不相同，他们每个进程各对应着一个独一无二的PID号码。 并且操作系统还要记录为进程分配了那些资源（如：分配了多少内存，正在使用那些I/O设备和正在使用哪些文件），这样以便实现操作系统对于共享资源的管理和分配。 同时还要记录进程的运行情况（如：CPU使用情况，磁盘使用情况，网络流量使用情况等）以方便实现操作系统对进程的控制和协调调度。 而这些信息均被存放在一个数据结构–PCB中（Process Control Block)中，即进程控制块，操作系统需要对各个并发运行的进程进行管理，但凡是管理时所需要的的信息，都会被放在PCB中，因此PCB不仅仅是记录PID号码而已，而是进程的信息以及所运行的环境都要记录，这样放切换不同的进程时操作系统可以保证其在合适的运行环境下进行适当的工作。并且要注意每一个进程都对应一个自己的PCB存储着自己的信息，当进程结束时操作系统会回首PCB，因此可以说PCB是存储进程完整信息的最小单位。 同时进程还拥有程序段和数据段，分别用来存储程序的代码和运行过程中的各种数据。而PCB虽然也属于进程的一部分，但是他并不是提供给进程自身使用的，而只是一个个人身份信息卡，用来提供给操作系统使用，只有程序段和数据段才是进程自己使用的。 思考：程序是怎么运行的？ 先看下图： 这个是一个高级语言翻译到指令然后由cpu执行的过程，那么高级语言-&gt;指令-&gt;cpu执行所对应的一个程序具体的运行过程是怎样的呢？如下图： 高级语言所编写的可执行文件.exe存放于硬盘上存储，当双击打开程序的一个进程时，首先会把程序放入内存，并且操作系统会创建一个PCB分配给这个进程，此时进程所组成的三部分PCB，程序段和数据段都被存放在了内存中，并且程序段中是二进制指令，这样cpu在执行指令时会取出相对应的指令，并将执行所产生的的数据存放在数据段，在进程运行时cpu和内存会频繁的进行信息交换。 我们在这里定义一个新名词叫做进程实体或者进程映像，就是程序段，数据段，PCB三部分的组合，那么引入进程实体后，我们可以更加准确的定义进程：进程并不是一个可见的物质实体，而是一个进程实体的运行过程即是一种连续性的状态组成的，是系统进行资源分配和调度的独立单位，所以准确来说我们之前所称呼的进程实际上是进程实体，因此对于每一个进程，他们的PCB，数据段各不相同会时刻发生着变化，而程序段就是指令集合，一般同一个程序所产生的进程程序段代码会相同。又因为PCB是记录进程的信息，所以PCB中的某些信息（如占用内存的状况等环境信息，当然PID除外）肯定也是时刻发生变化的并且PCB是进程存在的唯一标志。 进程的特征 程序是静态的而进程是动态的，因此进程拥有以下几个特征： 动态性：进程是程序的一次执行过程，是动态产生，变化和消亡的，这也是进程最基本的特征。 并发性：内存中有多个进程实体，各进程可以并发地执行。 独立性：进程是能够独立运行、独立获得资源、独立接受调度的基本单位。 异步性：各进程按各自独立的、不可预知的速度向前推进，操作系统要提供“进程同步机制”来解决异步问题。 结构性：每个进程都会配置一个PCB，结构上看，进程由程序段、数据段、PCB组成。 进程的组织方式 在一个系统中，一般会有数十至数百乃至数千个PCB，为了能够对他们有效的管理，需要适当的方式将PCB组织起来存储管理。进程的组成讨论的是一个进程内部的组成结构信息，而进程的组织讨论的是多个进程之间的组织方式。这里我们讨论两种方式。 链接方式 链接方式是按照进程的状态将PCB分为多个队列，操作系统持有各个队列的指针方便管理进程。如下： 这种方法优点很明显，各个PCB形成一个队列并且PCB指针执行下一个PCB，由指针统一指向调配，切换时更改指针指向后一个PCB即可，切换简便，但是缺点时当有对个PCB块时采用这种队列，当中途需要删除或者提高某个PCB优先级时则需要对两边的指针进行更新比较复杂。 索引方式 根据进程状态的不同，建立几张索引表用来记录PCB地址，操作系统指向各个索引表的指针。如下： 这种方法优点是建立索引表存储，更改指针执行即可切换进程，并且创建删除只需对索引表项进行操作，非常简便，但是缺点是当PCB很多时，索引表就会长可能需要建立多级索引表，同时索引表也许占用额外的空间。 进程的状态与转换 进程的状态 这是个非常重要的概念，他根据不同的进程所处信息环境将进程分为以下几个状态： 创建态 进程正在被创建时，他的状态就是创建态，这个阶段操作系统会为进程分配资源并初始化PCB。因此可能会有多个程序处于就绪态都带等待cpu的情况出现。 就绪态 当进程创建完成后并不是立刻就上cpu执行，当创建完成后便进入了就绪态，此时已经具备了运行条件，但是如果此时cpu没有空闲，则该就绪态进程则需要等待暂时不运行随时准备进入cpu执行，当然运气好的话，也可能刚创建完就上cpu。 运行态 当cpu空闲时，并且内存中有处于就绪态的进程，操作系统就会选择一个就绪态进程（这个会涉及到多种不同的算法，后面介绍）让其上处理机pu执行，此时这个进程就是处于运行态。 阻塞态 当然进程不可能会提前将所有图中的准备都一次性做好，他在运行时可能中途会请求等待另一个事件的发生后才能继续执行，或者发现某个所需的共享资源已被占用，则需等待其他进程的相应，此时操作系统会让这个进程下cpu，并让他进入阻塞态等待（毕竟cpu不只是服务这一个进程，可没时间一直等），此时操作系统就会启动另一个就绪态的进程运行。所以阻塞态不可能直接变为运行态，一定要先恢复到就绪态（毕竟要先准备好切回该进程的工作环境等工作）。并且可以看出在阻塞态之前进程一定是处于运行态。 终止态 一个进程已经执行完成或者中途被用户关掉时就会执行exit系统调用，此时会请求系统终止该程序，然后程序就会进入终止态，操作系统让该进程下cpu，并回收内存空间等资源，最后还要回首该进程的PCB，当终止进程的工作完成之后，这个进程就彻底消失了，如果是连接形式组织方式，那么这个PCB就会删除，指针指向它所指向的下一个PCB，而如果是所以方式，则PCB删除的同时，索引表的项也会删掉。注意就绪态/阻塞态/运行态的进程都可能立刻转换为终止态。 进程状态的转换 这个模型非常重要，需要牢记。 从上图可见运行态和就绪态之间是双向可以切换的，而当是外界因素例如时间片或者其他进程抢占导致的下cpu实惠直接回到就绪态，只有是通过系统调用的方式申请请求时才会切换到阻塞态，即是进程一种的自愿让出cpu的主动行为，所以可以理解为此时自愿下cpu,所以会将工作环境，工作的状态等记录下后下cpu,当可以继续执行的时候则首先需要在配置回所需的工作环境并做好之前的工作状态到达就绪态才能继续执行，而当是被动的不情愿下cpu时则会随时准备抢回cpu的使用权所以会直接切换到就绪态。当然要注意阻塞态切换到就绪态是被动地行为因为这不是进程想继续回到就绪态准备执行就可以随时自主切回就绪态的，而是需要等到某个事件发生后才能回到就绪态，由于时间无法预测所以只能被动等待。而运行态到终止态一般是调用了exit函数（不一定是正常执行完成，当遇到重大的bug如数组越界等导致进程无法继续运行也会触发），一旦转换为终止态，则只能重新创建进程了。 在进程PCB中，会有一个state变量来记录当下进程的状态，1代表创建态，2代表就绪态，3代表运行态…为了对同一个状态下的各个进程进行统一的管理，操作系统会将各个进程的PCB以链接形式或者索引形式统一存储管理。 进程控制 进程控制就是对系统中的进程实施有效的管理，它具有创建新进程，撤销已有进程，实现进程状态转换等功能，其中最重要的就是实现进程的状态转换。 进程控制的实现 实现进程的控制肯定会也是需要程序来实现的，而这个程序就是内核中的原语部分（重点：原语是一个程序），原语是一种特殊的程序，他的执行具有原子性，即这段程序的运行必须是一气呵成无中断的。 思考：原语为什么不能有中断？ 因为如果不能一气呵成，那么就有可能导致操作系统在某些关键数据结构信息处不统一，从而影响操作系统进行个别的管理工作。如果可以中断，就会出现重大的bug，如： 这是一个以连接形式组织的PCB队列，此时假设系统要执行将PCB2（此时他在队列头部，该轮到他了）所对应的进程2等待的进程已经发生了，则此时需要从阻塞态转换到就绪态，即放到就绪队列中，此时原语程序需要进行以下两个步骤： 将PCB的state变量设为1（假设1表示就绪态，2表示阻塞态） 将PCB2从阻塞队列放到就绪队列 那么如果原语可以被打断的话，此时刚刚执行完第一个步骤后收到了中断信号停止执行原语程序，就会出现state=1但是却在阻塞队列的bug。所以原语必须具有一气呵成的特点。 思考：如何实现原语的“原子性”？ 我们可以使用“关中断指令”和“开中断指令”两个特权指令实现原子性。 我们首先知道cpu在每执行完一个指令后都会例行检查是否有中断信号需要处理，如果有，就会暂停运行当前的这段程序，转而执行相应的中断处理程序。如下： 那么显然在执行原语时我们需要cpu不在根据中断信号而停止运行原语程序，因此就有了关中断指令和开中断指令两个特权指令（此时其他的指令如指令1，2和a,b还是非特权指令），那么可以这样实现：当cpu执行了关中断指令后，就不在例行检查中断信号，知道执行到开中断指令之后在恢复检查。这样关中断和开中断之间的这些指令序列（指令a,b)就是不可被中断的了，当然在这期间cpu还是会受到中断信号，但是此时不检查，就可视为忽略了，知道开中断以后在执行中断信息。如下图： 显然关开中断指令必须是特权指令，否则用户可以修改就会造成原语程序被打断的情况出现。 进程控制相关的原语 首先我们看有关进程创建的原语： 有关进程终止的原语： 这里面将该进程拥有的所有资源归还给父进程或操作系统要特别注意。撤销原语是指由就绪态/阻塞态/运行态切换到终止态再到释放时所执行的原语程序。 有关进程的阻塞和唤醒的原语 有关进程的切换的原语： 我们可以看出大部分原语都会有许多步骤，并且引起原语的事件也各不相同。 程序是如何运行的 学完进程转换后，我们在更加详细的讨论一下程序在运行时切换进程在切回进程的具体步骤。首先我们需要了解一个新的概念–寄存器，就是用来存储信息的，这里寄存器可以分为许多类（机组原理有讲），如下图： 我们可以看出PC和IR的作用分别是存储下一条指令和现在正在执行的指令的特殊功能寄存器，并且回忆PSW寄存器是用来记录当前cpu处于管态还是目态的，通用寄存器就是用来存储中间的某些计算数据结果的，所以PC和IR肯定是经常与内存中进程处的程序段进行交流的，而通用寄存器就是与数据段进行频繁的信息交换。 当执行完指令1后PC和IR会立刻更新，并且如果需要通用寄存器会存储信息。那么鸡舍现在进程A执行到了指令3以后需要开始执行另一个就绪进程B，则此时进程A需要切换到阻塞态，因为通用寄存器和PC,IR都是共享资源，那么进程A的信息肯定就不能在占用了，那么之后执行完进程B切换回进程A时如何能够恢复到之前的运行环境呢？如下图： 上图是执行进程B到指令x，此时需要切换回进程A的运行环境并开始执行进程A。这时我们就需要PCB来保存之前进程A运行态时的环境信息（一些必要的寄存器信息），这样在切换回进程A（当然这之前进程A肯定是要先到达就绪态）后可以保证其正常运行。如下图： 因此PCB会存储一些进程必要的环境信息，所以我之前说道PCB会随时发生小部分变化。但是要注意到这个信息是提供给操作系统，然后操作系统进行恢复cpu到之前进程A的运行环境的任务。所以PCB存储的环境信息也是提供给操作系统的，进程自身不使用。 进程通信 顾名思义，进程之间也是需要信息交换的，进程是分配系统资源的单位（包括内存地址空间），因此各进程拥有的内存地址空间相互独立。为了保证安全，一个进程不能直接访问另一个进程的地址空间。但是进程之间的信息交换又是必须实现的，所以为了进程之间的通信安全，操作系统提供了一些方法。 共享存储 即有一个共享空间可以来实现两个进程之间的信息交换，但是需要满足两个进程对共享空间的访问必须是互斥的（互斥访问通过操作系统提供的工具实现）。操作系统只负责提供共享空间和同步互斥工作，具体的信息编写和读入是由进程之间完成的。 思考：为什么共享空间的访问要互斥？ 访问互斥即意味着每次只能有一个进程进入共享空间进行读写，原因很简单，如果可以有多个进程同时进入共享空间进行信息的编写，那么就会出现冲突，即两个进程可能同时对某一个变量更改，这种冲突应该避免。 思考：共享空间的实现方式 共享存储有基于数据结构的共享和基于存储区的共享两种方式来实现，两种不同的共享空间会对共享速度产生影响。 基于数据结构的共享：比如共享空间里只能放入一个长度为10的数组，这种共享方式速度慢并且限制多，是一种低级的通信方式。 基于存储区的共享：在内存中划出一块共享存储区，数据的形式，存放位置都由进程控制，而不是操作系统，相比之下，这种共享方式速度更快是一种高级的通信方式。 消息传递 进程间的数据交换格式与共享一个空间不同，而是“信书传递”，数据交换以格式化的消息为单位，进程通过操作系统的“发送消息/传递消息”两个原语（不可打断）进行数据交换。一般一个消息由两部分组成： 而消息传递又有两种方式，一种是直接通信方式，类似于邮递员链接，消息直接挂到接受进程的消息缓冲队列上，另一种是先发送到中间实体类似于信箱，然后另一个进程从中间实体收取，因此也称为“信箱通信方式”。如下图： 上半部分是直接通信，下半部分是简介通信方式，无好坏之分。 管道通信 如下图： 管道实际上是一个用于连接读写进程的一个共享文件，又名pipe文件，其实就是在内存中开辟一个大小固定的缓冲区。那么他和共享空间又有什么本质区别呢？ 管道采用的是半双工通信，某一个时间段内只能实现单向的传输，即一个时间段只能我传给你或者你传给我，当然方向可以任选只是一个时间段只能一个方向，如果需要双向同时通信，则只能在设置一个管道即两个管道才能同时双向通信。因此管道在一个时间段内永远只有一端是可以写数据的口，另一端是读数据的口，且不能同时打开。 各进程要互斥的访问管道（即读写不同时）。 数据以字符流的形式写入管道，当管道满时，写进程的write()系统调用就会被阻塞即使没有写完，等待读进程将数据取走。当读进程将数据全部取走后，管道变空后，此时读进程的read()系统调用会被阻塞，此时才能继续write()。 如果没写满，就不允许读，如果没读空，就不允许写。 数据一旦被读出，就从管道被抛弃，这就意味着读进程最多只有一个，否则可能会有读错误数据的情况，但是写进程可以有多个。 小测试：项目实战 如果你对管道通信了解透彻了，尝试完成以下这个大作业吧😬：作业大礼包"},{"title":"同步与互斥","path":"/wiki/操作系统笔记/同步与互斥/index.html","content":"进程同步与进程互斥 进程同步 首先我们在前面已经知道进程具有异步性的特点，异步性是指各进程并发地以各自独立的，不可预知的速度向前推进。而在某些需求上，仅仅实现异步性是不可以的。例如：进程通信中的管道通信，读进程和写进程是并发执行的，由于并发性必然导致异步性，因此“写数据”和“读数据”两个操作虽然根据并发和异步的特点会在一个时间间隔内交叉运行，但是归根结底，“读数据”时必须在“写数据”后面的，所以这里仅仅有异步性是不能满足这个需求的实现的，因为并发异步执行的先后顺序是不确定的。 所以这里我们引入了“进程同步”的概念，它是指为了完成某个任务（如上面的管道通信）而建立的两个或多个进程，这些进程因为需要在某些位置上协调他们的工作次序而产生的限制关系。进程间的直接限制关系主要就是来源于他们之间的相互合作。 进程互斥 进程的“并发”需要“共享”的支持（前面讲过并发和共享相互存在，是操作系统进程的两大基础特性）。那么各个并发执行的进程不可避免的需要共享一些资源（比如内存，又比如打印机、摄像头等I/O设备），所以这里根据同一时间段内能否允许多个进程同时使用这个共享资源将共享细分为两类。 同时共享方式 即系统中的某些共享资源，允许一个时间段内由多个进程“同时”对其进行访问。 互斥共享方式 即系统中的某些共享资源，虽然可以提供给多个进程使用，但是一个时间段内只允许一个进程访问该资源。 我们把一个时间段内只允许一个进程使用的资源称为临界资源（前面也讲过访问临界资源的代码段称为临界区）。许多物理设备（比如摄像头，打印机）都属于临界资源，此外还有许多变量，数据，内存缓冲区等都属于临界资源。对临界资源的访问，必须互斥的进行。互斥，亦称为间接制约关系。 进程互斥是指当一个进程访问某临界资源时，另一个想要访问该临界资源的进程必须等待。当前访问临界资源的进程访问结束，释放该资源以后，另一个进程才能去访问临界资源。所以互斥共享实际上就要求进程互斥关系。 对临界资源的互斥访问，可以在逻辑上分为如下四个部分： 临界区是进程访问临界资源的代码段，进入区和退出区是负责实现互斥的代码段（后面所讲的互斥锁PV操作代码段就是这两个区内完成），临界区也称为“临界段”。 思考：如果一个进程暂时不能进入临界区，那么该进程一直占用着处理机吗？该进程要是一直进不去临界区怎么办？ 为了实现对临界资源的互斥访问，同时还要保证系统的整体性能，需要遵循以下原则： 空闲让进：临界区空闲时，可以允许一个请求进入临界区的进程立即进入临界区。 忙则等待：当已有进程进入临界区，其他试图进入临界区的进程必须等待。 有限等待：对请求访问的进程，应保证能在有限时间内进入临界区（保证不会饿死）。 让权等待：当进程不能进入临界区，应立即释放处理机，防止进程忙等待。 思考：有限等待和让权等待不矛盾吗？ 可能你会认为一方面保证进程会在优先时间内等待到进入临界区，一方面又说当不能进入临界区就释放处理机，那进程到底等不等？实际上有限等待的意思是保证进程在一段时间后必定会被操作系统提示可以进入临界区，而让权等待的意思是该进程在等待时就不要在这用cpu了，先阻塞或者挂起等待，在有限等待以后在唤醒该进程上cpu然后范文临界资源，所以不矛盾。 总结 进程互斥的软件实现方式 很明显进程互斥有研究的内容，毕竟具体怎样实现互斥的方法有多种，我们一一了解。 思考：进程互斥需要解决的根本问题？ 首先我们先思考一个问题，现在有两个进程A和B并发地运行，如下图： 那么当A上处理机运行使用打印机时，我们知道根据调度算法，一般一个进程会在执行一个时间片（一般现在的操作系统都是MFQSA调度）后，可能并没有执行完，但是也需要下cpu让另一个进程开始执行，此时进程B上cpu后也开始运行使用打印机，这样结果就是A,B的内容混在了一起（这也解释了前面介绍系统调用必要性的例子中打印内容会混合在一起的原因），那么该怎样才能解决这个问题呢？ 单标志法 算法思想：两个进程（注意只适用于两个对象的情况）在访问临界区后会把使用临界区的权限转交给另一个进程，也就是说每个进程进入临界区的权限只能被另一个进程赋予）。代码如下： 首先我们定义一个全局变量turn=0,turn表示当前允许进入临界区的进程号，因为就两个进程，所以0表示允许进程0访问临界资源，1表示允许进程1访问临界资源。 1int turn = 0; 然后进程0的代码如下，这里我们要自己完善进入区和退出区，因为两个进程要互斥访问，所以1能否进入临界区取决于进程0有没有访问完，所以进程0访问完只需通知一下进程1现在可以进入即可，相对应的就是在退出区让turn=1，这样就可以允许进程1在进程0访问完以后进入临界区了。 P0： 12345//如果此时是对方进入回合即turn值为1则一直执行空语句等待while(turn!=0)&#123;&#125;; ----1//进入区critical section； ----2//临界区turn =1; ----3//退出区remainder section ----4//剩余区 相对应的P1访问完临界区后也要告诉P0现在可以访问临界区了，所以turn=0; P1: 12345//如果此时是对方进入回合即turn值为0则一直执行空语句等待while(turn!=1)&#123;&#125;; ----5//进入区critical section； ----6//临界区turn =0; ----7//退出区remainder section ----8//剩余区 这样turn的初始值为0，所以一开始只允许进程0进入临界区，若P1先上的cpu执行到了访问临界区代码，则会一直卡在代码段5，即进入区禁止进入，此时就一直到P1的时间片用完，调度切换到P0上处理机运行，代码1就不会卡在P0，所以P0可以正常访问临界区，此时如果在P0还在访问临界区阶段时间片到了，此时turn还没有更新到1，所以即使切换到P1此时P1还是在等待，直至P0访问完临界资源并更新了turn值以后P1才能进入临界区。 因此，这种算法可以在软件层次上实现“同一时刻最多只允许一个进程访问临界区”。但是貌似有一些小问题，比如只能按照P0-&gt;P1-&gt;P0-&gt;P1…这样的顺序轮流访问，这种必须“轮流访问”的问题是，如果此时允许进入临界区的进程是P0，而P0一直不访问临界区或者根本就不想访问临界区，那么此时虽然临界区是空闲的，但是也不允许P1访问，因此单标志法违背了“空闲让进”原则。并且这种方法对于多个进程的情况会比较复杂。 注意：上方代码的while后面带分号什么意思？ 一定要仔细观察代码，发现是对于P0，当turn!=0时在一直循环while的空语句表示等待，所以P0也可以写成： 1234while(turn!=0); ----1//进入区critical section； ----2//临界区turn =1; ----3//退出区remainder section ----4//剩余区 这个;很致命，一定要透彻理解第一行whlie语句的意义，他是判断是否需要等待，所以这里的判断条件为turn!=0则表示此时如果不是0的回合，那么0代码就一直在while后面的{};语句中执行空语句等待，这里因为{}里面是空语句所以可以省略大括号，P1同上原因。后面也都是这样表示。 双标志先检查法 算法思想：设置一个布尔型数组flag[],数组中各个元素用来标记各进程想进入临界区的意愿，比如&quot;flag[0]=true&quot;表示0号进程P0现在想要进入临界区。每个进程在进入临界区之前先检查当前有没有别的进程想进入临界区，如果没有，则把自身的对应标志位flag[i]设置为true，之后开始访问临界区。 我们这里还是以进程P0和进程P1为例。首先初始化flag为两位，且一开始都默认为不想访问临界资源。 1bool flag[2]=&#123;false,false&#125;; 然后P0和P1每次访问前都先检查对方是否进入临界区，如果对方不想（即flag=false)那么就将自身的标志位设置为true,这样在自己访问临界资源期间，对方是不能进入的，当访问结束后再将自身的标志位设置为false。 P0： 123456//如果此时对方想进入即布尔值为true则一直执行空语句等待while(flag[1]); ----1 // 进入区flag[0]=true; ----2//将自身标志位更新为true 进入区critical section ---3//访问临界资源 临界区flag[0]=false ----4//访问完毕，将自身标志位在更新为false 退出区remainder section //剩余区 P1: 123456//如果此时对方想进入即布尔值为true则一直执行空语句等待while(flag[0]); ----5 //进入区flag[1]=true; ----6//将自身标志位更新为true 进入区critical section ---7//访问临界资源 临界区flag[1]=false ----8//访问完毕，将自身标志位在更新为false 退出区remainder section //剩余区 这个看似完美，实际上比上一个单标志法还不靠谱，他存在一个重大的bug，就是如果按照1-&gt;5-&gt;2-&gt;6-&gt;3-&gt;7的顺序执行，即假设现在flag数组所有位置都为false,即此时临界资源空闲，然后P0执行完允许进入后进入区判断以后突然时间片用完了，那么此时P0该下cpu了并且PCB记录此时状态是被允许进入空闲资源的，然后此时调度切换到P1执行了进入区代码，此时他发现P0的标志位仍然是false呢，所以他也进入，但是就那么巧，此时P1也用完时间片了下cpu前PCB记录此时被允许进入临界资源的状态，然后又切回了P0开始执行临界区，结果此时又用完时间片了此时P0还在临界资源，里面的P0临街资源还没被释放，P1又进来了，此时也可以对临界资源进行读写，完蛋，打印的内容又混在了一起。所以此时违背了“忙则等待”的原则。 思考：能不能优化一下代码避免这种bug? 可以，我们仔细观察，发现出现这种问题主要是因为进入区的“检查”和“上锁”两个操作不是一气呵成的所以会有可能在“检查完”和“上锁前”出现进程切换。所以我们可以更改一下操作顺序，管那么干嘛，先上上锁不让别人进来然后再检查，即更改成如下 双标志后检查法 首先初始化肯定是不变的 1bool flag[2]=&#123;false,false&#125;; 但是此时是先上锁在检查 P0： 123456flag[0]=true; ----1//先上锁 进入区//如果此时对方想进入即布尔值为true则一直执行空语句等待while(flag[1]); ----2 // 进入区critical section; ----3//临界区flag[0]=fasle; ----4//访问完后解锁 退出区remainder section; P1: 123456flag[1]=true; ----5//先上锁 进入区//如果此时对方想进入即布尔值为true则一直执行空语句等待while(flag[0]);----6//进入区critical section; ----7//临界区flag[1]=fasle; ----8//访问完后解锁 退出区remainder section; 好像更改完确实不会在发生“忙则不等待”的问题了，但是此时貌似又出问题了，按照1-&gt;5-&gt;2-&gt;6的顺序即进程0想访问临界资源然后上锁了但是在检查前时间片用完了，切换到进程1他也先上锁然后发现0貌似已经上锁了所以就一直停在了代码6直至时间片用完，此时又切换回了进程0，由于进程1并没有检查完发现不能进以后解锁的操作，所以此时进程0也会发现进程1貌似也已经上锁了，所以进程0也一直卡在检查区代码2最终时间片用完，就这样即使空闲资源空闲，但是双方都发现对方上锁了就都一直不进入（其实就是产生了对方正在访问的误会），产生了饥饿现象。所以双标志后检查法不能应用。 思考：难道不能进一步优化双标志位法避免bug? 你可能会想到之所以双方都不进入是因为在检查到对方上锁后自己不能进入后没有解锁的操作，所以可能认为在2和6下方各加上一个若发现对方在就自己解锁的操作。但是你会发现无论这个功能根本实现不了，无论这个解锁操作放在哪里都不太合适仍然会触发更多的bug，所以就不要在尝试优化双标志法了，直接放弃思考一个更好的方法。 Peterson算法 算法思想：结合双标志法和单标志法的思想特点，如果双方都想争着进入临界区，那么就尝试互相退让，作一个有礼貌的进程。 其实实现也很简单就是有添加一个新的参量turn表示优先让哪个进程进入临界区，这样就实现了进程之间的谦让，代码如下： 还是先初始化 12bool flag[2]=&#123;false,false&#125;;//初始化时默认此时双方都不想访问临界资源turn=0;//初始化时默认进程1谦让进程0 P0: 1234567flag[0]=true; ----1//表达自己想进去的意愿上锁 进入区turn=1; -----2//优先让1进程进，即自己谦让有礼貌 进入区//如果此时1也想进入即对方上锁了且自己谦让，那么就循环空语句等待while(flag[1]&amp;&amp;turn==1);----3 //进入区critical section; ----4//临界区flag[0]=false; ----5//访问完了，解锁 退出区remainder section; //剩余区 P1： 1234567flag[1]=true; ----6//表达自己想进去的意愿上锁 进入区turn=0; -----7//优先让0进程进，即自己谦让有礼貌 进入区//如果此时0也想进入即对方上锁了且自己谦让，那么就循环空语句等待while(flag[0]&amp;&amp;turn==0);----3 //进入区critical section; ----4//临界区flag[1]=false; ----5//访问完了，解锁 退出区remainder section; //剩余区 此时进程们都很有礼貌了，每次自己想进去时都会谦让，这是我们在走一次1-&gt;2-&gt;3-&gt;6-&gt;7-&gt;8我们会发现确实做到了“忙则等待”和“空闲让进”并且没有在出现重大致命bug了，但是此种方法也不是太好，首先turn值限制了当涉及到多各进程之间时也很复杂，turn就会变得不那么简单，其次他和前面两种方法一样也没有做到“让权等待”即等待是下处理机，而是一直在循环执行while的空语句所以一直在占用cpu，所以peterson算法虽然比前面的方法好但是也不够好。 总结 进程互斥的硬件实现方法 中断屏蔽方法 利用“开/关中断指令”实现（和原语的实现思想相同，即在某个进程开始访问临界区到结束访问临界区为止都不允许中断，也就不会发生进程切换了因为时钟管理也只是在时间片结束后向cpu发送中断信号但是cpu可以忽视即决定权在cpu手中，这样也就不可能发生两个同时访问临界区的情况了） 优点很明显，简单高效确实实现了进程互斥的所有原则，但是缺点是不适用于多处理机，只适用于操作系统内核进程，不适用于用户进程（因为开/关中断指令都是特权指令，只能运行在内核态，这组指令如果用户可以随意使用会很危险，所以覆盖范围太小） TestAndSet指令 简称TS指令，也有地方成为TestAndSetLock指令，或者TSL指令，TSL是用硬件实现的，执行的过程不允许被中断只能一气呵成（很像原语），这里我们用C语言描述一下逻辑（但是一定要注意是硬件实现的，不是软件实现）。 1234567891011121314//布尔共享变量lock表示当前临界区是否被加锁//true 表示已加锁，false 表示未加锁bool TestAndSet(bool *lock)&#123; bool old; old=*lock;//old用来存放lock原来的值 *lock=true;//无论之前是否已经加锁，现在都将lock设置为true; return old;&#125;//以下是使用TSL指令实现互斥的算法逻辑while(TestAndSet(&amp;lock));//上锁并检查 进入区critical section;//临界区lock=false;//访问完解锁 退出区remainder section;//剩余区代码 实际上TSL就是通过硬件手段强制检查和上锁必须一气呵成执行（主要是因为TSL指令必须一气呵成，而TSL就一次完成上锁和检查），此时如果刚开始lock是false,则TSL返回的old值不满足while循环条件，直接跳过等待循环，进入临界区，如果刚开始lock是true,则执行TSL后old返回的值为true，此时满足while循环条件，会一直循环等待，直至当前访问临界区的进程在退出区进行“解锁”后该进程再访问临界资源。相比软件实现方法，TSL指令把“上锁”和“检查”操作用硬件的方式绑定为一气呵成的原子操作，但是注意他不是真的原语，只是具有原语的特性。 优点是实现简单就可以避免bug，这种无需软件实现方法那样严格检查是否会有逻辑漏洞，适用于多处理机环境，缺点是不满足“让权等待”，一旦无法进入临界区进程就会一直执行循环空语句占用cpu。 Swap指令 有的地方也叫作Exchange指令，或简称XCHG指令，Swap指令也是使用硬件实现的，执行的过程中不允许被打断，只能一气呵成，以下是用C语言描述的逻辑（但是一定要注意是硬件实现的，不是软件实现）。 1234567891011121314151617//Swap指令的作用是交换两个变量的值Swap(bool *a, bool *b)&#123; bool temp; temp=*a; *a=*b; *b=temp&#125;//以下是用Swap指令实现进程互斥的算法逻辑//lock 表示当前临界区是否被加锁bool old=true;while(old==true)&#123; Swap(&amp;lock,&amp;old);&#125;critical section;lock=false;remainder section; 看着有点晕😫，正常。我们来屡一下思路，首先我们有一个地方需要注意，此时的while后面不再是空语句了，而是Swap语句，这也就说明加入while判断条件返回为true即old为true,那么就会一直执行Swap(&amp;lock,&amp;old)交换lock和old值然后在判断，所以知道这一点后我们先假设lock=true的情况，那么此时说明有其他进程正在访问临界资源呢，然后bool设置为true后满足while条件进入循环语句内此时lock和old都是true，所以交换后还会满足while判断条件，所以又交换因此当当前正在访问临界资源的进程没有访问完，lock和old就会一直为true，所以等待的进程就是一直在不断的swap，直到那个进程访问完将lock更改为false,此时在经过1~2次的swap就会出现lock=true(实际上lock的true是和old换来的）,old=false(实际上old的false是和lock换过来的)此时这个进程就不在等待了出循环开始访问临界资源。而当lock一开始为false即临界资源空闲的情况，那么进入while循环一次后就会出现lock=true(实际上lock的true是和old换来的）,old=false(实际上old的false是和lock换过来的)的情况所以此时该进程就不需要等待就可以进入了。所以这个指令实现的方法很神奇，他唯一可以进入临界资源的情况就是出现lock=true(实际上lock的true是和old换来的）,old=false(实际上old的false是和lock换过来的)，当没有出现这个情况时等待的进程也不是一直循环空语句，而是一直在swap(虽然此时lock和old都为true😂),即使初始时临界资源空闲也要执行一次swap，所以无论何种情况，swap至少执行一次。并且转来转去实际上Swap和TSL实现的逻辑思路一模一样。 优点也是实现简单就可以避免bug，这种无需软件实现方法那样严格检查是否会有逻辑漏洞，适用于多处理机环境，缺点是不满足“让权等待”，一旦无法进入临界区进程就会一直循环执行Swap语句占用cpu。 总结"},{"title":"内存管理概念","path":"/wiki/操作系统笔记/内存管理概念/index.html","content":"内存基础 内存的定义和作用 其实我们在前面学习进程时已经经常提到了内存的部分知识，我们知道一个进程在上cpu之前需要现在内存中处于就绪态，上cpu后进程实体的PCB,数据段，代码段大部分都处于内存中方便随时和cpu进行信息交换。所以内存可存放数据，程序执行前需要先放到内存中才能被cpu处理----所以cpu的功能是缓和cpu与硬盘之间的速度矛盾。 思考：内存如何区分多个程序的数据存储地？ 我们知道在多道程序环境下，系统中会有多个程序并发执行，也就是说会有多个程序的数据需要同时放在内存中，那么如何区分每一个数据段是属于哪个程序的呢？实际上内存会分为许多部分，有一个一个小房间，每一个小房间就是一个“存储单元”，内存地址从0开始，每个地址对应一个存储单元。 如果计算机“按字节编址”，则每个存储单元为1字节（1Byte)，即1B，即8个二进制位。 如果字长为16位的计算机“按字编址”，则每个存储单元为1个字，每个字的大小为16个二进制位，所以一个字=两个字节。 补充：常用的数量单位与换算 1KB(1K)=210Byte1KB(1K)=2^{10}Byte 1KB(1K)=210Byte 1MB(1M)=220Byte1MB(1M)=2^{20}Byte 1MB(1M)=220Byte 1GB(1G)=230Byte1GB(1G)=2^{30}Byte 1GB(1G)=230Byte 所以我们知道换算进制为2^10也就是1024Btye,所以1K实际上已经非常大了。 思考：4GB内存是什么意思？ 一台手机/电脑的内存为4GB，是什么意思。我们按照上面的公式计算，4GB=4*2^30Byte,如果内存是按照字节编址的，那么也就是会有2^2*2^30=2^32个房间，又因为是从0开始编号，所以房间编号为0~2^32-1。所以需要2^32个地址一一标识这些房间，所以需要32个二进制位来表示。 指令的工作原理 我们思考现在要对x=x+1指令语句进行执行，具体过程如下图： 首先高级指令x=x+1翻译成处理机可以看懂的二进制指令串（可能一个高级指令会对应多条二进制指令），然后cpu执行这个二进制指令串。 我们从上面可以看到cpu根据二进制指令找到010011111处的数据进行取出到寄存器中，然后+1操作，在返还该值到地址处，这样就完成了一个读写操作将x+1。可见，我们写的代码要翻译成CPU能识别的指令，这些指令会告诉CPU应该去内存中的那个地址读/写数据，这个数据应该做什么样的处理。在这个例子中，我们默认这个进程的相关内容从地址#0开始连续存放，指令中的地址参数直接给出了变量x的实际存放地址（物理地址）。 思考：如果进程不是从地址#0开始存放的会影响正常执行吗？ 比如如下面这个案例，我们现在将79处的存储单元写入10然后再将79处的数据读入到寄存器3中，如果进程是从#0开始存放数据的，那么确实可以正常执行： 从上面的图中我们也可以看出程序经过编译，链接后生成的指令中指明的是逻辑地址即相对地址，即相对于进程其实地址而言的地址，如上图中实际上指令中的地址为79处并不是指的物理地址79处，而是相对于进程起始处79处的地址，只不过是刚好此时进程是从地址为#0开始存储的，所以逻辑地址处的79就是映射的物理地址的79处。所以可以正常运行。（为了简化理解，本次我们都默认操作系统为进程分配的是一片连续的内存空间）。 但是实际上情况不可能总是如此的理想。如下图： 我们如果默认逻辑地址就是物理地址的话，此时上面的过程就会出现重大错误。因为此时指令0和1值的还是逻辑地址处的79，但是此时这个进程并不是放到内存中的#0地址开始毕竟内存中会存入许多进程（并发性导致许多进城会在内存中存储随时准备就绪上cpu)，所以此时指令0和1处的所说的的逻辑地址79处实际上是相对于此时起始地址#100开始后面的79个存储单元即绝对地址179处的数据，但是如果我们仅仅是按照逻辑地址==绝对地址执行的话，那么此时就会映射到其他进程的数据段（物理地址79处）这明显是不对的，所以我们在装入模块（可执行文件）进入内存时（这是高级调度/作业调度）需要对地址进行转换以达到在执行指令时读/写数据的地址正确，此时我们需要某些策略来使得指令中的逻辑地址转换为正确的物理地址。 思考：如何将指令中的逻辑地址转换为物理地址？ 策略1：绝对装入 策略2：可重定位装入（静态重定位） 策略3：动态运行时装入（动态重定位） 模块装入的三种方式 绝对装入 在编译时，如果知道程序将放到内存中的那个位置，编译程序将产生绝对地址的目标代码。装入程序按照装入模块中的地址，将程序和数据装入内存。比如上面那道题我们在装入模块到内存之前知道将要在内存地址为100的地方开始存放。 那么此时在对文件进行编译，链接后指令中不在使用逻辑地址，而是直接转换为物理地址如上图，此时在将装入模块（可执行文件）放入内存中，当处理机执行到指令0和指令1时就会到正确的存储单元（物理地址为179）读/写数据。如下图： 虽然没有什么大问题，但是绝对装入只适用于单道程序环境。程序中使用的绝对地址，可在编译或汇编时给出，也可由程序员直接赋予。通常情况下都是编译或汇编时在转换为绝对地址。 思考：为什么只使用于单道程序环境？ 很简单，因为在装入模块进入内存后指令一直是不变的物理地址，但是我们知道在多道程序环境中进程是并发异步执行的，不可能一直存储于内存的一个固定地方，但是一旦装入模块变换了存储地址那么初始地址就也改变了，那么此时很显然此时装入模块中的地址就又出现指向错误了，而且如果绝对装入只能适用于单道环境程序，显然也不满足进程并发执行和内存建立的初衷，所以这种方法缺陷较大，有待改进。 可重定位装入（静态重定位） 静态重定位（可重定位装入），顾名思义肯定是能够弥补上面绝对装入的缺陷，具体做法是编译，链接后的装入模块的地址还是从0开始的，但是指令中使用的地址，数据存放的地址都是相对于起始地址而言的逻辑地址。可根据内存的当前情况，将装入模块放入内存的适当位置。装入时进行重定位，将逻辑地址变换为物理地址（地址变换是在装入时一次完成）。 我们从上图可以看到，他是在装入时对于逻辑地址进行了+100的处理，这样当再次进入内存分配到内存的其他地方时也可以随时更新为正确的地址，不像绝对装入那样直接改变为绝对地址当再次进入内存就有可能出现错误。 思考：还有没有什么可以改进的地方？ 我们对比一下绝对装入和静态可重定位装入两者的区别。 装入策略 地址变化 异同点 绝对装入 编译后逻辑地址-&gt;绝对地址装入内存 有效解决了逻辑地址-&gt;绝对地址的问题，使得可以映射到正确的物理地址上，但是编译后直到运行完销毁前起始存放地址不许更改 静态重定位装入 编译后仍是逻辑地址，装入内存时逻辑地址-&gt;绝对地址 在编译后还是逻辑地址，只有在放入内存前进行+起始地址操作转换为正确的绝对地址，当出内存再次进内存时如果更改了起始存放地址可动态转换为正确的物理地址 但是我们发现静态重定位的特点是一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，作业就不能装入该内存并且最大缺陷是作业一旦进入内存后，在运行期间就不能在移动，也不能再申请内存空间。所以我们好需要解决在内存运行期间移动的问题。 动态运行时装入（动态重定位） 动态重定位：编译，链接的装入模块的地址还是从0开始的，装入程序把装入模块装入内存后，并不会立即把逻辑地址转换为物理地址，而是把地址推迟到程序真正要执行时才进行。因此装入内存后所有的地址依然是逻辑地址。这种方法需要一个重定位寄存器的支持。 装入时： 执行时： 我们看出动态重定位满足所有要求是最好的策略。动态重定位在满足程序在内存中移动的同时，还可以将程序分配到不连续的存储区，所以他区别于静态重定位不需要一次性申请所有连续的地址空间并且每次都只需要取出部分代码执行，如果需要映射地址则通过重定位寄存器可以随时指向正确的存储单元（都不需要连续存储了），简直是太棒了。并且由于是重定位寄存器更改映射地址所以可以向用户提供一个比存储空间大得多的地址空间（虚拟性）。 思考：总结三种策略的异同点？ 我们从以下几个角度区分这三个策略： 装入模块起始地址：绝对装入策略装入模块中的起始地址未必是0，但是静态重定位和动态重定位一定是0 逻辑地址-&gt;物理地址转换时期：绝对装入策略中是在编译，链接后即将逻辑地址转换为物理地址，静态重定位是在装入内存时，而动态重定位是在执行时借助重定位寄存器转换。总的来说，只有动态重定位是在内存中还保存逻辑地址。 借助外界手段：只有动态重定位需要一个辅助的重定位寄存器，静态重定位不需要。 装入的地址要求：绝对装入和静态重定位都需要一次性申请一片连续的容量够大的地址空间，而动态重定位可以离散装入。 思考：链接编译到底是什么？ 我们知道在一个程序从写到运行一次需要经过以下几个过程： 编译就是将用户源代码编译成若干个目标模块（编译就是把高级语言翻译为机器语言），而链接程序将编译后形成的一组目标模块，以及所需要的的库函数链接在一起，形成一个完整的装入模块，装入是由装入程序将装入模块装入内存运行。所以链接很重要，他是形成一个模块的关键步骤，这里面有3中链接方式。 链接的三种方式 静态链接 静态链接就是在程序运行之前先将各目标模块及它们所需要的库函数连接成一个完整的可执行文件（装入模块），之后不再拆开。如下图： 没啥大问题，但是这样就必须一次性申请一片连续的存储地址貌似难以实现因为内存中会造成许多内存碎片。而且准备工作时间很长，没有链接成一个完整的模块之前不能进入内存执行。 装入时动态链接 将各目标模块装入内存时，边装入边链接的一种链接方式，如下图： 这样即使还没有完成全部链接，但是前面的部分模块已经可以进入内存，准备工作时间明显缩短。 运行时动态链接 在程序执行中需要该目标模块时，才对他进行链接。如下图： 不但占用内存空间小，准备时间短，而且便于修改和更新，便于实现对目标模块的共享。 思考：怎么就便于实现目标模块的共享了？ 我们思考有两个程序现在都有一个调用打印机I/O设备的代码段，那么对于运行时动态链接的好处是不需要写两份了，谁需要谁就链接这部分模块，加大了模块的可重复利用率，这也是组件化思想的体现。 总结 基本上全是重点和易错点 内存管理 回顾前面所讲的知识，我们主要着重于对装入模块装入内存前和装入内存时的问题如正确的地址转换，链接方式等，那么接下里来我们在讨论一下对于内存中运行时对于各进程的管理。 内存空间的分配与回收 操作系统作为系统资源的管理者，当然也需要对内存进行管理，要管些什么？首先对于内存空间的分配和回收的任务必不可少。如下： 这些问题都会涉及到许多后续问题所以有不同的算法策略，后面我们将详细讲到。 内存空间的扩展 我们前面也讲过操作系统的虚拟性，实际上就是用过逻辑地址和物理地址的映射以及内外存切换装入等方式实现的，从而能够在有限大小的内存空间中虚拟出远大于物理空间大小的内存空间。所以操作系统需要提供某种技术从逻辑上对内存空间进行扩展。 地址转换 为了使变成更方便，程序猿写程序时应该只需要关注指令、数据的逻辑地址，而逻辑地址到物理地址的转换（这个过程称为地址重定位就是之前讲的三种装入策略）应该由操作系统负责，这样就保证了程序猿写程序时不需要关注物理内存的实际情况。类似的还有刚学写管程概念，也是为了更加方便于程序猿只集中于程序的编写而提出的。所以对于三种装入方式，我们可以看出只有动态重定位是现代操作系统才拥有的，毕竟其他两种方式还需要程序猿关注地址转换为体以防止出错。 内存保护 同时操作系统还需要提供内存保护功能，保证各进程在各自存储空间内运行，互不干扰。这里有两种策略： 策略1: 在cpu上设置一对上、下限寄存器，存放进程的上，下限地址。进程的指令要访问某个地址时，cpu检查是否越界。 所以可以看出上、下限寄存器存储的是物理地址。 策略2 采用重定位寄存器（又称基址寄存器）和界地址寄存器（又称限长寄存器）进行越界的检查，重定位寄存器中存放的是进程的起始物理地址，界地址存放的是进程的最大逻辑地址。 所以管理判断是否越界的是界地址寄存器，并且策略2是根据逻辑地址进行越界检查的，而策略1是根据物理地址进行越界检查的。 总结 覆盖与交换 这里我们讲的覆盖与交换技术不用想肯定是内存空间扩充的技术来实现操作系统的虚拟性，扩大内存空间大小。 覆盖技术 在早期的计算机内存很小，也没有操作系统所以无虚拟性的概念，即就是逻辑地址==物理地址的情况，那么比如IBM推出的第一台PC机最大只支持1MB大小的内存，那么就真的只是1MB了，所以会经常出现内存大小不够的情况出现（比如一个文件为20MB，那么放都放不进去更谈何运行）。所以后来提出了覆盖技术来解决程序大小超过物理内存总和的问题。 覆盖技术的思想是将程序分为多个段（多个模块）。常用的段常驻内存，不常用的段在需要时再调入内存（很容易想到）。所以内存中相应的有一个“固定区”和多个“覆盖区”。需要常驻内存的段放在“固定区”，调入后就不再调出（除非运行结束），不常用的段放在“覆盖区”，需要用到时调入内存，用不到时调出内存。如下： main函数部分在固定区，而BCDEF在覆盖区，这种覆盖技术确实解决了问题，但是必须由程序猿声明覆盖结构，操作系统完成自动覆盖。所以缺点是对用户不透明，增加了用户编程负担。覆盖技术只用于早期的操作系统，现在已经成为历史。并且我们发现还有一个小细节操作系统还可以做到让不能同时被访问的程序段共享同一个覆盖区，这样也做到了一定的减少占用内存空间的作用。 交换技术 交换技术（对换技术）的设计思路是当内存空间紧张时，系统将内存中某些进程暂时换出内存，把外存中某些亿具备运行条件的进程换入内存（进程在内存与磁盘间动态调度，这也是绝对装入方式易出错的地方）。 这里面的进程挂起和就绪运行的状态切换涉及的是中级调度（内存调度），就是决定将那个处于挂起状态的进程重新调入内存。 所以暂时被换出到外存等待的进程为挂起状态（suspend),挂起态又可细分为就绪挂起和阻塞挂起两种状态，这就不得不再提一下状态经典三角切换模型。 思考：交换技术应该将挂起的进程放在外存（磁盘）的什么位置？ 具有对换功能的操作系统中，通常把磁盘空间分为文件区和对换区两部分。文件区主要用于存放文件，主要追求存储空间的利用率，因此对文件区空间的管理采用离散分配方式（空间碎片少）。 对换区空间只占磁盘空间的小部分，被换出的进程数据就存放在对换区。由于对换的速度直接影响到系统的整体速度，因此对换区空间的管理主要追求换出速度，因此通常对换区采用连续分配方式（学过文件管理章节后即可理解）。总之，对换区的I/O速度比文件去更快。 思考：什么时候应该交换？ 交换通常在许多进程运行且内存吃紧时进行，而系统负荷降低就暂停。例如：在发现许多进程运行时经常发生缺页（后面会讲）就说明内存紧张，此时就可以换出一些进程，如果缺页率明显下降了，那么就可以暂停换出了。 思考：换出时应该换出那些进程？ 可优先换出阻塞进程（毕竟在哪都是等😉），可优先换出优先级低的进程，为了防止优先级低的进程在被调入内存后很快又被换出，有的系统会考虑进程在内存的驻留时间来决定换出哪个进程。但是一定要注意PCB是一定不会被换出的，他是常驻内存的。 总结 一定要注意覆盖技术和交换技术的区别在于角度不同，覆盖技术着眼于一个程序或进程，而交换技术是着眼于全局多个进程之间的关系，所以覆盖技术与交换技术互相配合最大限度的对内存空间进行扩展。"},{"title":"基本页式存储管理","path":"/wiki/操作系统笔记/基本页式存储管理/index.html","content":"基本地址变换机构 本节还是讲解基本页式存储管理，在上一节中我们学习了页式存储的地址变换方法，这里我们来理解基本地址变换机构（用于实现逻辑地址到物理地址转换的一组硬件机构）的原理和流程。 页表寄存器 基本地址变换机构可以借助进程的页表将逻辑地址转换为物理地址。通常我们要设置一个页表寄存器（PTR）存放页表在内存中的起始地址F和页表长度M。这样我们才可以找到页表，进程未执行时，页表的起始地址和页表长度放在进程控制块PCB中，当进程被调度室，操作系统内核会把他们放到页表寄存器。 地址变换过程 我们现在已图示的方法演示，注意这里页面大小是2的整数幂，设页面大小为L，逻辑地址A到物理地址E的变换过程如下： 其实前面都讲过了，这里只是演示一下借助页表寄存器具体的转换流程。理解后不需要死记硬背。这里我们来练习一下：假设页面大小为1KB，页号2对应的内存块号b=8，将逻辑地址A=2500转换为物理地址E。相当于告诉了我们以下有效信息： 系统按字节寻址 页内偏移量占地址的10位 页号2对应页框8 那么我们首先计算页号=2500/1024=2,页内偏移为2500%1024=452。所以物理地址实际上就已经出来了是8*1024+452=8644。 所以在分页存储管理的系统中，只要确定了每个页面的大小，逻辑地址结构就可以啦，因此，页式管理中地址是一维的。即只要给出一个逻辑地址系统就可以自动地算出页号、页内偏移量两个部分，并不需要显示的告诉系统这个逻辑地址中，页内偏移量占多少位。 页表项大小的深究 我们知道页表项所占字节大小是根据最大页号所占的位数确定的并且每个页表项的长度是相同的，页号是“隐含”的。前面我们讲过一个例题：物理内存为4GB，页面大小4KB，我们最终算出来页表项至少要占3个字节也就是24bit。但是实际上一般我们是让页表项占4个字节，即即使我们一直页表项为20bit，3字节就已经可以表示了我们还是宁愿在让他多占一个字节为4字节。 思考：为什么要这样做，有什么意义？ 我们知道页表项会按顺序连续地存放在内存中。那么如果页表在内存中的存放起始地址为X，那么M号页对应的页表项是存放在内存地址为X+3*M的。但是如果此时一个页面大小为4KB，那么每个页框都可以存放4096/3=1365个页表项（因为页表存在内存中，那么显然页表也是按页式存储的，所以页表项也是存在页框中的，只不过这几个页框会相邻这样就实现了连续存储了），但是这个页框还会剩余4096%3=1B页内碎片。这其实内部碎片大小是可以忽略的，但是此时就会导致X+3*M公式不适用了，即整体看来页表项不再是连续存储的了，而是每1365个页表项就间隔1B这可很难受。如下： 所以此时我们发现如果每个页表项按4B存储，那么一个页框就可以放4096/4=1024个页表项，并且刚刚好没有内部页内碎片，这样整体看来页表项就还是连续存储的也就说明此时公式X+3*M是可以适用的，所以明显4B更好，即使这样一个页框所存储的页表项就少了但是不差这一点空间。但是我们要注意不是每次都是4个字节都是刚刚好，根据页框大小的不同我们要动态更新但是原则上就是每次都要使得页面恰好可以装得下整数个页表项。 总结 具有快表的地址变换机构 实际上就是一种在基本地址变换机构的改进版本使得查询速率更快了。 什么是快表（TLB） 快表，又称联想寄存器（TLB,translation lookaside buffer)，是一种访问速度比内存快很多的高速缓存（注意TLB不是内存），用来存放最近访问的页表项的副本，可以加速地址变换的速度。与此对应，内存中的页表（前面说过页表存放在内存中）称为慢表。 地址变换过程（引入快表） 一般访问快表TLB只需要1微秒，而访问内存需要100微秒。现在我们同样以图示演示一下用快表寻找（0,0）（0,4）（0,8）这几个逻辑地址。 我们发现和之前的过程不相同的是在得到页号后不是立刻取内存慢表中查找对应的内存块号，而是先在TLB寻找有没有最近刚刚查找过此页表项，如果有那么就可以直接命中知道内存块号了，这样就加速了查找速度，但是前提是TLB中得有即最近查找过这个表，如果没有那么还是需要去慢表中查找。 思考：两种查找方法的本质区别？ 速度不同那是肯定地了，还有就是访问内存单元的次数也是不同的，对于直接TLB命中的，只需要一次内存单元访问即得到物理地址后访问内存中的存储数据的单元，而如果没有在TLB中命中，那么还需要额外在进行一次在内存中的慢表中查询页表项得到物理块号，所以需要两次内存的访问，当然如果这次未命中，那么查询完此次页表项后会将这个页表项的副本加入到TLB中以便下一次再查找这个页表项时可以命中快速查询。 思考：可以提速多少？ 我们知道由于查询快表的速度比查询页表的速度快很多，所以只要尽可能多的快命中，就可以节省很多时间。由局部性原理（即CPU访问存储器时，无论是存取指令还是存取数据，所访问的存储单元都趋于聚集在一个较小的连续区域中）一般来说快表的命中率可以达到90%以上。我们以一道例题来计算： 某系统使用基本分页存储管理，并采用了具有快表的基本地址变换机构，访问一次快表耗时1微秒，访问一次内存耗时100微秒。如果快表的命中率为90%，那么访问一个逻辑地址的平均耗时时间为多少？ （1+100）*0.9+（1+100+100）*0.1=111微秒 对于上面的计算可不要轻视，一定要理解透彻，前面是快表命中，需要一次查询快表的时间1微秒+一次内存访问时间（查询数据）100微秒，而对于未命中时也是查询了一次快表1微秒+两次内存访问时间200微秒（一次慢表查询，一次数据访问）。 思考：能否进一步提速？ 可以，对于某些系统来说支持快表和慢表同时查找即两个搜索同时进行，谁先找到就用谁，这样快表找到的时间还是101微秒，但是慢表查询就是200微秒了（因为不查找快表了）这样计算一个逻辑地址平均查找时间为110.9微秒。而如果不采用快表，那么查找一个逻辑地址所用的平均时间为200微秒。显然引入快表以后，访问一个逻辑地址的速度快了一倍。 这里对于两种查询快表的方式进行对比： 思考：如果把所有页表全部放在TLB那么岂不是更快？ 显然不可能，TLB造价高昂，肯定是容量有限不能容下所有页表，所以这种想法目前为止还不可能实现，但是这样就会出现一个问题，当TLB满了以后再添加新的页表项副本时就需要先淘汰一些页表项，这就涉及到了淘汰谁的问题，这里也大有讲究有许多置换算法后面细讲。 局部性原理 时间局部性原理：如果执行了程序的某条指令，那么不久后这条指令很有可能会再次执行，如果某个数据被访问过，那么不久之后这个数据很有可能会再次被访问。（因为程序中存在大量的循环） 空间局部性原理：一旦程序访问了某个存储单元，在不久之后其附近的存储单元也很有可能再次被访问。（因为许多数据在内存中都是连续存放的） 所以对于上节介绍的无快表的基地址变换机构中，每次访问一个逻辑地址，都需要查询内存中的页表。由于局部性原理，可能很多次查到的都是同一个页表项。 总结： 这里我们在学习了机组原理后知道有一个和TLB非常类似的机构叫做Cache实际上两者是由区别的：TLB中只有页表项的副本，而普通Cache中可能会有其他各种数据的副本。但是解决问题的思路是类似的，实战请参考：缓存Cache实验 两级页表 单级页表存在的问题 我们以一个例题为例：某计算即系统按字节寻址，支持32位的逻辑地址，采用分页存储管理，页面大小为4KB，页表项长度为4B。 那么4KB=2^12B，因此页内地址要用12位表示，所以剩余的20位表示页号。因此，该系统中用户进程最多有2^20页。相应的，一个进程的页表，最多会有2^20=1M=1,048,576个页表项，所以一个页表最大需要2^20*4B=2^22B=4M，共需要2^22/2^12=2^10个页框存储该页表。即需要专门给进程分配1024个连续的页框来存放它的页表。 注意：页表的存储方式！ 这里我还是想再谈一谈页表的存储方式，虽然我们知道按照分页存储，数据是可以离散存放的，但是对于页表这一特殊的数据结构在分页内存中存放时还是要连续放的，所以这就要求页框必须是连续的，并且为了地址好查询即X+4*M还尽量要求页框能够放入整数个页表项。 思考：上面的单级页表存储有什么问题？ 页表必须连续存放，因此当页表很大时，需要占用很多个连续的页框 没有必要让整个页表常驻内存，因为局部性原理进程可能一般在一个时间段内只会访问几个特定的相邻的页面。 所以我们想把页表再分页并离散存储，然后在建立一张页表记录各个部分的存放位置，称为页目录表，或称外层页表，或称顶层页表。这就是两级页表甚至多级页表的由来。 两级页表的原理、地址结构 我们先来看一下单机页表时怎么存储的，此时将页表分为了1024个部分即1024个连续页框，每个页框里有1024个页表项如下： 现在我们将这1024个部分也离散存放然后建立一张表格来记录各部分所存储的位置。如下： 此时一级页号（页目录号）有1024项记录的是1024个部分所存储的页框号，然后二级页号对应的是某个页表中的页号（即页号），最后12位还是页内偏移量。例如此时 此时我们发现实际上空间并没有变大，还是只能存储2^20页，但是此时可以不用连续存储了并且最终的计算还是满足X+4*M的公式。但是此时内存里的分布就比较复杂了，原先是某一连续区间里存放的全是页表（里面放的是页表项），然后另外的地方存储着数据单元，但是现在3号存放的是页表（里面对应的是页表项）但是2号页框里面存储的就是数据了，所以数据存储和页表存储夹杂在一起了。并且分成二级发现无论是哪个页表都是最大为1023的数不会再出现1048575这么大的数了就是因为两级导致的拆分。相当于原先的0~1048575的一张大表变成了1024个小页表了每个页表时0~1023并且每个小页表有了编号为0#~1023#。 现在还没有解决页表常驻的问题，所以我们还需要在页目录中添加一栏状态标志位表示此时i#页表是否在内存中如下： 只有在需要访问i#页表时才放入到内存中（虚拟存储技术），当然页目录肯定是得一直在内存中的。如果想访问的页面不在内存中，就是缺页中断了（内中断/异常：自发指令触发，在cpu中发出中断信号）然后将目标页面从外村调入内存（此时不能说是中级调度，因为不是挂起进程进入调入内存而是缺的页表调入）。 思考：之前为什么说当缺页率高时说明内存紧张了？ 毕竟内存有限，如果内存很小时那么每次调入缺的页表同时还需要调出某些页表腾地方，那么老缺页中断就说明没有足够的地方存放被访问页的地方了。 使用多级页表是有没有什么变化？ 我们知道在单级页表时每一个页表项的地址就是起始地址+页号*页表大小，但是当使用多级页表时只有同一张页表中的页表项还使用（因为一张页表内还是连续存储的）但是此时页表间的页表项就不再适用了，因为此时各个页目录表是离散存储的了。 思考：什么时候使用多级页表？ 若分为两级页表后，页表依然很长，那么我们就可以采用更多级页表，一般来说各级页表的大小不能超过一个页面，毕竟页目录最好就放入一个页框中最合适。例如：某系统按字节编址，采用40位逻辑地址，页面大小为4KB，页表项大小为4B，假设采用纯页式存储，则需要几级页表？页内偏移量多少位？ 页面大小=4KB=2^12B，按字节编址所以页内偏移量就是12位。 那么页号所占位数=40-12=28。又因为页面大小为4KB=2^12B,页表项为2^2B，那么每个页面可以放2^10个页表项。因此各级页表最多包含2^10个页表项，需要10位二进制才能映射到2^10个页表项，因此每一级的页表对应页号为10位（可以少但是不要多于10要不就会出现一张表放不下两张表多于的情况，少的话后面就空着呗）。所以需要三级页表，逻辑地址结构如下： 思考：如果就要用两级页表会怎么样？ 按理论你要是非得用也不是不可以，但是此时就会出现一级页号18位即有2^18个二级页表，那么页目录一张放不下，需要1.8个这就很不优雅。所以不推荐。而如果是8,10,10这样分布就很好，一级页号对应有多少个二级页目录一共有2^8个二级页目录，然后二级页号每一个都是一张二级页目录，页目录中的每一项映射着一个三级页表，三级页号表项映射着页号所对应的物理块号。这样一共是有2^28个物理块存储着数据。 思考：为什么不是10,10,8分布？ 这样很不优雅，意味着有2^20个表都是填不满的，虽然对于查找没什么影响，但是页内碎片多，而8,8,10就只有一级页目录有页内碎片。既节省空间还优雅。 思考：两级页表和三级页表查询步骤（没有快表）？ 两级页表： 第一次访存：访问内存中的页目录表 第二次访存：访问内存中的二级页表 第三次访存：访问目标内存单元 三级页表： 第一次访存：访问内存中的一级页目录表 第二次访存：访问内存中的二级页目录表 第三次访存：访问内存中的三级页表 第四次访存：访问目标内存单元 总结 超级重点，必须会计算！"},{"title":"信号量与经典同步问题(1)","path":"/wiki/操作系统笔记/信号量与经典同步问题(1)/index.html","content":"信号量机制 在信号量机制中一个信号量代表一种资源，信号量的值就是这种资源剩余的值，如果信号量的值小于0，说明此时有进程在等待这种资源。P(s)代表申请一个资源s,如果资源不够就阻塞等待，V(s)代表释放一个资源s,如果有进程在等待该资源，则唤醒一个进程。 信号量机制实现进程互斥 首先我们先看一下信号量的结构体 12345//数值型信号量的定义typedef struct &#123; int value;//剩余资源数 struct process *L;//等待队列&#125;semaphore 一般我们如果要借用信号量机制实现进程间的互斥，那么首先毫无疑问需要将并发进程间的关键活动划定为临界区，如各进程对临界资源打印机的访问的代码就应该划分到临界区。然后我们这里初始化一个新的信号量mutex（代表一种新的资源）初始化为1，则进入区P(mutex)就是申请资源，退出区的V(mutex)就是释放资源。所以我们对于不同的临界资源都需要设置各自对应的互斥信号量进行判断。并且P,V操作必须成对出现，缺少P就不能保证临界资源的互斥访问，而缺少V就会导致资源永不释放，等待的进程一直永不被唤醒造成饥饿现象。这样当mutex设置为1时每次都保证了只会有一个进程访问临界资源并且做到了进程互斥的四大原则（“空闲让进”，“忙则等待”，“有限等待”，“让权等待”）。如下： 这样P1,P2之间就通过mutex1和P,V操作实现了并发进行间的互斥，而P3,P4之间通过mutex2和P,V操作也实现了并发进行间的互斥。代码如下： 12345678910111213141516171819202122232425262728semaphore mutex2=1;//初始化信号量semaphore mutex1=2;//初始化信号量P1()&#123; ... P(mutex1)//使用临界资源前需要加锁 critical section;//临界区 V(mutex1);&#125;P2()&#123; ... P(mutex1)//使用临界资源前需要加锁 critical section;//临界区 V(mutex1);&#125;P3()&#123; ... P(mutex2)//使用临界资源前需要加锁 critical section;//临界区 V(mutex2);&#125;P4()&#123; ... P(mutex2)//使用临界资源前需要加锁 critical section;//临界区 V(mutex2);&#125; 实际上上面这种方法也就是PV互斥锁实现进程互斥，这种方法无疑是最优解他做到了四个原则。 信号量实现进程同步 我们前面学到了进程同步它是一种直接限制关系，即一般发生在某个进程P2必须等待进程P1发生以后才可以发生，这种存在先后关系的并发运行就存在进程同步问题。那么这种让本来异步并发的进程互相配合，有序推进就需要信号量机制加以约数实现。 实现方法如下： 分析什么地方需要实现“同步关系”，即必须保证“一前一后”执行的两个操作（或两句代码） 设置同步信号量S，初始化为0 在“前操作”之后执行V(s) 在“后操作”之前执行P(s) 从上面的操作中我们也可以看出P,V操作一定还是成对出现的但是出现顺序可以发生改变，对于进程互斥是P前V后并且每个进程同时拥有P,V操作，而对于进程同步一般是V前P后，且每个进程只拥有P或者V。 代码如下： 我们还是要先声明信号量但是此时一定要注意是初始化为0： 1semaphore s=0 这样我们可以理解为信号量s代表“某种资源”，刚开始是没有的，而P2需要这种资源才能继续执行，所以他需要等待P1执行完P操作后s++，此时P2才可以继续执行，这种P,V操作就完美实现了进程间的同步。 即如果先执行到了进程1的V(s)操作，那么s++,之后当执行到进程2的P(s)操作时，由于s=1满足条件表示有可用资源，会执行s–，s的值会变回0同时p2进程不会执行block原语而是继续向后执行代码4。 如果先执行到了进程2的P(s)操作，由于s=0,s–后s=-1，此时表示没有可用资源，前面也提到过了当信号量&lt;0表示有进程等待，所以此时进程2执行block原语主动请求阻塞等待。之后当执行到P1的V(s)后s++,使s变回0同时前面也提到过当执行V操作对信号量++的同时还会检查等待队列是够有阻塞进程，如果有就会执行wakeup原语唤醒等待进程，这样P2进程就可以继续执行了。 信号量机制实现前驱关系 进程P1有句代码S1,P2有句代码S2，P3有句代码S3,…，P6有句代码S6，这些代码需要按照如下图的先后顺序执行，其实仔细看对于每一对前驱关系都对应一个信号量一组P,V操作所以前驱关系的子问题就是进程同步。因此对于前驱关系(具有多组进程同步关系的问题)需要为每一对前驱关系都设置一个同步信号量，在“前操作”之后设置相对应的V操作(由于信号量不同，V操作并不相同)同时对于“后操作”之前对应的同步信号量也需要执行P操作(由于信号量的不同，P操作也不相同)。 各进程之间的代码如下： 我们可以画出树状结构来更好的看出前驱关系如下： 总结 生产者-消费者问题 我们首先看一下问题描述 问题描述 系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品都会放入缓冲区，而消费者进程每次从缓冲区都取出一个产品使用(这里的&quot;产品&quot;就是一种数据)。所以生产者和消费者共享一个初始为空，大小为n的缓冲区，只有缓冲区没满时生产者才能把产品放入缓冲区，否则就要阻塞等待。只有缓冲区不空时消费者才能从产品中取出产品，否则必须阻塞等待。并且缓冲区是临界资源，需要各进程互斥的访问。 问题分析 我们对于这种问题很容易就能想到刚刚介绍的PV操作实现同步互斥关系。一般对于PV操作解决问题就是分析出题目中的各个进程之间的关系，然后画图确定P,V操作的大致顺序，同时设置信号量(一般都是数值型)进程互斥设置为1就行，而对于同步信号量的处置要根据对应资源的初始值设置。对于生产/消费者问题我们分析一下题目要求： 缓冲区没空的时候消费者可以消费(同步关系) 缓冲区没满的是够生产者才可以生产(同步关系) 进程之前（即生产者-消费者，消费者-消费者）之间都需要互斥访问临界资源缓冲区(互斥关系) 所以我们需要设置三个信号量如下： 1234semaphore mutex=1;//互斥信号量，实现对缓冲区的互斥访问semaphore empty=n;//同步信号量，表示空闲缓冲区的数量，当&lt;=0表示缓冲区满semaphore full=0;//同步信号量，表示产品的数量即非空缓冲区的数量当&lt;=N时表示缓冲区没满 那么我们可以用如下代码实现： 首先无论是哪个进程进入临界资源都要新进行mutex互斥锁判断，然后对于生产者和消费者之间的同步关系分别具有一个P,V同步锁的一部分。一定要记住P操作每次都–，V操作每次都++同时V操作每次还会进行唤醒等待进程的操作。所以一定要理解并不是每次产品者都把缓冲区用产品填满以后才唤醒消费者，而是当生产出了产品就可以唤醒等待的消费者了同时也并不是消费者每次都用空了缓冲区才唤醒生产者，而是只要缓冲区没满就会唤醒生产者，即边生产边消费的现象出现，但是最终边生产边消费的现象会趋于一个稳态： 生产的速度和吃的速度整体看来相当，那么缓冲区既不空也不满整体上看来生产者和消费者都有并发互斥访问临界资源 生产的速度整体上比消费的速度快，最终缓冲区满了以后生产者沉睡并且直至缓冲区再次没满的情况出现前一直沉睡直至消费者消费了产品后再次唤醒生产者 生产的速度整体上慢于消费的速度，那么缓冲区空了以后消费者全部沉睡等待，当生产者再次生产出产品时就会再次唤醒消费者 思考：相邻的P操作能够更改顺序？ 即如下图： 临界区访问互斥锁的P操作和同步信号量检验的P操作进行了交换，此时是先检验能否进入临界资源在检验能否生产或者消费的情况。 我们假设此时缓冲区已经满了，即empty=0,full=n。此时生产者又生产了一个产品按照① 使mutex变为0，然后在执行②，由于此时已经没有空闲缓冲区了，所以生产者被阻塞沉睡，由于生产者阻塞，因此切换回消费者进程，消费者进程执行③，由于mutex=0即生产者还没有释放对临界资源的&quot;锁&quot;，所以生产者就一直沉睡等待与此用时生产者也在沉睡等待消费者消费产品造成了&quot;死锁&quot;现象的出现。 同样的假设缓冲区现在没有产品，即empty=n,full=0。那么此时消费者进入临界区想取出一个产品，此时mutex=0，发现没有产品可以取出了，所以阻塞沉睡切换到生产者进程生产者生产了产品想放入临街资源缓冲区中，但是此时mutex=0消费者还没有释放&quot;锁&quot;所以生产者只能也沉睡等待消费者出来在放入产品，而此时消费者也在等待生产者放入产品也造成了&quot;死锁&quot;现象。 所以我们可以总结出来无论是哪个进程实现互斥的P操作必须放在实现同步的P操作之后。 思考：上图右边的问题：能否把使用产品放到PV操作之前 即如下图： 12345678910consumer()&#123;\twhile(1)&#123; //使用产品 P(mutex); P(full); //从缓冲区取产品 V(mutex); V(empty); &#125;&#125; 想也不要想这种情况会造成一个很离谱的情况就是此时如果缓冲区已但是消费者却可以不经过检验继续先使用产品，但是此时缓冲区已经没产品了他消费的产品从哪里来呢？ 思考：相邻的V操作能够更改顺序？ 事实证明，V操作之间是可以更改顺序的，此时不会造成&quot;死锁&quot;现象仍可以正常执行。 多生产者-多消费者问题 问题描述 桌子上有一只盘子，每次只能向其中放一个水果，爸爸专向盘子中放苹果，妈妈专向盘子放橘子，儿子专等着吃盘子中的橘子，女儿专等着吃盘子中的苹果，只有盘子为空时，爸爸或妈妈才可向盘子中放一个水果，仅当盘子中有自己需要的水果，儿子或者女儿才可以从盘子中取水果。 问题分析 我们发现有如下几个关系： 盘子需要互斥访问(互斥关系) 只有盘子中为苹果时女儿才取(同步关系) 只有盘子中为橘子时儿子才取(同步关系) 只有盘子为空时爸爸或者妈妈才可以放水果(同步关系) 所以我们第一想法就是设置4个信号量如下： 1234semaphore mutex=1;//实现盘子互斥访问的信号量semaphore apple=0;//盘子中有几个苹果&lt;=0为无苹果semaphore orange=0;//盘子中有几个橘子&lt;=0为无橘子semaphore plate=1;//盘子中还可以放几个水果&lt;=0表示不可以放水果 那么4个进程之间的代码实现如下： 很明显确实实现了题目要求。 思考：可不可以不要互斥信号量？ 即plate互斥信号量删除不用能否满足要求，如下： 我们发现也没有太大的问题，因为此时apple,orange,plate三个信号量永远只有一个会是1也就说明每次都只会有一个进程是非阻塞状态，所以也就只有他可以访问临界区完全不可能出现多个进程用时访问临界资源的情况，所以完全不需要互斥信号量mutex。所以这种方法更好，但是注意仅适用于plate=1的情况，如果一次性可以放入2个或多个水果，即缓冲区容量&gt;1的情况就必须使用mutex互斥信号量了。 思考：为什么缓冲区容量&gt;1时必须使用互斥信号量mutex? 因为此时假设缓冲区容量为2，即盘子可以放2个水果，那么此时假设父亲P(plate)可以访问盘子(此时plate初始值为2，一次P操作plate–=1)，母亲此时也进行P(plate)也可以访问盘子(因为此时plate&gt;0还可以通过检验)此时就出现了两个进程同时访问缓冲区的情况，有可能导致两个进程写入的缓冲区数据相互覆盖。因此当缓冲区大于1的情况时就必须专门设置一个互斥信号量mutex来实现互斥访问了。 总结 对于无论是生产者-消费者问题还是多生产者-多消费者问题最好都加上mutex锁以避免特殊情况出现。"},{"title":"处理机调度","path":"/wiki/操作系统笔记/处理机调度/index.html","content":"处理机调度 基本概念 我们前面说过处理机cpu空闲时会在就绪态进程中挑选一个上cpu执行，这就是调度。当有一堆任务要处理，但由于资源有限，这些事情没有办法同时处理，这就需要某种规则来决定处理这些任务的顺序，这就是“调度”索要研究的问题。在多道程序系统中，进程的数量往往是多于处理机的个数的，这样不可能同时并行地处理各个进程。处理机调度，就是从就绪队列中按照一定的算法选择一个进程并将处理机分配给他使用，以实现进程的并发执行。 调度的三个层次 高级调度（作业调度） 由于内存的空间有限，有时无法将用户提交的所有作业全部放入内存中，因此就需要某种规则来决定将作业调入内存的顺序。高级调度（也叫做作业调度）就是按一定的原则从外存（硬盘等）上处于后备队列的作业中挑选一个（或多个）作业，给他们分配内存等必要资源，一旦进入内存就说明次进程被创建并处于了就绪态，所以需要操作系统创建并分配给其一个PCB，以使他们获得了竞争处理机的权利。 所以高级调度是外存（也叫辅存）与内存之间的调度。每个作业只调入一次，作业调入时会建立相应的PCB，作业调出时才撤销PCB，这也就说明当一个进程处于阻塞态下cpu后仍会存储在内存中随时准备上cpu，而只有当是终止态时才会从内存中取出然后作业调出。高级调度主要是指调入的问题，因为只有调入的时机需要操作系统确定，但调出的时机必然是作业运行结束才掉出。 中级调度（内存调度） 引入了虚拟存储技术后我们知道每次并不是将进程全部的数据放入内存，而是将常用的放入，而可将暂时不能运行的进程调至外存等待，等他重新具备了运行条件且内存又稍有空闲时，再重新调入内存，这么做的目的就是为了提高内存利用率和系统的吞吐量。暂时调到外存等待的进程状态为挂起状态（不是阻塞态），值得注意的是，PCB并不会一起调到外存，而是会常驻在内存中。PCB会记录进程数据在外存中存放的位置，进程状态等信息，操作系统通过内存中的PCB来保持对各个进程的监控和管理，被挂起的进程PCB会被放到挂起队列中。 中级调度（内存调度）就是要决定哪个处于挂起状态的进程重新调入内存，一个进程可能会被多次调出，调入内存，因此中级调度发生的频率要比高级调度更高。 思考：挂起状态在状态转换模型中的位置以及他和阻塞态的区别？ 暂时调到外存等待的进程状态为挂起状态，挂起态又可以进一步分为就绪挂起和阻塞挂起两种状态。所以加上后5状态模型-&gt;7状态模型 低级调度（进程调度） 低级调度（进程调度）其主要任务是按照某种方法和策略从就绪队列中选取一个进程，将处理机分配给他（这个会涉及到很多种调度算法，后面会详细讲解）。进程调度是操作系统中最基本的一种调度，在一般的操作系统中都必须配置进程调度，进程调度的频率很高，一般几十毫秒一次。 思考：三种调度的联系与对比 调度名称 要做什么 发生地点 发生频率 对进程状态的影响 高级调度(作业调度) 按照某种规则，从后备队列中选择合适的作业将其调入内存，并为其创建进程 外存-&gt;内存（面向作业） 最低 无-&gt;创建态-&gt;就绪态 中级调度（内存调度） 按照某种规则，从挂起队列中选择合适的进程将其数据调回内存 外存-&gt;内存（面向进程） 中等 挂起态-&gt;就绪态（阻塞挂起-&gt;阻塞态） 低级调度（进程调度） 按照某种规则，从就绪队列中选择一个进程为其分配处理机 内存-&gt;cpu 最高 就绪态-&gt;运行态 总结 进程调度 这里我们详细展开对低级调度或者叫进程调度的详细研究。 进程调度的时机 进程调度就是按照某种算法从就绪队列中选择一个进程为其分配处理机。一般进程调度会发生在以下两种情况，当然，在某些系统中只允许进程主动放弃处理机（即只有上半部分功能），当然也有的系统除了可以进程主动放弃处理机以外，当有更紧急的任务需要处理时，也会强行剥夺该进程的处理机使用权给更加紧急重要的进程使用（被动放弃）。 一般是发生在运行态转换为其他状态，cpu空闲时发生。但是在以下的情况下一般进程调度很难发生 一定要注意第二点，是进程在操作系统内核程序临界区而不是进程自身处于临界区。当进程处于自身临界区时是可以进行处理机调度的。 思考：什么是临界区？什么是临界资源？ 临界资源是一个时间段内只允许一个进程使用的资源，各进程需要互斥地访问临界资源。而临界区就是访问临界资源的那段代码。所以内核程序临界区就是一般用来访问某种内核数据结构的代码，比如访问进程的就绪队列（由个就绪进程的PCB组成）。 之所以此时一般不发生进程调度，是因为如果内核程序还没退出临界区（即临界资源还没解锁） 就进行进程调度，但是进程调度相关的程序也需要访问就绪队列， 但此时就绪队列被锁住了，因此又无法顺利进行进程调度，同时，内核程序临界区访问的临界资源如果不尽快释放的话，极有可能影响到操作系统内核的其他管理工作。因此在访问内核程序临界区期间不能进行调度与切换。但是当在进程处于临界区时是可以进行处理机调度的，例如：在打印机打印完成之前，进程一直处于临界区内，临界资源不会解锁。但打印机又是慢速设备，此时如果一直不允许进程调度的话就会导致CPU一直空闲内核程序临界区访问的临界资源如果不尽快释放的话，极有可能影响到操作系统内核的其他管理工作。因此在访问内核程序临界区期间不能进行调度与切换而普通临界区访问的临界资源不会直接影响操作系统内核的管理工作。因此在访问普通临界区时是可以进行调度和切换的。 进程调度的方式 非剥夺调度方式 又称为非抢占方式，即只允许进程主动放弃处理机（一般是通过系统调用陷入函数发送请求中断该进程），这种方式，在运行过程中即便有更紧迫的任务到达，当前进程依然会继续使用处理机，知道该进程结束或者主动要求进入阻塞态。这种方式明显不合理，但是实现简单，系统开销小但是无法及时处理紧急任务，一般适用于早期的批处理系统。 剥夺调度方式 又称为抢占方式，当一个进程正在处理机上执行时，如果有一个更重要或者更加紧迫的进程需要使用处理机，则操作系统会立即暂停正在执行的进程，将处理机分配给更加紧迫重要的进程。这种方式优先处理更紧急的进程，也可以实现让各进程按时间片流转执行的功能（通过时钟中断）。适用于分时操作系统，实时操作系统。 进程的切换与过程 思考：狭义的进程调度和进程切换的区别？ 狭义的进程调度就是指从一个就绪队列中选出一个要运行的已就绪的进程（这个进程可以是刚刚被暂停执行的进程或者也可以是另一个进程，而后一种情况就需要进程切换）即仅仅是选择，进程切换是指一个进程让出处理机，由另一个进程占用处理机的过程。 广义的进程调度包括选选择一个进程和进程切换两个步骤，进程的切换过程主要完成了： 对原来运行进程各种数据的保存 对新的进程各种数据的恢复（如操作系统根据PCB对程序计数器，程序状态字寄存器PSW，各种段寄存器等处理机现场信息） 所以进程切换是有代价的，因此如果过于频繁的进行进程调度，切换必然会导致整个系统的效率降低，使得系统花费大量时间在进程切换上，而真正用于执行进程的时间减少。 总结 调度算法的评价指标 cpu利用率 因为早期的cpu造价昂贵（说实话现在对于学生来说也得吃好几天土。。），因此人们希望cpu尽可能的多工作，所以就引出了cpu利用率–用来描述cpu忙碌的时间占总时间的比例。 利用率=忙碌的时间/总时间利用率=忙碌的时间/总时间 利用率=忙碌的时间/总时间 一般设备的利用率指的都是cpu利用率，如：某计算机只支持单道程序（单核），某个作业刚开始需要在CPU上运行5秒， 再用打印机打印输出5秒，之后再执行5秒，才能结束。在此过程中， CPU利用率、打印机利用率分别是多少？ 很简单的计算：cpu利用率=5+5/5+5+5=66.6%，同理打印机利用率=33.3%。通常会考察多道程序并发执行的情况，可以使用“甘特图”来辅助计算。 思考：什么是甘特图？ 就是横道图、条状图。其通过条状图来显示项目、进度和其他时间相关的系统进展的内在关系随着时间进展的情况。类似于下图： 系统吞吐量 对于计算机来说，希望能够尽可能用少的时间处理完尽可能多的作业，这就是系统吞吐量–单位时间内完成的作业的数量。 系统吞吐量=总共完成了多少道作业/总共花了多少时间系统吞吐量=总共完成了多少道作业/总共花了多少时间 系统吞吐量=总共完成了多少道作业/总共花了多少时间 例如：某计算机系统处理完10道作业，共花费100秒，则系统吞吐量为？ 10/100=0.1道/秒。 周转时间 对于计算机的用户来说，他最关心的就是自己的作业从提交（注意不是开始运行）到完成花费的时间。周转时间就是指从作业被提交给系统开始（一般是双击应用程序开始）到作业完成为止的这段时间间隔。一般周转时间由四部分组成：作业在外存后备队列上等待作业调度（高级调度）的时间、进程在就绪队列上等待进程调度（低级调度）的时间，进程在cpu上运行的时间、进程等待I/O操作完成（一半是阻塞态或者挂起态）的时间。后三个在一个作业的整个执行过程中，可能发生多次。 （作业）周转时间=作业完成时间−作业提交时间（作业）周转时间=作业完成时间-作业提交时间 （作业）周转时间=作业完成时间−作业提交时间 上面的周转时间是指对于用户来说更加关心自己的单个作业的周转时间，而对于整个操作系统来说，更关系的是系统自身的整体表现即周转时间的平均值。 平均周转时间=各作业周转时间之和/作业数量平均周转时间=各作业周转时间之和/作业数量 平均周转时间=各作业周转时间之和/作业数量 带权周转时间 因为有的作业运行时间长，有的作业运行时间短，因此在周转时间相同的情况下，运行时间不同的作业给用户的感受也是不一样的。可能有的作业虽然运行时间长会导致平均周转时间更大，但是其对用户产生的满意度更高，所以不能操作系统不能仅仅用平均周转时间来衡量，如果只追求平均周转时间短，那岂不是每次都会尽量做运行时间短的任务，但是可能运行时间更长的大型主机游戏对用户的满意度提升更高也就更主要。所以这里引出一个新的概念–带权周转时间。 带权周转时间=作业周转时间/作业实际运行的时间带权周转时间=作业周转时间/作业实际运行的时间 带权周转时间=作业周转时间/作业实际运行的时间 因为一作业周转时间还包括等待时间，所以带权周转时间必然&gt;=1，但是等待时间肯定是越小越好，所以操作系统需要尽量使带权周转时间和周转时间都是越小越好。 平均带权周转时间 当然肯定也会再引进一个平均带权周转时间的概念 平均带权周转时间=各作业的带权周转时间之和/作业数平均带权周转时间=各作业的带权周转时间之和/作业数 平均带权周转时间=各作业的带权周转时间之和/作业数 思考：周转时间和带权周转时间的关系？ 对于周转时间相同的两个作业，实际运行时间更长的作业在相同时间内被服务的时间肯定更多，相应的带权周转时间就更小，用户满意度更高。对于实际运行时间相同的两个作业，周转时间更短的带权周转时间更小，等待时间更短，用户满意度越高。 等待时间 计算机的用户希望自己的作业尽可能少的等待处理机，等待时间就是指进程（或作业）处于等待处理机状态时间（即处于就绪态）之和，等待的时间越长，带权周转时间越大，用户满意度越低。 对于进程来说，等待时间就是指进程建立后等待被服务的时间，在等待/O完成的期间其实进程也是在被服务的，所以不计入等待时间。对于作业来说，不仅要考虑建立进程后的等待时间，还要加上作业在外存后备队列中等待高级调度的时间。 一个作业总共需要被cpu服务多久，被I/O设备服务多久一般是确定不变的，因此调度算法其实只会影响进程/作业的等待时间，当然，和前面的指标类似，也有“平均等待时间”来评价整体性能的。 响应时间 对于计算机用户来说，会希望自己的提交的请求（比如通过键盘输入一个调试指令）尽早地开始被系统服务、回应，响应时间就是指从用户提交到首次产生相应的时间。"},{"title":"文件共享与保护","path":"/wiki/操作系统笔记/文件共享与保护/index.html","content":"文件共享 操作系统为用户提供文件共享功能，可以让许多个用户共享的使用同一个文件。所以也意味着系统中只有一份文件数据，并且只要某个用户修改了该文件的数据，那么其他用户也可以看到文件数据的变化。如果是多个用户都“复制”了同一个文件，那么系统就会由好几份文件数据，其中一个用户修改了自己的那份文件数据此时并不会其他用户的文件数据造成影响。 基于索引节点的共享方式（硬链接） 我们知道，索引节点是一种文件目录瘦身策略，由于检索文件只需要文件名，所以其他的信息都存放到了索引节点中，这样目录项就只包括文件名和索引节点指针了，如下图： 索引节点中设置了一个链接技术变量count，用于表示链接到本索引节点的用户目录项。如果count=2,说明此时有两个用户目录项都链接到了该索引节点上也就意味着这两个用户共享这个文件。如果某个用户决定删除该文件，那么只需要把用户目录中与该文件对应的目录项删除即可，且索引节点的count值减1。只要count&gt;0，就说明此时还有别的用户要使用这个文件，那么就不能把文件数据删除，否则就会导致目录项中索引节点指针悬空（NULL）。当count==0时就说明没有用户使用这个文件那么就可以删除了，操作系统会负责删除这个文件。 基于符号链的共享方式（软链接） 当User3访问ccc时，操作系统会判断文件ccc属于Link类型文件，那么就会根据其中记录的路径层层查找目录最终找到User1的目录表中的aaa表项，于是就找到了文件1的索引节点。 思考：这种链接方式的一种特点？ 我们发现这种方式的链接会有如下情况发生：当User1的aaa目录项被删除了而此时User3的目录项ccc还没有删除时，那么即使ccc可以指向Link文件2，但是由于User1的aaa不存在了所以不能找到文件1了，所以此时ccc会出现无法找到文件的情况，即User3ccc访问文件是基于User1在存在文件aaa存在的前提下才能实现的。 总结 文件保护 这里我们介绍几种文件保护的具体做法。 口令保护 主要是用于保护文件，设置特殊口令，只有正确才可以访问。所以此时用户访求文件时必须提供正确的口令，口令一般会存放在文件对应的FCB活索引节点中，用户访问文件前需要输入口令，然后操作系统将口令和FCB中的口令做对比，如果正确则允许用户访问文件（那么FCB中的口令肯定是不允许普通用户随意获得的）。这种方式优点是保存口令的空间开销不多，验证口令的时间开销也很小，但是缺点是正确的口令存放在系统内部不够安全。 加密保护 使用某个密码对文件进行加密，在访问时需要提供正确的密码才可以进行正确的解密。那么密码肯定是有多种的，我们以一个最简单的加密算法–异或加密为例。假设用于加密/解密的密码为01001，那么： 上面这种方法确实做到了加密的作用，并且优点是保密性强，不需要在系统中存放正确的密码，缺点是编码/译码（加密/解密）需要花费一定的时间。 访问控制 为每个文件的FCB（或者索引节点）中增加一个访问控制表（Access-Control List,ACL),该表记录各个用户可以对文件执行那些操作。 精简的访问列表就是：以组为单位，标记各组用户可以对文件执行那些操作。如下表： 当某用户想要访问文件时，系统会检查该用户所属的分组是否有相应的访问权限。所以一般重要的OS内核文件肯定是不允许用户访问的即使是操作者。 总结 磁盘的结构 磁盘、磁道、扇区 磁盘的表面由一些磁性物质组成，可以用来存储二进制数据，所以磁盘不能被刮坏。这里我们讲解一下一个磁盘的具体结构术语。 上图就是一个磁盘，他会被等大分为许多扇形，每一个扇形就是一个磁盘块（前面讲过磁盘块存储的数据大小相同），这里的一圈就是磁道，越靠近内侧数据的密度就越大。 磁盘读/写数据方法 我们知道磁盘在被访问时会一直转动，所以就会一直切换不同的扇区即磁盘块，而右边的磁头就是从磁盘上滑过，但是他只会从里到外滑动，不会旋转转动，这样他想在某个扇区（磁盘块）进行数据的读写只需要等到磁盘转到指定扇区即可，即磁头主要是负责切换磁道。 盘面、柱面 为了高效，一般一个盘片的两面均可以存放数据，所以会有许多盘面。所以每一个盘面均对应着一个滑过的磁头，但是磁头是连在一起的所以共进退。这也就意味着一次只能有一个磁头到达他想要到达的位置对应着的就是（柱面号，盘面号，扇区号），柱面号就是规定了磁头将要访问的磁道，而盘面号规定的是此时是哪一个磁头访问这个柱面磁道的数据，扇区号就是访问盘面的哪一个扇面，其他的磁头如果没有到达所要访问的位置也只能等待。 思考：为什么磁头要设计成这样？ 当然我也想过每个磁头可以不用共进退，各自访问他们想要去的位置这样岂不是更加读写高效，但是貌似实现起来也更复杂，并且我们也知道磁盘一转很快的，实际上磁头读写的速度非常快，上面这种已经可以高效实现磁盘的读写操作了。 磁盘的物理地址 其实我们上面已经介绍过来，可以用(柱面号，盘面号，扇区号)来定位任意一个磁盘块，在文件的物理结构中，我们经常提到文件数据存放在外存中的几号块，这个块号就是对应着一个具体的物理地址所以可以转换成（柱面号，盘面号，扇区号）的地址形式。 可根据该地址读取一个块： 根据柱面号移动磁臂，让磁头指向指定柱面（柱面号的作用） 激活指定盘面对应的磁头（盘面号的作用） 磁盘旋转的过程中，当指定的扇区划过时，磁头进行数据的读写，如果一次没有完成可以再等磁盘转过时进行直至完成数据的读/写（扇区号的作用） 磁盘的分类 此时图二的下方也是有许多磁头的只是没画出来，实际上这两种各有利弊，第一个磁臂需要频繁移动来切换磁道，而第二个磁头太多。 总结"},{"title":"文件存储与基本操作","path":"/wiki/操作系统笔记/文件存储与基本操作/index.html","content":"文件的物理结构 文件块，磁盘块 外存中的文件存储方式我们前面提到过实际上和内存中的分页存储类似，磁盘中的存储单元也会被分为一个个“块/磁盘块/物理块”，甚至在许多操作系统中，磁盘块的大小和内存块，页面的大小是相同的。 所以I/O操作的时间开销较大，一般要避免I/O操作（例如内存中页面调度优先淘汰干净页面以避免写回）或者降低I/O的操作次数例如上面讲到的PCB索引节点机制。因为在外存中也是分成一个个外存块，所以文件的逻辑地址也是逻辑块号+块内地址拼接的形式。用户给出的是逻辑地址而操作系统转换为物理地址进行映射。 连续分配 连续分配要求每个文件在磁盘上占有一组连续的块。如下图： 并且对应的文件目录中也需要记录起始块号，占据的块的长度： 并且由于逻辑地址-&gt;物理地址的方法相似，所以当用户给出逻辑块号后，只需要操作系统根据目录项FCB找到对应的物理块号=起始块号+逻辑块号即可，然后在检验合法后在拼接上块内地址即可完成，因此连续分配支持顺序访问和直接访问（随机访问）。 思考：连续分配存储的好处？ 当读取一个磁盘块时，需要移动磁头。那么访问的磁盘块距离越远，移动磁头所需要的的时间也就越长。所以连续分配时块号相邻，那么文件再顺序读/写时速度最快。 思考：连续分配的缺点是什么？ 我们考虑一种情况，比如下图： A此时是占用了连续的3个黄色的磁盘块，但是现在A要进行拓展，需要在增加一个磁盘块并且由于要连续存储，因此此时A放不下了，又因为后面的连续块都已经被橙色所占用，所以A只能全部迁移到绿色区域。这无疑会造成很大的开销。所以物理上采用连续分配的文件不方便拓展。 并且还会造成如上图的情况，这是连续分配存储的共性问题，大量的外部碎片会降低空间的利用率，当然那我们同样可以使用紧凑技术来处理碎片，但是很明显会有大量的开销。 链接分配 隐式链接 为了解决上面的外部碎片问题，我们采用链接分配磁盘块，这里先给出隐式链接的方法，还是如上图，我们此时把文件离散存储并记录起始块号和结束块号（中间块号不记录），每一个中间块都有一个尾指针指向下一个存储文件信息的磁盘块并且对用户开放，这样就不要紧凑技术仍然可以充分利用外部碎片了。如下图： 并且貌似文件拓展也就不是什么难事了所以不会有外部碎片，但是此时却产生了另一个缺点。 思考：这种链接方法的缺陷？ 我们发现此时实际上并不是连续存储了，那么就产生了非连续存储的一个常见问题，就是假设现在用户要访问逻辑块号i，那么操作系统找到对应的FCB然后从起始块开始一个一个查找直至找到所需块号，这样假设要读入逻辑块号i，那么需要i+1次磁盘I/O。这种采用隐式链接的方法，只支持顺序访问，不支持随机访问，查找效率低。另外，指向下一个盘块的指针也需要移动很长距离的磁头，时间开销较大。 显式链接 为了解决上面所遇到的问题，可以把用于链接文件各物理块的指针显式的存放在一个表中，即文件分配表（FAT,File Allocation Table)。同样目录中只需要记录起始块号即可，会另有FAT用来存储这些指针如下： 此时我们为每一个磁盘设置一张FAT，开机时，将FAT放入内存并常驻内存。因为此时按物理块号递增排列，所以物理块号可以隐含不需要占用额外的空间。 此时我们在观察目录表和FAT可以轻松得知文件aaa的磁盘块有2-&gt;5-&gt;0-&gt;1，bbb的文件一次存放在4-&gt;23-&gt;3中。此时如果我们得到了逻辑块号i，那么找到其实块号，如果i&gt;0,则查询内存中的文件分配表FAT，往后查找第i个物理块号即可。所以此时逻辑块号转换成物理块号不需要再进行读磁盘操作。 所以采用显式链接方式的文件，支持顺序访问，也支持随机访问（想访问i号逻辑块时，并不需要依次访问之前的0~i-1号逻辑块），由于块号的转换也不需要访问磁盘，所以相比于隐式链接来说，显式链接访问速度更快。并且显式链接也不会产生外部碎片，可以很方便的进行文件的拓展。 索引分配 这就是分页存储思想在外存中的应用，索引分配允许文件离散的分配在各个磁盘块中，系统会为每个文件建立一张索引表，索引表记录了文件的各个逻辑块对应的物理块（索引表的功能类似于内存管理中的页表–建立逻辑页面到物理页之间的映射关系）。索引表存放的磁盘块称为索引块。文件数据存放的磁盘块称为数据块。 假设某个新创建的文件aaa的数据依次存放在磁盘块2-&gt;5-&gt;13-&gt;9,7号磁盘块作为aaa的索引块，即7号块存放aaa的索引表存放aaa文件的逻辑块号与物理块号的映射关系。所以我们注意到与FAT不同，索引分配中，索引表是一个文件对应一张。 同样我们也涉及到索引表项的字节大小的问题，我们假设磁盘总容量为1TB=2^40B，磁盘块大小为1KB，那么总共有2^30个磁盘块，所以索引表项即可以用4B来表示磁盘块号（同样我们也争取凑成2的整数次幂），因此索引表的逻辑块号也是可以隐含的。 所以，索引分配中逻辑块-&gt;物理块的转换就是通过查表得知，并且索引分配也是可以支持随机访问的，稳健拓展同样容易实现（只需要给文件分配一个空闲块，并增加一个索引表项即可）但是索引表需要占用一定的空间。 思考：一个磁盘块装不下文件的整张索引表时，此时如何解决？ 假设如果每个磁盘块1KB，一个索引表项4B，那么一个磁盘块只能存放256个索引项，但是如果一个文件的大小超过了256块，那么很明显此时一个磁盘块装不下文件的整张索引表，该怎么办。我们有以下几种解决策略： 链接方案 如果索引表太大，一个索引块放不下，那么可以将多个索引块链接起来存放。如下： 看似问题有效解决了，但是考虑一种情况：假设磁盘块大小为1KB，一个索引表项占4B，则一个磁盘块只能存放256个索引项。如果一个文件大小为256*256KB=64MB，那么这个文件一共需要256*256块,也就是256个索引块来存储，那么如果真的是按照链接形式存放，如果想要访问最后一个索引块就需要先将前面的255个全部访问一遍，这样顺序查找时间开销太大。 多层索引 建立多层索引（类似于多级页表），是第一层索引块指向第二层索引块，还可根据需要再建立第三层，第四层索引块。 如果采用这种二层索引，那么该文件的最大程度可以到达64MB，并且还可以根据逻辑块号算出应该查找索引表中的哪个表项。例如现在要查找1026号逻辑块： 1026/256=4,1026%256=2。所以先将第一层索引表调入内存，查询4号表项，然后在对应的二级索引表调入内存，再查询二级索引表的2号表项即可知道1026号逻辑块存放的磁盘块号了。这样访问数据块，需要3从I/O操作，那块采用K级索引结构，且顶级索引表未调入内存（一定要注意，一般顶级索引表常驻内存），那么访问一个数据块只需要K+1次读磁盘操作。同理如果是三层索引，那么文件最大长度就是256*256*256KB=16GB,并且查找到一个物理块需要4次磁盘读操作。 混合索引 多种索引分配方式的结合。例如：一个文件的顶级索引表中，既包含了直接地址索引（直接指向数据块），又包含一级间接索引（指向单层索引表）、还包含两级间接索引（指向两层索引表）。 这样需要经常被访问的就放在直接地址索引，对于不经常使用的放在多级地址索引，高效同时长度拓展的也很大。非常合理，同时我们也可以进行文件的大小估计。例如上图中的最大文件长度就是65800KB，其实计算和多级索引类似。 三种索引分配的总结 索引策略 策略规则 缺点 链接方案 一个索引块装不下可以多个索引块链接存放 I/O次数过多，查找效率极低 多层索引 建立多级索引表，类似于多级页表 即便是小文件也需要K+1次读磁盘 混合索引 多种索引方式的结合，既有直接地址索引也有多级间接索引 对于小文件访问次数少，查找高效 ❗超级重点：一定要回根据多层索引和混合索引的结构（各级索引表必须放在一个磁盘块中）计算文件的最大长度，公式是： 个数∗索引块的大小个数*索引块的大小 个数∗索引块的大小 要回分析所需要的读磁盘次数，并且一定要注意题目条件–顶级索引块是否已经掉入内存。 总结 分配方式 怎样实现 目录项内容 优点 缺点 顺序分配 为文件分配的块必须是连续的磁盘块 起始块号、文件长度 顺序存取速度快，支持随机访问 会产生碎片，不利于文件拓展 链接分配（隐式链接） 除文件的最后一个盘块之外，每个盘块中都存有指向下一个盘块的指针 起始块号，结束块号 可解决碎片问题，外存利用率高，文件拓展实现方便 只能顺序访问，不能随机访问 链接分配（显式链接） 建立一张文件分配表FAT显式记录盘块的先后关系（开机后FAT常驻内存） 起始块号 除了拥有隐式链接的优点之外，还可以通过查询内存中的FAT实现随机访问 FAT需要占用一定的存储空间 索引分配 为文件数据块建立索引表，索引表存储在索引块中，如果文件太大，可采用链接方案，多层索引或者混合索引策略 链接方案记录第一个索引块的块号，多层/混合索引记录的是顶级索引块的块号 支持随机访问，易于实现文件的拓展 索引表需占用一定的存储空间，访问数据块前需要先读入索引块。如果采用的是链接方案，查找索引块时可能需要很多次读磁盘操作 混淆点：什么是支持随机访问？ 假设现在这个文件的逻辑结构是“顺序文件”，并且是定长记录，每个记录的长度是16B，那么i号记录的逻辑地址是多少？（从0开始编号） 每块大小为1KB，定长记录时16B，所以一各磁盘块可以存放64个记录，则： “文件的某种逻辑结构支持随机存取/随机访问”是指：采用这种逻辑结构的文件，可以根据记录号直接算出该记录对应的逻辑地址（逻辑块号，块内地址）。 文件存储空间管理 存储空间的划分与初始化 安装windows操作系统的时候必须经历的步骤–为磁盘分区（C盘，D盘等）。 存储空间管理–空闲表法 空闲表法主要适用于连续分配方式，这里是用一张空闲盘块表进行对空闲物理块的记录，如下： 如何分配磁盘块：其实和内存管理的动态分区分配很相似，为一个文件分配连续的存储空间，同样可以采用首次适应，最佳适应，最坏适应等算法来决定为文件配到那个区。 所以毋庸置疑回收磁盘块时肯定是情况也类似有以下几种情况： 回收区的前后都没有相邻空闲区 回收区的前后都是空闲区 回收区前面是空闲区 回收区后面是空闲区 所以我们也需要注意合并的问题。 存储空间管理–空闲表链法 空闲盘块链 操作系统保存着链头，链尾指针。分配时如果要申请K个盘块，那从链头开始依次摘下K个盘块分配，并修改空闲链的链头指针。回收时回收的盘块挂到链尾，并修改空闲链的链尾指针。这种方法适用于离散分配的物理结构，为文件分配多个盘块时可能要重复多次操作。 空闲盘区链 操作系统保存着链头，链尾指针。 分配时若某文件申请K个盘块，则可以采用首次适应，最佳适应等算法，从链头开始检索，按照算法规则找到一个大小符合要求的空闲盘区， 分配给文件。若没有合适的连续空闲块，也可以 将不同盘区的盘块同时分配给一个文件，注意分配后可能要修改相应的链指针、盘区大小等数据。 回收时若回收区和某个空闲盘区相邻，则需要将回收区合并到空闲盘区中。若回收区没有和 任何空闲区相邻，将回收区作为单独的一个空闲盘区挂到链尾。 这种方法离散分配和连续分配都适用，为一个文件分配多个盘块时效率更高。 存储空间管理–位示图法 其实就类似于矩阵存储，但是又不是完全一样，如下图： 这是磁盘的情况，那么我们可以列出一种特殊的矩阵形式，如下： 他是由字号和位号来表示的，因为这个矩阵时16列，所以一个字号就是代表几个16，而位号就是几个1，优点类似于满16进1的表示意味。 位示图：每个二进制位对应一个盘块，在本例中，“0”代表空闲，“1”代表盘块已分配。位示图一般用连续的“字”来表示，如本例题中一个字的字长是16，字中的每一位对应一个盘块。因此可以用（字号，位号）对应一个盘块号。当然有的题目中也描述为（行号，列号）。但是总之就是要自己会推算出每个盘块的空闲状态。这里主要要注意开头的号码是0还是1千万要注意一个字是多少位。 (字号,位号)=(i,j)的二进制位对应的盘块号b=n∗i+j(字号,位号)=(i,j)的二进制位对应的盘块号b=n*i+j (字号,位号)=(i,j)的二进制位对应的盘块号b=n∗i+j 所以如下图我们可以推出： 同理我们也可以一直盘块号反推出字号和位号 b号盘对应的字号i=b/nb号盘对应的字号i=b/n b号盘对应的字号i=b/n b号盘块对应的位号j=b%nb号盘块对应的位号j=b\\%n b号盘块对应的位号j=b%n 所以我们可以反推出： 所以分配时：若分配需要K个块， ①顺序扫描位示图，找到K个相邻或不相邻 的“0” ②根据字号、位号算出对应的盘块号，将相应盘块分配给文件 ③将相应位设置为“1” 回收时： ①根据回收的盘块号计算出对应的字号、位号 ②将相应二进 制位设为“0” 存储空间管理–成组链接法 空闲表法和空闲链表法都不适用于大型文件系统，因为空闲表或者空闲链表会过大。所以UNIX系统采用了成组链接法对磁盘空闲块进行管理。 文件卷的目录区中专门有一个磁盘块为“超级块”，当系统启动时需要将超级块读入内存。并且要保证内存与外存中的“超级块”数据一致。 超级块记录的是下一组的空闲块数，然后底下表示的就是空闲块号，并且这些空闲块不是连续的，而是离散存储使用指针相连的，这里只是为了方便表示。所以现在上图中的情况是表示超级块表示下一组有100个空闲块分别是201~300号同时发现300号，即此时300号是空的，但是同时300号有表示下一组有100个空闲块分别是301~400号，同时400号表示下一组7801~7900是空闲块，但是当遇到-1表示这个是空闲块的末尾了即使空闲块此时显示一个正整数但是此时也表示没有空闲块了。 思考：如何分配空闲块？ 我们现在假设需要给一个空闲块分配，那么首先检查第一个分组的块数是否满足。发现1&lt;100所以第一组就可以满足，然后分配第一组中的一个空闲块并修改数据即可。所以加入一个后变成： 此时第一个超级块变成了99，同时201~300号中有一个变成了非空闲块。一定要注意即使此时300上面显示下一组的空闲块数说明他是一个空闲块但是他仍然自身还是一个空闲块。 现在我们假设要分配100个空闲块，那么显然此时第一组刚好放下，所以201~300全部填满都变成了非空闲块，但是此时300底下也有一组空闲块为301~400所以此时不能放在300底下了赋值到超级块底下如下： 所以超级块第一个位置是400，直接指向400开头的组，所以此时超级块底下为400，7801~7900。 思考：如何回收非空闲块？ 因为每组就只能100（一般是规定好的最大值），那么此时下图中： 超级块此时是99表示下一组有99个空闲块，还可以回收一个，所以回收的如果刚好1个那么就放到第一组的末尾。但是如果此时要回收100个，那么就会超了，所以要新建一组来存放空闲块，并且组头用来标记下一组的空闲块数，如下图： 那么此时组成的100个空闲块的一组就组成一个新组并且300中的400指向之前的400开头的组，那么此时超级块就表示第一个组只有1个空闲块了。 对于成组链接法不要求掌握，确实不太好描述和理解。主要是知道超级块是一切的起点然后同时记录着下一组的空闲块数就好。我讲的不太好，可以参考这篇博客大佬博客 总结 文件的基本操作 这里我们学习基本功能的具体实现方法，了解即可 创建文件 需要调用Create系统调用，主要需要提供以下几个参数： 文件所需要的外存空间大小（如：一个盘块，即1KB）。 文件存放的路径(“D:/Demo”) 文件名（这个地方默认为&quot;新建文本文档.txt&quot;) 操作系统在进行Create系统调用时，主要进行了两件事： 在外存中找到文件所需的空间（结合上小节学习的空闲链表法，位示图等） 根据文件存放的路径的信息找到该目录对应的目录文件（此处就是D:/Demo目录)，在目录中创建该文件对应的目录项。目录项中包含了文件名、文件在外存中的存放位置信息。 删除文件 进行Delete系统调用，需要以下几个参数： 文件存放路径（D:/Demo ） 文件名（test.txt） 操作系统在处理Delete系统调用时，主要做了几件事： 根据文件存放路径找到对应的目录文件，从目录中找到文件名对应的目录项 根据该目录项记录的文件在外存中的存放位置。文件信息大小等回收文件占用的磁盘块。（同样的，回收时也需要根据空闲链表，空闲表，位示图等策略回收） 从目录表中删除文件对应的目录项 打开文件 在很多操作系统中，在对文件进行操作之前，需要用户使用Open系统调用打开文件，需要提供以下几个参数： 文件存放路径（D:/Demo） 文件名（test.txt) 要对文件的操作类型（如：r只读，rw读写等） 操作系统需要处理Open系统调用时，主要做一下几件事： 根据文件存放路径找到相应的目录文件，从目录中找到文件名对应的目录项，并检查该用户是否有指定的操作权限。 将目录项复制到内存中的“打开文件表”中，并将对应表目的编号返还给用户，之后用户再使用打开文件表的编号来指明要操作的文件。 思考：一共有多少个打开文件表？ 每个进程会对应着自己的一个打开文件表，同时系统也还拥有一张系统的打开文件表： 关闭文件 与打开文件相反，操作也类似。 一定不要忘记最后一步的系统打开文件表项的操作。 读文件 进程使用read系统调用完成读操作，需要指明是哪个文件（在支持“打开文件”的操作系统中，只需要指明文件在打开文件表中的索引值就行了），还需要指明要读入的数据大小、读入的数据的存放位置。 操作系统在处理read系统调用时，会从读指针指向的外存中，将用户指定大小的数据读入到内存的指定存放位置。 写文件 进程使用write系统调用完成写操作，需要指明是哪个文件（在支持“打开文件”操作系统中，只需要提供文件在打开文件表中的索引号即可），还需要指明要写出多少数据，写回外存的数据的存放位置。 操作系统在处理write系统调用时，会从用户指定的内存区域，将指定大小的数据写回写指针指向的外存。 思考：读写操作的细节？ 读操作是数据进入内存，写操作是写回外存，并且读/写指针都是指向的外存，一定要注意。 总结 实际上当遇到文件重名时，系统会请求用户端能否使用(1)(2)后缀表示或者用户端更改文件名。"},{"title":"段页式管理与虚拟内存概念","path":"/wiki/操作系统笔记/段页式管理与虚拟内存概念/index.html","content":"基本分段存储管理 分段 实际上类似于分页管理中的“分页”思想。我们先看一下分段的定义。 地址空间：按照程序自身的逻辑关系将程序划分为若干个段，每个段都有一个段名（在低级语言中，程序猿使用段名来编程），每段从0开始编制。而分段的内存分配规则就是：以段为单位进行分配，每个段在内存占据连续空间，但各个段可以离散存储不相邻，所以大小也是可以不相等的（不像分页必须规定固定大小的页帧）。 思考：为什么要引进分段的概念？ 我们熟悉的数据段，代码段都是分段的概念，那么分段和分页的区别是什么呢，又为什么要引入分段的概念呢？我们思考分页的存储方式，他是直接将进程按照固定大小切成许多小部分存到了不同的页。但是这里会涉及到一个尴尬的问题，就是大部分页要不存的全是代码，要不存的全是数据，但是总是会有那么几个页且在了数据和代码的交界处，这样这个页就会同时存储着数据和代码的混合体，这种页对于系统管理当然是没有问题的，但是对于编程人员来说就会可读性很差，不友好，导致用户编程不方便。按照人类的正常思维，最好将程序的不同的种类分段，这一段全存储数据，这一段全存储数据等等这样就会对人类可读友好。所以引进了段的概念。即分页是按照物理大小划分，分段是按照逻辑功能模块划分。 那么分段系统中同样有逻辑地址此时的逻辑地址是由段号（段名）和段内地址（段内偏移量）组成，如下： 段号决定每个进程最多可以分为几个段，而段内偏移量决定了每个段的最大长度是多少。如上图这样的32位划分，那么如果系统是按照字节编址，那么段号16位，因此程序最多可以分为2^16=64K个段，段内地址为16位，因此每个段最大长度为2^16=64KB。 同样的和分页查找地址类似，段中的某一个地址也是根据段号+段内偏移量找到具体的位置，如下： 那么 段号一般是用[]包裹，然后根据逻辑地址和段号就可以求得段内偏移量同样也需要段表查找到段的物理起始地址然后就可以找到真正的物理地址进行相应的读/写操作了(实际上和分页存储的查找方式是一样的)。 段表 问题：程序会被分为许多个段，各段离散地装入内存中，为了保证程序能正常运行，就必须从物理内存中找到各个逻辑段的存放位置。因此同样需要建立一张段映射表简称“段表”。 不同的是此时段表没有块号而是改为基址，并且有新添加了一栏段长，这是因为分段存储中段的大小长度不固定而导致的。所以物理块的起始地址也就不能使用X+4*M这种来计算了。 这里有几个要注意的点： 每个段对应一个段表项，其中记录了该段在内存中的起始位置（又称“基址”）和段的长度 各个段表项的长度是相同的。例如：某系统按照字节寻址，采用分段存储管理，逻辑地址结构为（段号16位，段内地址16位），因此使用16位即可表示最大段长。物理内存大小为4GB（可以用32位表示整个物理内存）。因此可以让每个段表项占16+32=48位，即6B。由于段表项长度相同，因此段号是可以隐含的，不占存储空间。若段表其实地址为,。则K号段对应的段表项存放的地址为M+K*6。 思考：分段存储和大小不等的固定分区分配的区别？ 我们知道在讲解分页时我们对比了分页和固定大小的固定分区分配两者的区别。这里我们同样来区分以下分段和大小不等的固定分区分配的区别。大小不等的固定分区分配是将内存块分成大小不等的一个个小空间，每个空间存放一个作业/进程，各个空间之间的进程/作业互不干扰。但是分段存储是将进程首先分成许多个大小长度不固定的块然后离散存储到物理块的不同位置处。这就是区别。 思考：此时我们是不是也需要考虑页表项凑成刚好被放入整数个时的字节大小问题？ 答案是不用的，我们思考以下问什么，对于分页存储每一个存储单元是固定大小的，所以需要连续存储时就是选取相邻连续的页帧凑成的，只有刚好恰好装满整数个时才能实现页表项之间的连续。但是现在段存储长度是可以改变的，所以就是页表项有多少，段的大小就是多少不需要再去凑了，毕竟段表肯定也是以段的方式存储到物理内存的（此时我们不讨论段页式存储，就仅仅是纯页式存储或纯段式存储） 思考：有没有可能有多级段表？ 应该是没有的，毕竟之所以有多级页表是因为页完全不需要连续，可以在离散的基础上再次离散，但是段就是按照逻辑模块进行划分的最小单元了，在离散逻辑模块就不连续了。所以应该是没有多级段表的。 地址变换 同样的我们也讨论以下如何实现逻辑地址到物理地址的转换。此时就没有什么“相除取页号，取余得偏移量”的步骤了，段式存储最大的特点就是在指令中直接就指出了段号和逻辑地址，那么直接就可以求得段内偏移然后查表就可以了。即少了求段号的一步（段号一般用[]包裹）。 同样的此时在二进制串中的转换方法是不变的，还是后K位表示偏移量，剩下的位数表示段号，然后根据段号查表找到基址，那么物理地址就是基址+段内偏移量。多简单，都不用像页式存储那样还得得到块号自己算物理起始地址，此时段式存储直接就可以根据表得到基址。 具体的变换演示如图： 我们一定要注意此时还多了一个步骤就是步骤4需要进行检查段内偏移量是否越界了。这个可千万不要忘记。所以也是需要进行两次访存一次是查表，一次是访问数据。 分段、分页管理的对比 我们从以下几个角度探讨： 角度1：划分单位 页是信息的物理单位，分页的主要目的是为了实现离散存储，提高内存的利用率（内部碎片很少）。分页仅仅是系统管理上的需要，完全是系统行为，对用户是不可见的。 段是信息的逻辑单位，分段的主要目的是更好的满足用户需求，一个段通常包含着一组属于一个逻辑模块的信息。分段对用户是可见的，用户编程时需要显示的给段名。 角度2：单位大小 页的大小固定由系统决定 段的长度不固定，决定于用户编写的程序 角度3：地址空间 分页的用户进程地址空间是一维的，程序猿只需要给出一个记忆符即可表示一个地址 分段的用户进程地址空间是二维的，程序猿在标识一个地址时，既要给出段名，也要给出段内地址。 角度4：信息的共享与保护 分段比分页更容易实现信息的共享和保护。不能被修改的代码成为纯代码或可重入代码（不属于临界资源），这样的代码是可以共享的。可修改的代码是不能共享的（比如，有一个代码段中有很多变量，各进程并发地同时访问可能造成数据不一致） 思考：如何使得某个段是临界资源可以被共享？ 很简单，就是使表项的基址指向同一个段即可： 而页不是按照逻辑模块划分的，这就很难实现共享。因为假设一个函数模块被放在了两个页A,B，那么如果想让这个函数模块被共享，那么就需要A,B页都是可以允许共享的，但是如果此时A只有一半存的是函数模块，另一半存的是不允许共享的资源，那么显然现在A就很难实现共享，那么这个函数模块就也不能被共享了，这种冲突就造成了分页很难实现共享，同时风险也就更大。 角度5：访存流程 对于分页存储（单级页表）：第一次访存–查内存中的页表，第二次访存–访问目标内存单元。总共两次访存。 对于分段存储：第一次访存–查内存中的段表，第二次访存–访问目标内存单元。总共两次访存。 并且分页和分段都可以引入快表机构，这样近期访问过得表项再次被访问时就只需要访存一次了。 总结 段页式管理方式 前面我们一直都是讲解的纯分页式管理或者纯段式管理，那么能不能同时集合两者的优点，这就出现了段页式管理方式。 分页、分段的优缺点 存储方式 优点 缺点 分页管理 内存空间利用率高，不会产生外部碎片，只会有少量的页内碎片 不方便按照逻辑模块实现信息的共享和保护 分段管理 很方便按照逻辑模块实现信息的共享和保护 如果段长过大，为其分配很大的连续空间会很不方便。另外，段式管理会产生很大的外部碎片（当然可以使用“紧凑”技术来解决，但是时间开销大） 分段+分页=段页式管理 我们前面说过页可以再分页，但是段不能再分段，但是我们可以用分段后在分页的方式存储，这样逻辑模块功能划分的优点保存的同时也可以将较大的段离散存储。所以首先要知道段页式存储一定是先分段再分页并且是对相同的段分页。如下： 段页式管理的逻辑地址结构 那块此时逻辑地址的结构也是变化的，我们此时会将逻辑地址变为如下： 此时就没有段内偏移量了，“分段”对用户是可见的，程序猿编程时需要显式地给出段号、段内地址。而将各段“分页”对用户是不可见的，系统会根据内地址自动划分页号和页内偏移量来具体存储段的位置。因此段页式管理的地址结构也是二维的。 段号的位数决定了每个进程可以最多被分成几个段，页号位数决定了每个段可以最多被分成几个页，而页内偏移量决定了页面大小和内存块大小（当然如果是多级页表存储，页号还可以分成许多多级页号更复杂）。 例如上图中的结构如果系统是按字节寻址的，那段号占16位因此该系统中，每个进程最多有2^16=64个段，而页号占4位，因此每个段最多被分为2^4=16页，页内偏移量是12位，因此每个页面/内存块大小为2^12=4096=4KB。 段表、页表 显然此时段表和页表也是有变化的，如下图： 此时每个段对应一个段表项，每个段表项由段号，页表长度（实际上就是段长度），页表存放块号（可以算出页表其实地址）组成。每个段表项长度相同，段号是隐含的。所以段表变化很大，页表基本上不变。每个页表对应一个页表项，每个页表项由页号和页面存放的内存块号组成。每个页表项长度相同，页号是隐含的。当然对于页表为了连续存储公式计算方便，最好还是一个页帧可以放入整数个页表项。 地址变换 注意此时一般来说就需要三次访存了，一次查段表，二次查页表，三次访存目标内存单元。当然如果加入了快表，那就只需要一次访存就是访存目标内存单元。 总结 虚拟内存的基本概念 这部分主要谈论内存管理中内存空间的扩充部分。我们之前已经讲过覆盖和交换技术了，那么这次来讲一讲虚拟存储技术（在中级调度中有过应用）。 传统存储管理方式的特征与缺点 所以我们之前讲的都是属于传统存储管理的部分。缺点是长期占用内存，所以可以使用虚拟存储技术解决。在传统的存储管理方式中有以下特点： 一次性：作业必须一次性全部装入内存才能开始运行。这会造成两个问题：一是作业很大时不能全部装入内存导致大作业无法运行，二是当大作业要求运行时，由于内存无法容纳所有作业，因此只能有少量作业能运行，导致多道程序并发度降低。 驻留性：一旦作业被装入内存，就会一直驻留在内存中，直至作业运行结束。事实上，在一个时间段内，只需要访问作业的一小部分就可以正常运行了，这就导致内存中会驻留大量的，暂时用不到的数据，浪费了宝贵的内存资源。 而以上的缺点我们都可以使用虚拟存储技术来解决。 局部性原理 我们之前已经讲过局部性原理了，实际上虚拟存储技术就应用了局部性原理的思想，已知的应用有快表机构就是将常访问的页表项副本放到更快速访问的TLB中，这种高速缓冲技术的思想就是利用了局部性原理，将近期频繁访问到的数据放到更高速的存储器中，暂时用不到的就放在更低速的存储器中。 虚拟内存的定义和特征 基于局部性原理，在程序装入时，可以将程序中很快会用到的部分装入内存，暂时用不到的部分留在外存，就可以让程序开始执行。 在程序执行过程中，当所访问的信息不在内存时，由操作系统负责将所需信息从外存调入内存，然后继续执行程序。（覆盖技术的思想） 如果内存空间不够，由操作系统将内存中暂时用不到的信息换出到外存。（交换技术的思想） 在操作系统的管理下，在用户看来似乎有一个比实大的多的内存，这就是虚拟内存。（操作系统虚拟性的体现） 思考：覆盖技术，交换技术，虚拟存储技术的区别？ 我们可以发现实际上虚拟内存的出现就是虚拟存储技术的具体应用，而且思想貌似就是覆盖交换技术，实际上他们的思路就是一样的，只是作用的单位不同。覆盖技术和交换技术都是对于进程来说的，而虚拟内存是对内存中的数据信息块进行管理。 易混淆知识点： 虚拟内存的最大容量是由计算机的地址结构（CPU寻址范围）确定的 虚拟内存的实际容量=min(内存和外存容量之和，CPU寻址范围) 例如：某计算机地址结构为32位，按字节编址，内存大小为512MB，外存大小为2GB。 那么虚拟内存的最大容量就是2^32B=4GB（虚拟出来的），但是虚拟内存的实际容量就是min(2^32B,512MB+2GB)=2GB+512MB 思考：虚拟内存的特征？ 虚拟内存凭借虚拟存储技术有以下三个特点： 多次性：无需在作业运行时一次性全部装入内存，而是允许分成多次调入内存 对换性：在作业运行时无需一直常驻内存，而是允许在作业运行过程中，将作业换入、换出。 虚拟性：从逻辑上扩充了内存的容量，使用户看到的内存容量远大于实际容量。 虚拟内存技术的应用条件 虚拟内存技术允许一个作业多次调入内存，如果是采用的连续分配方式很明显不方便，所以虚拟内存的实现方式需要建立在离散分配的内存管理方式基础上。 但是区别于传统的离散分配存储管理，虚拟内存不是一次性装入全部，所以才会像之前所说的出现缺页中断等现象，此时就需要请求调入内存了，所以在虚拟内存技术中很明显会频繁的发生&quot;请求&quot;所以在虚拟内存技术下的存储方式叫做 所以主要区别就是在程序执行过程中，当所访问的信息不在内存时，有操作系统负责将所需要的信息从外存调入到内存中，然后继续执行程序。若内存空间不够了，就由操作系统将内存中暂时不需要的信息换出到外存。所以操作系统需提供请求页面功能和页面置换功能，当然后面会讲解页面置换算法决定具体该将那个信息暂时调出内存。 总结"},{"title":"文件系统基础与目录","path":"/wiki/操作系统笔记/文件系统基础与目录/index.html","content":"初识文件管理 那么我们现在要了解一席文件内部的数据的组织方式与文件之间的组织方式。并且思考从下往上看OS是提供哪些服务方便用户、应用程序使用文件，而从上往下看文件数据又要怎么存放到外存（磁盘）的问题。 文件属性（文件构成） 文件名：由创建文件的用户决定文件名，主要是为了方便用户找到文件，同一目录下不允许有重名文件。 标识符：一个系统内的各文件标识符唯一，对用户来说毫无可读性，因此标识符只是操作系统用来区分各文件的一种内部名称（同样的，操作系统为各个进程是通过PID来识别的）。 类型：文件的类型 位置：文件存放的路径（让用户使用），在外存中地址（操作系统使用，对用户不可见） 大小：文件的大小 创建时间，上次修改时间，文件所有者信息等 保护信息：对文件进行保护的访问控制信息 文件内部的组织方式 无结构文件：如文件文件，就是一些由二进制或者字符流组成，又称“流式文件” 有结构文件：如数据库表，由一组相似的记录组成，又称&quot;记录式文件&quot;，这里要注意记录是一组相关数据项的集合，而数据项才是文件系统中最基本的数据单位。 文件之间的组织组织方式 实际上就是通过目录来实现分层，这里的层最好不要太少（这样会造成目录下文件密度过大），同时也不要太多，否则分层太多，搜索查找文件时间开销大。 思考：文件夹是有结构文件吗？ 当然是，用过电脑的都知道文件夹可以以文件项排列各文件的详细信息，很明显每一个文件此时都是一个记录，而每一个文件的具体文件名，大小，创建时间等就是文件夹这一个记录式文件的数据项。 OS向上提供的服务 创建文件：可以“创建文件”，点击新建后，图形化交互进程在背后使用“create系统调用” 删除文件：可以“删除文件”，点击删除操作之后，图形化交互进程通过操作系统提供的“删除文件”功能，即delete系统调用，将文件数据从外存中删除 读文件：将文件数据从外存中读入内存，这样才能让CPU处理，使用的是read系统调用 写文件：将更改过的数据写回外存，使用的是write系统调用 打开文件：读/写之前需要打开文件 关闭文件：读/写文件结束之后需要关闭文件 基本上更多复杂的功能都是通过上面的基本功能组合实现的。 文件如何存放至外存 我们发现其实这个就是分页存储的思想，所以外存中与内存一样也是分成一个一个存储单元然后将文件切割离散存储，当然如果文件特别小，那么一个存储单元就可以放下整个文件，所以明显外存中也会有内部碎片。 思考：这里我们思考是不是外存中也会有连续存储的方式？ 肯定是有的，这里也有固定分区分配的存储思想和分页存储思想两种方式存储文件。 操作系统需要完成的其他文件管理的操作 文件共享：使多个用户共享一个文件 文件保护：保证不同的用户对文件有不同的操作权限 总结 大部分都是概念性知识点，记住即可 文件的逻辑结构 类似于数据结构的“逻辑结构”和“物理结构”，如线性表就是一种逻辑结构，在用户看来，线性表就是一组有先后顺序的元素序列。而线性表这种逻辑结构可以通过许多种物理结构实现，不如顺序表和链表均可以，顺序表是元素在逻辑和物理上都是连续相邻的，而链表是物理上不相邻但是逻辑上相邻的数据结构。所以顺序表的线性表可以随机访问，而链表形式的线性表就不可以了。所以算法的具体实现与逻辑结构和物理结构都有关（文件也是一样，文件操作的具体实现和文件的逻辑结构，物理结构有关) 无结构文件 首先无结构文件前面也提到过了就是“流式文件”是一组二进制或者字符流，所以没有明显的结构，也就不用讨论“逻辑结构”的问题。 有结构文件 前言：记录的分类 记录式文件，每条记录都是由若干个数据项组成的集合。一般来说，每一条记录的一个数据项都是一个关键字（可以作文识别不同记录的ID）。 根据各条记录的长度我们可以将记录分为定长记录和可变长记录： 定长记录：一般每条记录的长度是必须等长的，每一个数据项所在位置也都是相同不变的。 可变长记录：数据项的长度不一定相同因此记录的长度也是各不相同的，甚至某一个数据项没有是可以删掉的即下图中如果无特长可以直接删掉。 顺序文件 文件中的记录一个一个的地顺序排列（逻辑上），记录可以是定长或者变长的。各个记录在物理上可以是顺序存储或者链式存储。 根据记录之间的顺序是否与关键字有关我们分成： 思考：加入现在知道文件的起始位置，那种文件可以快速找到第i个记录的位置？那种文件又可以找到某个关键字对应的记录的位置？ 首先链式存储肯定是不可能实现随机随机存放的，所以每次都需要从头开始查找这样很难快速找到第i个文件的位置。所以快速查找只能在顺序存储中，又由于可变长记录长度不相同不能使用X+i*M的连续查找公式所以也不能和实现快速查找，所以只有定长记录的顺序存储才可以实现，同时只有采用顺序结构即存储的顺序和关键字有关的才可以找到关键字对应的记录的位置。 很明显从上图我们就可以看出可变长记录不能随机存取的原因了，由于长度不同，连续查找公式是不能使用的。定长记录的顺序文件，如果采用物理上的顺序存储那么就可以实现随机存取。如果还能保证记录的顺序结构那么就可以关键字快速检索了。一般上，考试题中的“顺序文件”指的是逻辑结构和物理结构上都是顺序存储的文件。所以顺序文件一般如果不说都是默认定长记录所以可以随机存放，但是缺点是增加/删除一个记录就很复杂，需要整体记录前移或者后移，但是如果是串结构那么就相对简单。 索引文件 对于很多场景都需要快速查找都第i个记录的位置，但是又是可变长记录文件，那么这时就需要索引文件的逻辑结构形式，即建立一张索引表，每个索引表都有唯一的索引号，长度m(毕竟是可变长的需要记录)以及一个指针ptr指向文件再外存中存放的地址，所以索引文件在结构上还顺序的，但是物理结构上是可以离散存储的，当然如果你非得在物理结构上也顺序存储也可以。 其实感觉就是分页存储的思想，只不过那个是讨论内存存放时提出的方法，现在这种思想应用在了文件管理上，实际上思想类似。索引表本身是定长记录的顺序文件（这里指的是物理结构上也顺序存储，否则不能实现随机存取），因此可以快速找到第i个文件的索引项。并且可以在索引表中以关键字作为索引号内容，若按照关键字顺序排列，那么还可以实现按照关键字折半查找。这是我们在尝试删除/增加一个记录时就是对索引表进行修改。因为索引表有很快的检索速度，所以主要用于对信息处理的及时性要求很高的场合。并且，可以用不同的数据项建立很多个索引表，如：学生信息表可以用关键字“学号”“姓名”都各建立一张索引表。这样就可以根据不同的关键字检索文件了。 索引顺序文件 我们思考一个问题，每个记录都会对应一个索引表项，因此索引表可能会非常巨大。比如：文件的每个记录平均只占8B，而每个索引表项占32个文件，那么索引表都要比文件本身还要大4倍，这样就降低了空间利用率。所以提出了索引顺序文件。 我们可以看出索引顺序文件的索引项不需要按关键字顺序排列，这样就极大方便新表项的插入，同时在上图中我们发现学生记录按照学生的姓名开头字母进行分组，每一个分组就是一个顺序文件，分组内的记录不需要按关键字排序。索引顺序文件就是索引文件和顺序文件思想的结合。索引顺序文件同样会为每个文件建立一个索引表，但是不是每一个记录对应一个索引表项，而是每一组数据对应一个索引表项。然后每一组文件中顺序存储，这样就大大瘦身了。 思考：三种文件的区别？ 顺序文件就是顺序存放，那么查找时如果是不定长就只能逐一查找并且顺序存放不好增删所以出现索引文件每一个记录对应索引表一定长个表项，索引表是物理顺序存放那么查找时就可以很快找到并且增删只需要修改索引表项，但是空间会很大毕竟索引表项和文件各占用很大的空间，所以索引顺序文件是将整个文件组按某些标准分成许多组然后为每个组建立一个索引表这样就实现了瘦身。 思考：用这种索引顺序策略能否解决不定长记录的顺序文件检索速度慢的问题？ 我们假设现在有一个10000个记录的顺序文件（不定长记录的物理结构顺序存储的顺序文件），那么如果根据关键字检索文件，只能从头开始顺序查找，平均需要查找5000个记录。 如果是索引文件，虽然表项定长，但是索引文件只是在已知起始地址查找第i个文件时加快了速度，现在是要查找某个关键字的搜索，那么也只能从头开始顺序查找，所以最终也是5000次平均查找。 如果使用的是索引顺序文件结构，可以把10000个记录分成100组每组100个记录，那么先顺序查找索引表找到分组（共100个分组，所以平均需要查找50次）然后找到分组后再在分组中顺序查找记录也是平均需要50次，那么最终只需要100次。很明显确实相较于顺序文件和索引文件有了检索速度的提升。 思考：但是如果现在是10^6的记录怎么办？ 我们发现如果是10^6个记录，那么此时索引顺序文件的查找次数还是很大，所以此时多建几层次级索引表就好了（毕竟每建一层索引表理论上会减少查找没有必要的多组文件），所以此时就是多级索引顺序文件如下： 此时对于一个10^6记录的文件(注意还是可变长记录文件），可以先为该文件建立一张低级索引表，每100个记录为一组，所以总共会有10000个表项，即10000个定长的表项，然后再把这10000个定长记录再次分组为每组100个再为其建立顶级索引表，那么顶级索引表就有100个定长表项。 这里有个公式：N个记录的文件建立K级索引，则最优的分组是每组N^(1/K+1)个记录。这样检索一个记录的平均查找次数是 (N1/K+1/2)∗(K+1)(N^{1/K+1}/2)*(K+1) (N1/K+1/2)∗(K+1) 次，例如上面我们分成了2级，那么每组就是(10^6)^1/3=100个记录，并且平均查找次数就是（100/2)*(2+1)=150次。 总结 文件目录 文件控制块 思考当我们双击照片这个文件夹后OS是如何找到文件夹下的文件和显示到我们屏幕上的呢？ 实际上此时是借助文件控制块实现的，我们双击“照片”后，操作系统会在这个目录表中找到关键字“照片”对应的目录项（也就是记录，毕竟记录就是存得许多不同的数据项也就是关键字），然后从外存中将“照片”目录的信息读入内存，于是“照片”目录中的内容就可以显示出来了。所以我们所说的目录实际上就是一个索引表。 那么目录文件中每一条记录实际上就是一个文件控制(FCB),FCB实现了文件名和文件之间的映射，使得用户(用户程序)可以实现“按名存取”。FCB的有序集合就成为“文件目录”，一个FCB就是一个文件目录项。FCB中包含了一个文件的基本信息，存取控制信息使用信息等。当然最重要的就是文件名和文件存放的物理位置。 那么通过文件控制块FCB我们可以实现哪些功能呢？ 搜索：当用户使用一个文件时，系统根据文件名搜索目录，找到该文件对应的目录项。 创建文件：当创建一个文件时，需要在所属的目录中增加一个记录项。 删除文件：当删除一个文件时，需要在目录中删除相应的目录项。 显示目录：用户可以请求显示目录的内容，如显示该目录中的所有文件及相应的属性。 修改目录：某些文件属性保存在目录中，因为这些属性变化时需要修改相应的目录项（如：文件重命名等）。 目录结构 单机目录结构 早期的操作系统不支持多级目录，所以整个系统只建立一张目录表，每个文件占据一个目录项，单机目录实现“按名存取”，不允许有任何文件重名。在创建一个文件时，需要先检查目录中有没有重名文件，只有确定不重名后才能建立文件，并将新文件对应的目录项插入到目录表中。 很显然，不适合多用户(这里的用户值得是多程序,应用软件)操作系统。想一想也知道现在的应用程序肯定都有许多重名文件例如：data,dist.source,js等 两级目录结构 早期的多用户操作系统采用两级目录结构，分为主目录(MFD,Master File Directory)和用户文件目录(UFD,User File Directory)。 此时就允许有重名文件了只要不在一个FCB下，但是此时还是不够灵活，毕竟用户文件夹可能也需要有自己的目录，所以就有了多级目录结构。 多级目录结构 又称树形目录结构，灵活高效，解决了上面两种目录结构的缺陷。 此时因为有许多可能重名的文件但是他们所在的位置是不同的，所以要访问某个文件时要用文件路径标识文件，文件路径是一个字符串。各级目录之间用&quot;/&quot;隔开，从根目录出发的就是绝对路径，从当前位置或者当前位置的父目录出发就是相对路径。例如：./就是相对路径表示从现在的位置出发，…/是从当前位置的父文件开始出发明显也是相对路径，但是…/或者/就是根目录出发就是绝对路径了。 思考：为什么要用相对路径？ 考虑一个问题现在我们要对某个文件夹下的许多文件进行操作，那么如果使用绝对路径，那么每一次都要输入很长的根目录路径明显低效，而如果此时使用相对路径./就很方便简洁。同时不仅仅是为了输入简便，如果我们使用绝对路径，那每一次都需要从根目录开始逐一从外存读入对应的目录表，然后在找到该文件夹下的文件，而是用相对路径就可以直接一次性将这个文件夹读入内存然后访问文件。可以见到，在引入“当前目录”和“相对路径”以后，磁盘的I/O次数减少了，这样就提升了访问文件的效率。所以相对路径是常用的文件路径方式，当然对于某些特殊的情况是必须使用绝对路径的。 所以树形目录结构不仅可以很方便的对文件进行分类，层次结构清晰，而且也能够更加有效的进行文件的管理和保护。但是树形结构不便于实现文件的共享，所以提出了“无环图目录结构”。 无环图目录结构 可以用不同的文件名指向一个文件，甚至可以指向同一个目录（共享一个目录下的所有内容）。需要为每一个共享节点设置一个共享计数器，用于记录此时有多少个地方在共享该节点。用户提出删除节点的请求时，只是删除该用户FCB、并且使共享计数器减1，并不会直接删除共享节点。直至共享计数器为0时，才删除节点。 我们发现共享文件不是赋值文件。在共享文件中，由于各用户指向的都是同一个文件，因此只要其中一个用户修改了文件数据，其他所有用户都可以看到文件数据的变化。 FCB的改进 我们之前进行的瘦身策略，实际上是对目录项进行分组然后多级索引表的存储，但是对于同一个目录下的目录项最好是对应一个目录表，那么该怎样实现瘦身呢？我们可以对FCB进行修改，毕竟查找时只是按照“文件名”进行查找，只有文件名匹配才能读出文件的信息，所以可以考虑让目录表瘦身吗，如下： 瘦身前： 瘦身后： 思考：好处是什么？ 假设一个FCB是64B，磁盘块大小为1KB=2^10B，那么每个盘块只能存放16个FCB，如果一个文件目录中共有640个目录项，那么需要占640/16=40个盘块。因此按照某文件名检索目录平均需要查找320个目录项，平均需要启动磁盘20次（每次磁盘I/O读入一块）。但是如果瘦身后即使用索引节点机制那么文件名占14B，索引节点指针占2B，一个FCB只占用16B，那么一个盘就可以放64个目录项，那么按文件名目录平均只需要读入320/64=5个磁盘块。显然这就大大提升了文件检索速度。 只有找到文件名对应的目录项才会将索引节点放到内存中，索引节点中记录了文件的各种信息，包括在文件再外存中的位置，根据“存放位置”即可找到文件。存放在外存中的索引节点就叫做“磁盘索引节点”而当索引节点放入到内存后就称为“内存索引节点”。相比之下内存索引节点需要增加一些信息，比如：文件是否被修改，此时有几个进程正在访问该文件等。 总结"},{"title":"早期分配管理方式","path":"/wiki/操作系统笔记/早期分配管理方式/index.html","content":"连续分配管理方式 这里我们讲一讲内存空间的分配方式，分为连续分配管理方式和非连续分配管理方式，我们首先学习连续分配管理方式。 连续分配顾名思义就是为用户进程分配的必须是一个连续的内存空间，这里有三种连续分配方式如下： 单一连续分配 在单一连续分配方式中，内存被分为系统区和用户区。系统区通常位于内存的低地址部分，用于存放操作系统相关数据，而用户区存放用户进程相关数据。内存只有一道用户程序，用户程序独占整个用户区空间。 这种方式优点是实现简单，没有外部碎片，可以采用覆盖技术扩充内存，不一定需要采取内存保护（例如早期的PC操作系统MS-DOS）。但是缺点是只能用于单用户，单任务的操作系统中（这显然不适合并发进程），并且有内部碎片，存储器利用率极低。 思考：什么是内、外部碎片，有什么区别？ 我们先给出定义： 外部碎片是还没有被分配出去（不属于任何进程），但由于太小了无法分配给申请内存空间的新进程的内存空闲区域。外部碎片是处于任何已分配区域或页面外部的空闲存储块。这些存储块的总和可以满足当前申请的长度要求，但是由于它们的地址不连续或者其他原因，使得系统无法满足当前申请。多道可变连续分配只有外部碎片。 内部碎片就是已被分配出去（能明确指出属于哪个进程）却不能被利用的内存空间。内部碎片是处于区域内部或页面内部的存储块。占有这些区域或页面的进程不能使用这个存储块。而在进程占有这块存储块时，系统无法利用它。知道进程释放他，或进程结束，系统才有可能利用这个存储块。 所以内外碎片本质区别就是是否已经属于某个被分配的进程。并且外部碎片在操作系统的调整下可以消除，而内部碎片操作系统无权调配管理，只能等待被分配进程结束。 这里我们举一个例子来形象介绍，假设现在有1,2,3,4,5,6六个仓库，当1,2,3,4,5已填满后，4清仓了，那么此时空仓库有4,6，此时来了一批货物大小为2个仓库刚好满足4,6容量之和，但是货物要去连续存储，此时4,6无法使用并且4,6不属于任何一个货物所以4,6形成的就是外部碎片。而现在假设六个仓库都是空的，并且要求货物最小分配空间为间，所以一个仓库只能装一种货物，那么现在有一批2.5间容量的货物装载1-3号仓库，那么3号仓库只装了半间，但是此时再有别的货物也不能装入3号仓库并且此时3号仓库属于第一批货物，并且管理员也不能调整再使用3号仓库了，那么3号仓库剩余的空部分就是内部碎片。–大佬博客 按照上面的介绍我们知道了单一连续分配不会产生外部碎片的原因了，并且这种分配管理方式不用想也知道肯定会产生很大的内部碎片，所以不合理。 固定分区分配 20世纪60年代支持多道程序的系统出现后，为了能在内存中装入多道程序并且这些程序之前互不干扰，于是将整个用户控件划分为若干个固定大小的分区，在每个分区中只装入一道作业，这样就形成了最早的。最简单的一种可运行多道程序的内存管理方式，但是显然这种固定大小分区的分配方式很不合理，于是又出现了分区大小不等的分配方式。 分区大小相等：缺乏灵活性，但是很适用于一台计算机控制多个相同对象的场合（比如：钢铁厂有n个相同的炼钢炉，就可以把内存分为n个大小相等的区域存放n个炼钢炉控制程序） 分区大小不等：增加了灵活性，可以满足不同大小的进程需求，根据常在系统中运行的作业大小情况进行划分（比如：划分多个小分区，适量中等区，少量大分区）。 思考：那我们如何知道某个分区i的具体大小是多少？ 所以我们需要建立一个数据结构–分区说明表来实现各个分区的分配与回收。每个表项对应一个分区，通常按分区大小排列。每个表项包括对应分区的大小，起始地址，状态（是否已分配）。如下： 当用户程序要装入内存时，由操作系统内核程序根据用户程序的大小检索检索表，从中找到一个满足大小的，未分配的分区，将之分配给该程序，然后修改状态为“已分配”。 这种固定分区分配优点是实现简单，无外部碎片（因为可以调整消除）缺点是当用户程序太大时，可能所有的分区都不能满足需求，此时就不得不采取覆盖技术来解决但是这又会降低性能，并且这种方法肯定是会产生内部碎片的，内存利用率低（无论是大小固定还是大小不等的分区分配方式）。 动态分区分配 动态分区分配又称为可变分区分配，这种分配方式不会预先划分内存分区，而是在进程装入内存时，根据进程的大小动态地建立分区，并使分区的大小正好适合进程的需要。因此系统分区的大小和数目是可变的，（比如：假设某计算机的内存大小为64MB，系统区8MB，用户区56MB…） 很明显这种方法肯定是没有内部碎片的，解决了固定分区分配的缺陷。 思考：系统需要用什么样的数据结构记录内存的使用情况？ 其实和固定分区分配中分区不等大小的方法一样，也是建立表或者链来记录呗，如下： 当然名字还是得换一换的，实际上思路是异曲同工的。一定要注意此时只需要记录空闲连续分配区间的大小和起始地址就可以了。。 思考：当很多个空闲分区都能满足需求时选择哪个分区进行分配？ 如在上图中的空闲区域情况时，又来了一个进程5（之所以没有进程1,2,3可能是挂起或者运行完成释放了）大小为4MB，此时所有分区都满足，那么如何分配呢？ 这里我们有以下几种情况（后面会细讲）： 用最大的分区进行分配 用最小的分区进行分配（比较符合正常思维） 用地址最低的部分进行分配 把一个新作业装入内存时，须按照一定的动态分区分配算法，从空闲分区表（或空闲分区链）中选出一个分区分配给该作业。由于分配算法对系统性能会有很大的影响，因此人们研究后发明了几种动态分区分配算法（后面讲）。 思考：如何进行分区的分配和回收？ 空闲分区分配 首先看分配，这个简单，如果分配后某个分区刚刚好被占据了那么这个表项直接删除就好了。如下图： 5号进程放在了3号空闲分区刚好占满，那么删除3号表项即可。 如果空闲分区分配一定空间后没有被占满，那么就要更新分区大小和起始地址了，如下图： 5号进程放在了1号分区并且从1号分区的头部开始放，那么1号分区并未占满，此时就要更新1号分区，起始地址+4变为12同时大小-4变为16。 分区回收 分区的回收涉及的问题较多，这里我们逐一讨论 情况1：回收区的后面有一个相邻的空闲分区 例如下图： 此时要回收进程4了，那么很明显回收区后面相邻连着空闲分区1，那么此时只需要将后面的空闲分区更新起始地址-4同时分区大小+4就好。这个是两个相邻的空闲分区的合并。 情况2：回收区的前面有一个相邻的空闲分区 例如下图： 进程3完成后回收，此时回收区与前面的2号空闲分区所以合并更新2号分区的分区大小+18=28即可，起始地址此时是不需要变得。 情况3：回收区的前、后各有一个相邻的空闲分区 如下图： 此时比较复杂，需要将回收区和后相邻分配去全部加入到前相邻分配区，此时如上面的进程4被回收，那么回收区+后相邻空闲区的总大小为14都加到前相邻空闲分配区1中，所以此时1号空闲分配区大小为34，同时起始地址还是不变，然后还要将后相邻空闲分区删除表项，即2号分区变为了原先的3号分区。 情况4：回收区的前、后都没有相邻的空闲分区 如下图： 我们可以看到此时如果进程2回收，那么新的回收区就成为了一个新的最靠近低地址的空闲分区，所以加入一个新表项这里是1号分区大小为进程大小14，同时起始地址是28。 我们可以看出动态分区不会有内部碎片，但是会有外部碎片。如果内存中空闲空间的总和本来可以满足某进程的需求，但由于进程需要的是一部分连续的内存空间，因此这些“碎片”不能满足进程的需求，可以通过紧凑（拼凑，Compaction)技术来解决外部碎片。紧凑技术就是操作系统不时地对进程进行移动和整理来达到将几个外部碎片凑成一片连续的分区这需要动态重定位寄存器的支持。 总结 动态分区分配算法 接着前面的思考问题，我们来思考一下对于具有许多个空闲分区都可以容下进程数据时我们应该使用哪种策略选取分区。 首次适应算法 顾名思义，该算法的思想就是每次都从低地址开始查找，找到第一个能满足大小的空闲分区。实现方法就是空闲分区以地址递增的次序排列，每次分配内存时顺序查找空闲分区链（或空闲分区表），找到大小满足要求的第一个空闲分区。如下图： 那么很明显5号进程会放在1号空闲分区，6号进程会放到2号分区。这种算法没有什么问题，但是缺陷是可能前面有一个非常大的空闲分区可以放入很多很小的进程数据，但是后面有刚刚好可以放进该进程的大小的空闲分区，这样大的空闲分区会优先被使用，最终造成许多小的空闲分区，再来大的进程就放不下了。同时会导致许多小的空闲分区在低地址处排列，每次分配查找还要再经过这些分区，增加了查找的开销。 最佳适应算法 算法思想是由于动态分区分配是一种连续分配方式，为各进程分配的空间必须是连续的一整片区域。因为为了保证当大进程到来时也能有连续的大片空间，可以尽可能多地留下大片的空闲区，所以优先使用更小的空闲区，这样就弥补了首次适应算法的缺点。实现方法是空闲分区按容量递增次序链接，每次分配内存时顺序查找空闲分区链（或空闲分区表）找到大小能满足要求的第一个空闲分区也就是最小的可以容纳该进程的空闲分区（符合人类思维，尽可能不浪费的多放） 加入现在有一个进程6那么显然要放到2号分区这样2号分区就只剩下1MB了就要同时更新到表或链的最前面。我们发现这种方法的缺陷是每次都选更小的分区放最后只会导致很多外部碎片。 最坏适应算法 又称最大适应算法，就是为了解决最佳适应算法的问题而产生的，为了避免留下太多的外部碎片，优先使用最大的连续空闲区，这样分配后剩下的空闲区就不会太小，更方便使用。实现也很简单就是按照容量递减次序排列，每次分配也是顺序查找找到大小能满足要求的第一个空闲分区。 其实我们发现对于首次适应算法如果恰巧大的分区在前面，小的分区在后面，那么实际上就和最适应算法你一样了，所以最坏适应算法的缺陷就是大分区快速被消耗，再来大进程放不下了。 邻近适应算法 弥补首次适应算法的查找开销大的缺陷，这个算法思想是每次都从上次查找结束的位置开始向两侧检索就能解决上述问题，哪侧先找到大小合适的就放下该进程，所以是双向链表。 首次适应算法每次都要从头查找，每次都必须需要先检索低地址的小分区。但是这种规则决定了当低地址有更小的分区可以满足需求时，会更有可能用到低地址部分的小分区，也会有可能把高地址部分的大分区空闲出来，所以首次适应算法有可能会出现最坏适应算法的缺点即外部碎片多但同时也可能出现最佳适应算法的优点即合理利用空间。 而邻近适应算法的规则可能导致无论低地址，高地址部分的空闲分区都有相同的概率被使用，也就导致了高地址部分的大小分区更可能被使用，划分为许多小分区，最后导致无大分区可用，所以综合四种算法来看，首次适应算法的效果反而更好。 思考：四种分配算法的异同？ 算法 算法思想 分区排列顺序 优点 缺点 首次适应 从头到尾找合适的分区 空闲分区以地址递增次序排列 综合性能最好。算法开销小，回收分区后一般不需要对空闲分区队列重新排列 可能会出现低地址处许多非常小的空闲分区加大查找开销 最佳使用 优先使用更小的分区以保留更大的分区 空闲分区以容量递增次序排列 会有更多的大分区被保留下来，更能满足大进程的需求 会产生很多太小的、难以利用的碎片导致查找算法开销大，回收分区后可能需要对空闲分区队列重新排序 最坏适应 优先使用更大的分区，以防止产生太小的不可用的碎片 空闲分区以容量递减次序排列 可以减少难以利用的小碎片（外部碎片） 大分区容易被用完，不利于大进程，算法开销大（最后也会造成许多小分区导致查找开销大） 邻近适应 由首次适应算法演变而来，每次从上次查找结束为止开始查找 空闲分区以地址递增次序排列（可排列成循环链表） 不用每次都从低地址的小分区开始检索，算法开销小 会使高地址的大分区也被用完 基本分页存储管理的基本概念 我们先看一个图： 我们可以看出此节我们就开始讲解非连续分配管理方式了，这里只是一种一个小部分而已。非连续分配就是为用户进程分配的可以是一些分散的内存空间。 地址空间 我们先回忆一下什么是地址空间，我们知道地址分为两种逻辑地址（相对地址）和物理地址（绝对地址）两者有一定的映射关系。 但是我们发现这种存储只能连续存储，这很不方便，所以引出了分页存储的概念。 分页存储 我们将内存空间分为一个个大小相等的分区（比如每个分区为4KB），那么每个内存分区就是一个“页框”（页框=页帧=物理块=物理页面）。每个页框都有一个编号，即“页框号“（页框号=页帧号=内存块号=物理块号=物理页号），页框从0开始编号。 将进程的逻辑地址空间也分为与页框大小相等的一个个部分，每个部分称为一个“页”或“页面”。每个页面也有一个编号，即“页号”，页号也是从0开始。 操作系统以页框为单位为各个进程分配内存空间。进程的每个页面分别放入一个页框中。也就是说，进程的页面与内存的页框有一一对应的关系。各个页面不必连续存放，可以放到不相邻的各个页框中。 注意进程的最后一个页面可能没有一个页框那么大，也就是说分页存储有可能产生内部碎片，因此页框不能设置的太大，否则可能产生过大的内部碎片造成浪费。 思考：固定分区分配（分区大小相等）和分页存储的区别？ 我们第一想法一定是这个和分区大小相等的固定分区分配好像，实际上思路就是差不多的，只不过是前者是一个小空间里放一个作业，所以作业/进程还是连续分配的，并且分区大小无论是多大都有可能会放不下更大的进程并且内部碎片会很大，而现在分页存储类似于将作业分块离散存储在许多的小分区中，所以作业/进程本身是不连续的且相应的无论作业/进程多大理论上都可以放下（只不过是会分成许多页面存储在许多页框中)并且内部碎片在一定的页框大小时是很小可控的。 思考：那么我们怎么知道每个页面存放在了内存的那个页框中呢？ 所以我们同样需要建表来说明–页表 页表 每个进程都会被分为许多页面存放在许多同样数量的页框中，所以每个进程都要有一张的页表，所以页面一般存放在PCB中，所以页表会一直在内存中（毕竟PCB是一直在内存中）直至该进程销毁。 所以一个进程对应一个页表，进程的每个页面对应一个页表项，每个页表项由“页号”和“块号”组成。页表记录着进程页面和实际存放的页框之间的映射关系。每个页表项的长度是相同的。 思考：每个页表项多大？占几个字节？ 首先我们要走出误区，表项的个数只是和页面页框数量一样，但是大小不同，页框大小和页表表项没有直接关系，我们先看一下两者的区别。页框是存储数据的单位，而页表表项是存储页框号和页面号的单位。所以页框大小是人为划分的，但是一旦内存大小和页框大小确定了，那么页表表项也就确定了。比如下题： 假设某个系统物理内存大小为4GB，页面大小是4KB，则每个页表项至少应该为多少字节？ 首先内存大小为4GB也就是2^32字节，页面大小实际上等于页框大小，所以页面页框的大小都是4KB=2^12字节，所以一共可以有2^32/2^12=2^20个页框(页面当然也就是2^20个），编号为0~2^20-1，所以一共会有2^20个页表项并且编号至少需要20bit来表示，又因为一个字节为8bit,多以至少需要3个字节来表示。所以一个页表项为3B，一个页框或页面为4KB。 并且页表项肯定是连续的，假设页表中的页表项从地址为X的地方开始连续存放，那么第i号页表项的地址就是X+3*i,同时第i个页面的存储地址就是第i号页表项中所对应的块号。并且我们发现页号是隐含的，所以页号不占用存储空间，一个页表项所占空间就是块号所占的空间，页号就好像数组的键值一样是隐含不占用空间的。 并且还要注意，块号记录的不是页框的起始地址，而只是页框号，又因为0号页框号的起始地址就是0，所以j号内存块的起始地址就是j*内存块大小。 思考：如何实现地址的转换？ 我们回忆一下在连续存储时，操作系统有三种策略实现地址转换（绝对装入，静态重定位，动态重定位)。现在是分页离散存储，我们如何找到逻辑地址所对应的物理地址呢？我们知道虽然各个页面是离散存放的，但是页面内部是连续存放的。 所以如果想要访问逻辑地址A需要以下步骤： 确定逻辑地址A对应的“页号”P 找到P号页面在内存中的起始地址（需要查页表找到内存块号j,起始地址就是j*内存块大小) 确定逻辑地址A的“页内偏移量”W 逻辑地址A的物理地址=P号页面在内存中的起始地址+页内偏移量W逻辑地址A的物理地址=P号页面在内存中的起始地址+页内偏移量W 逻辑地址A的物理地址=P号页面在内存中的起始地址+页内偏移量W 思考：如何确定逻辑地址对应的页号和页内偏移量？ 我们以一道例题来讲解：假设在某个计算机系统中，页面大小时50B，某进程逻辑地址空间大小为200B，则逻辑地址110对应的页号、页内偏移量是多少？ 页号=逻辑地址/页面长度（取除法的整数部分） 页内偏移量=逻辑地址%页面长度（取除法的余数部分） 所以上面的题页面号=110/50=2，页内偏移量=110%50=10，所以逻辑地址可以拆分为页号和页内偏移量来表示。接下来我们去页表中寻找页号2所对应的块号j,那么物理地址就是j*50+10。这里我们是用十进制表示的，但是我们知道对于计算机来说，他的操作都是二进制串进行操作，现在我们还是按照这个思路来看一下二进制的表示： 在计算机内部，地址用二进制表示，如果页面大小刚好是2的整数幂，则计算机硬件可以很快就把逻辑地址拆分成页号和页内偏移量，这样自然转换到物理地址也就更快了。如果是2的整数幂，那么假设每个页面的大小为2^K字节，那么用二进制表示逻辑地址时，逻辑地址01串的末尾K为就是页内偏移量，其余部分就是页号。如下： 我们可以看出页面大小为2^12B，所以末尾12位就是黑色部分就是页内偏移量，同时红色的20位就是页号了。 这样我们可以就轻松的对逻辑地址进行拆分转换成物理地址了。又因为内存块的大小=页面大小，且块的起始地址就是页内偏移量为0的地址，所以各个块的地址可以表示为： 同样对于物理地址，假设现在我们通过查询页表得到1号页面存放在了9（1001）号内存块，那么 我们发现前面红色部分就是9的二进制串即内存块号 思考：二进制串中物理地址和逻辑地址的异同点？ 我们仔细观察发现逻辑地址和物理地址的表示公式都是如下（前提：页面大小是2的整数幂）： 逻辑地址=页面号+页内偏移量逻辑地址=页面号+页内偏移量 逻辑地址=页面号+页内偏移量 物理地址=页框号+页内偏移量物理地址=页框号+页内偏移量 物理地址=页框号+页内偏移量 所以假设现在页面大小为4KB=2^12B=4096B。那么4097的页号就是1，页内偏移量为1，所以逻辑地址二进制串为 并且通过查表得知1号页面存放在9号页框，那么物理地址就是 总结：页面大小刚好是2的整数幂的好处就是 逻辑地址的拆分更加迅速–如果每个页面大小为2^KB，用二进制表示逻辑地址，则末尾K为就是页内偏移量，其余部分就是页号。因此，如果让每个页面的大小为2的整数幂，计算机硬件就可以很方便地得出一个逻辑地址对应的页号和页内偏移量，而无需进行除法操作，从而提升了运行速度。 物理地址的计算更加迅速–根据逻辑地址得到的页号，查询页表找到对应存放的页框号，将二进制表示的内存块号和页内偏移量拼接起来，就可以得到最终的物理地址。 逻辑地址结构 实际上通过上面的例题我们已经掌握了逻辑地址的结构和应用，这里再给出严格定义，分页存储管理的逻辑地址结构如下图： 地址结构包括两个部分：前一部分为页号P，后一部分为页内偏移量W。在上图所示的例子中，地址为32位，其中0~11号为“页内偏移量”，或称“页内地址”，12~31位为“页号”。 如果有K位表示“页内偏移量”，则说明该系统中一个页面的大小是2^K个内存单元。如果有M位表示“页号”，则说明在该系统中，一个进程最多允许有2^M个页面。所以页面大小&lt;–&gt;页内偏移量位数。 思考：页面大小一般设为什么数比较好？ 当然就是2的整数幂啦，因为这样地址转换快，这也是现代操作系统大多的做法。当然考研的题中有些奇葩题（为了考而考）会出现页面大小不是2的整数幂的情况，那就只能按照最原始的公式计算了页号=逻辑地址/页面长度（取除法的整数部分），页内偏移量=逻辑地址%页面长度（取除法的余数部分）。 思考：从上面的结构中我们能否看出一些页号和页内偏移量的规律？ 我们可以易知页号简介反映了页框的数量，页内偏移量简介反应了一个页框的大小。那么当一个页框很大时，页内偏移量也就大，K值也就大，那么所占的地址位数也就多，那么相应的页号所占位数32-K也就越小代表着此时页号就下了，也就说明页框数量变少了，其实这很正常，毕竟空间就那么大，一个存储单元变大了，相应的存储单元数量自然就少了。 总结"},{"title":"操作系统的运行环境","path":"/wiki/操作系统笔记/操作系统的运行环境/index.html","content":"操作系统的运行机制 预备知识 在学习之前，我们先来回忆一下程序是如何运行的。首先指令是指处理器cpu可以识别、执行的最基本命令。在生活中，很多人习惯将Linux,Windows,MacOS的小黑窗中的命令也称为“指令”，实际上这些是“交互式命令接口”，与本节的“指令”不同，本节中的“指令”是指硬件层机器所能识别的二进制指令（即01串）。 一条高级指令如C,JAVA等都会首先通过编译器翻译为机器能够读懂的二进制指令然后才能被硬件机器识别和执行。高级语言逻辑复杂更符合人类思维，而二进制指令则更对机器的执行友善，简单地01交并补就可以实现高级语言。但是相对应的指令长度和数量也就更多，所以一条高级语言的代码可能会翻译出许多条对应的机器指令（举个例子，实际上通过二进制指令和操作系统的代码实现输出函数printf就已经对于机器来说是一个非常复杂高级的指令了，如果你做过nemu的话会深有体悟）。而cpu就是一条一条的执行二进制指令，当然执行的速度非常快。 内核程序和应用程序 这两个程序有本质上的区别，对于应用程序我们再熟悉不过，普通程序猿写的程序大多都是应用程序，其大部分都是应用于软件层，最终运行在操作系统上。而例如微软、苹果、华为等一些顶级大牛会负责实现操作系统，如果你还记得上节的内容，应该知道操作系统本质上也是一个软件，只是他是连接软件层和硬件层的中间层。这些很多内核程序组成的“操作系统内核”，又叫做内核（kernel)，内核是操作系统最重要的核心部分，也是最接近硬件的部分，可以说，一个操作系统只要有了内核基本上就够了例如Docke仅需要Linux内核，操作系统的内核是实现核心功能的部分，未必拥有操作系统的全部功能，录入图形化接口GUI就不在内核中实现。 特权指令和非特权指令 应用程序使用的都是“非特权指令”，例如加法指令，减法指令等，而操作系统内核作为管理者，就有权有时让cpu执行特权指令，如：内存清零指令，这些指令影响重大，一般会直接影响到操作系统，硬件上的工作，只能由“管理者”–操作系统内核使用。归根对比，应用程序使用的非特权指令权利很小，无权或者不能对操作系统和硬件层产生直接影响，并且一定是需要经过操作系统才能间接使用接口来和硬件层产生关联，而特权指令就是直接更改操作系统代码或者硬件层调度配合工作的代码，不可能暴露给外界以防产生恶意程序入侵破坏设备。在设计cpu时会划分特权指令和非特权指令，因此cpu可以执行一条指令前判断出指令的类型。 内核态和用户态 cpu有两种状态：内核态和用户态。处于内核态时，说明此时正在运行的是内核程序，此时cpu可以执行特权指令（注意是可以，也就是说此时还可以继续执行非特权指令）。而当处于用户态时，说明此时运行的是应用程序，此时只能执行非特权指令。这里用到了一个特殊地寄存器来存储程序状态–程序状态字寄存器（PSW），其中有个二进制位，1表示“内核态”，0表示“用户态”，这样cpu就可以随时判断出此时处于什么状态下。 当然这里有许多别名： 内核态=核心态=管态（即管理状态） 用户态=目态（即只能观看状态） 那么你一定会好奇仅仅用一个PSW就来判断cpu处于什么状态是否过于草率，那么只要更改这个位，岂不是可以按照人为意愿随意更改状态，更可怕的是如果有黑客此时病毒植入，更改了cpu状态然后执行了格式化等指令将系统破坏掉会造成很大的安全隐患，所以这里会有异常中断来避免这种情况，所以PSW不能随意更改，只能由特权指令更改。 这里有一个故事来描述这种情况的应急措施：首先，设备刚刚开机后首先会使cpu处于管态，此时操作系统内核程序先上cpu运行（原因是应用程序需要在操作系统上运行，所以操作系统需要先做准备工作提供接口环境），开机完成后，用户启动某个应用程序，待操作系统内核在合适的时候（准备工作完成）主动（此时处于管态，运行特权指令更改PSW是可以的）让出cpu,让该程序上cpu放入内存后上cpu执行，此时应用程序运行在目态，只能执行非特权指令，当此时有黑客在应用程序中植入一条特权指令（更改PSW）时，企图破坏系统，cpu此时在目态发现要执行的是特权指令（更改PSW）时发现自己是用户态时就会触发异常中断，此时操作系统发现中断信号会立刻夺回cpu的控制权以防有非法指令破坏系统，然后对引发中断的事件进行处理，处理完后在cpu使用权交给应用程序并且此时再次切回目态，这样就保护了系统不会受到入侵破坏了。 思考：内核态和用户态怎样切换 内核态-&gt;用户态：刚刚上面已经讲过了，当cpu处于内核态时可以执行一条特权指令修改PSW的标志位来实现主动切换到目态，这个动作意味着操作系统主动让出cpu的使用权。 用户态-&gt;内核态：任何情况下都不可能通过指令切换回管态，因为PSW只能通过特权指令更改判断位，而此时目态下cpu无权执行特权指令，但是可以通过中断信号引发，硬件自动完成变态过程，触发中断信号意味着操作系统将强行夺取cpu的使用权，因此除了非法使用特权指令以外，还会有许多事件触发中断信号，从而由目态切换到管态，但有一个共性是，但凡需要操作系统介入的地方，都会触发中断信号。 总结 操作系统内核 说了那么多，那么操作系统内核到底是什么，其实内核就是计算机上配置的底层软件，是操作系统最基本，最核心的部分，实现操作系统内核功能的那些程序就是内核程序。如下图： 那么哪些是不属于内核的操作系统的功能呢？例如记事本、任务管理器等设备自带的传说中免费的赠品软件APP，即使没有这些软件，我们仍然可以使用计算机。当然，这些非内核的功能用来推销也是不错哦😹：放松时刻 当然不同厂商对于内核的定义也不同，这里又对内核进行了细分：大内核和微内核。 操作系统的结构和企业的管理问题很相似，内核就是企业的管理层，负责一些重要的核心工作，只有管理层才能执行特权指令，普通员工就只能执行非特权指令。管态和目态之间的切换就相当于普通员工和管理层之间的工作交换。 大内核：企业初创时体量不大，人人都有官，人人皆高层，所以管理层的人会负责大部分的事情，有点事效率高，缺点就是组织结构混乱，难以维护。 微内核：随着企业的体量增大，管理层只负责最核心的一些工作，有点事结构清晰，方便维护，缺点是效率低。 中断和异常 中断的作用 cpu上会运行两种程序，一种是操作系统内核程序（是整个操作系统的管理者），另一种就是应用程序。前面已经基本上知道了中断实际上就是会使cpu由用户态变为内核态，使操作系统重新夺回对cpu的控制权。 在合适的情况操作系统内核会把cpu的使用权主动让给应用程序，而中断就是让操作系统内核夺回cpu使用权的唯一途径。如果没有中断机制，那么一旦cpu开始运行某个应用程序，cpu就会一直运行这个应用程序，那么又何来的并发性呢，所以当切换cpu上的应用程序时就是需要中断信息，使操作系统重新掌权，将cpu使用权让给其他的应用程序，所以中断保证了并发性。 中断的类型 所以中断切换状态是很常见的一种方法，那么根据不同触发的触发中断的情况我们可以分为两类–内中断和外中断。 内中断 内中断与当前执行的指令有关，一般来自cpu的内部，比如发现cpu执行了特殊的特权指令造成的异常或者除0出现计算异常等都是执行的指令自身引发的，这种就成为内中断。当然也不一定指令是出现错误才触发中断，比如应用程序想请求操作系统内核的服务时，此时会执行一个特殊的指令–陷入指令（在Nemu实验中也有，为trap()），此时该指令就会引发一个内部中断信号，也是内中断的一种，这种陷入指令虽然会触发中断，但是此动作意味着应用程序主动的将cpu控制权还给操作系统内核，系统调用就是通过陷入指令完成的。 外中断 外中断与当前的执行无关，不是当前指令引起的中断信号，所以自然不是来自于cpu内部，而是通过内核中某些算法（这些算法来实现任务间合理调度）引起的中断信号。比如内核中的时钟中断，它是由时钟部件发来的中断信号或者是IO设备发起的任务完成的中断信号。 例如时钟算法是用来分配调度任务之间的占用cpu的时间的，我们从上图可以看出时钟计时每50ms会发一个中断信号给cpu,而cpu每次执行完一条指令后都会例行检查是否有外中断信号。当检测到外中断信号时，就会由目态切换到管态。所以回忆之前的知识，可以猜出单批道操作系统的并发性实现即每隔一个时间片切换任务就是通过时钟算法外中断信号引起的。 中断分类的总结 经过上面两个的对比，我们可以看出外中断更符合我们广义上所说的中断，而内中断更多的像是故障，异常终止或者主动陷入，所以大多数的教材和讲义上中断都是特指的外中断，内中断一般称为异常。 中断机制的基本原理 那么对于不同的中断信号，如何知道该进行什么相应操作呢？这时cpu检测到中断信号后，会根据中断信号的类型去查询“中断向量表”，以此来找到相应的中断处理程序在内存中的存放位置。 总结 系统调用 什么是系统调用 我们在前面已经学到了操作系统作为用户和计算机硬件之间的接口，需要向上提供一些简单易用的服务，主要包括命令接口和程序接口。其中程序接口就是由系统调用组成的。例如C库函数中的system()就是一种库函数方法，需要通过程序接口实现。 一般系统调用是操作系统提供给程序猿等编程开发人员使用的接口，可以理解为一种可供应用程序调用的特殊函数，应用程序可以通过系统调用来获得操作系统内核的服务。 思考：系统调用和库函数的区别？ 通过上图我们可以看出应用程序一般是直接通过系统调用来请求内核服务的，当然也有一部分是调用库函数时，库函数中需要系统调用来请求内核服务。但是并不是所有的库函数都需要系统调用的，比如“取绝对值”的函数sqrt()只是一个数学操作函数，虽然需要引入cmath库，但是是不需要系统调用的。而“创建一个新文件”局就涉及到了系统调用的库函数。所以一般来说，普通用户是不会手动触发系统调用的，只有编程人员调用库函数和应用程序可能会触发系统调用。 系统调用的必要性 那么为什么要有系统调用呢，即为什么程序需要每次都向内核发送服务请求呢而不是直接自己执行呢？这就涉及到了操作系统的自身的功能–协调分配任务，管理资源。比如两个人的电脑连接着一个打印机，第一个人按下了打印按钮，此时打印机开始打印第一个文件，但是在打印至一半时，第二个人也按下了打印按钮，开始带引他的文件。如果没有系统调用申请内核服务的话，那么两个进程就会互相随意地并发的共享计算机资源，最终造成两个文件混杂在一起的情况。而使用系统调用，触发陷阱发送中断信号请求内核对共享资源的统一的管理，操作系统就会向上提供“系统调用”服务，内核会对这几个进程进行协调处理，使其互相不干扰的并发进行，即在某个进程该工作占用cpu和打印机共享资源时工作，非这个进程阶段就进制此进程占用共享资源，这样就会使得最终的结果互补混杂了。所以系统调用对于共享资源的管理和任务之间的协调调度起着至关重要的作用。 系统调用的分类 应用程序通过系统调用来请求系统的服务，而系统中的各种共享资源都又操作系统内核统一掌管，因此凡是与共享资源有关的操作（如存储分配，I/O操作，文件管理）等，都必须通过系统调用的方式向操作系统内核发出服务请求等待响应，然后又操作系统内核代为完成（所以是在管态进行的分配服务），这样就保证了系统的稳定性与安全性，防止了用户的非法操作。 系统调用的过程 因为系统调用是应用程序主动然爱过出cpu的使用权，使用陷入指令触发的中断信号，所以系统调用一定是内中断。 我们可以看到系统调用时并不是立刻就进行陷入指令，而是首先在目态进行一系列准备工作，比如记录中断地址（毕竟最终操作系统服务完以后还要回到这个地址继续执行），还有传参指令即将系统调用需要的参数存放到制定的寄存器以便操作系统使用，最终才调用陷入指令（此时已经做好了移交cpu的准备工作），然后操作系统掌握cpu使用权（管态）进行服务，完成后最终再返回到中断位置继续执行后面的指令。 总结 操作系统的体系结构 我们通过上图可以看出一个操作系统内核部分和非内核部分可以组装，比如Ubuntu等就是建立在linux基础上再加以非内核功能组装住的操作系统。并且我们也已经知道内核是操作系统最基本，最核心的部分，实现操作系统内核功能的那些程序就是内核程序。并且内核分为了四个部分： 因为其对软硬件的操作程度不同，有区分成了大内核和微内核，我们前面是以企业模型分析了两种内核类别的效率和优缺点。这里我们再以变态次数分析一下，首先我们需要知道应用程序想要请求操作系统的服务时，这个服务会涉及到进程管理，存储管理，设备管理即对硬件操作不是很大的那层（橘色层）。然后在涉及到最接近硬件层的时钟管理，中断处理和原语部分。按照大内核和微内核的定义： 我们可以看出他们两种类型的内核布置造成了不同的变态次数。 对于大内核其认为两层均是内核功能部分，所以这两层都处于管态执行，这样四个功能之间的切换就不会在涉及到变态过程了，唯一造成变态的位置就是应用程序和大内核之间的切换，所以只有2次变态。而对于微内核，其任务进程管理，存储管理和设备管理（橘色层）不属于内核部分，所以此部分还是需要在用户态执行，这样虽然应用程序和橘色层之间不再需要变态了，但是由于这三个操作都是会涉及到时钟管理，中断处理和原语部分，所以每一个都需要经历两次变态，最后总体来看会造成6次变态。而变态的过程是有成本的，要消耗不少的时间，频繁的变态会降低系统的性能，所以这也是大内核效率更高的原因之一。 生活中的系统分类 典型的大内核/宏内核/单内核操作系统：Linux,UNIX 典型的微内核操作系统：Windows NT 总结"},{"title":"磁盘","path":"/wiki/操作系统笔记/磁盘/index.html","content":"磁盘调度算法 显然对于一个磁盘数据的读/写序列（中途可能会更新）使用不同的调度算法会产生不同的效率，这里我们也探讨一下几种不同算法的性能，那么指标肯定就是时间了所以我们先介绍几个指标概念然后介绍调度算法。 一次磁盘读/写操作需要的时间 寻道时间Ts 寻道时间Ts：又称为寻找时间，在读/写操作之前，将磁头移动到指定磁道所花费的时间。 启动磁头臂是需要时间的。假设耗时为s。 移动磁头也是需要时间的，假设磁头均匀移动，每跨越一个磁道耗时为m，总共需要跨越n条磁道，则： Ts=s+m∗nT_s=s+m*n Ts​=s+m∗n 现在的硬盘移动一个磁道大约需要0.2ms,磁臂启动时间约为2ms。 延迟时间Tn 延迟时间Tn:通过旋转磁盘，使磁头定位到目标扇区所需要的时间。设磁盘转速为r(单位:转/s，转/min)，那么平均所要的延迟时间为 Tn=(1/2)∗(1/r)=1/2rT_n=(1/2)*(1/r)=1/2r Tn​=(1/2)∗(1/r)=1/2r 1/r就是转一圈所需要的时间，找到目标扇区平均需要转半圈，因此再乘1/2，硬盘的典型转速为5400转/min，或者7200转/min。 传输时间Tt 传输时间Tt:从磁盘读出或向磁盘写入数据所经历的时间，假设磁盘转速为r，此次读/写的字节数为b，每个磁道上的字节数为N，则： Tt=(1/r)∗(b/N)=b/rNT_t=(1/r)*(b/N)=b/rN Tt​=(1/r)∗(b/N)=b/rN 每个磁道可存N字节的数据，因此b字节的数据需要b/N个磁道才能存储，而读/写一个磁道所需要的时间刚好又是一圈所需要的时间1/r。 总的平均时间Ta Ta=Ts+Tn+Tt=s+m*n+1/2r+b/rN 延迟时间Tn和传输时间Tt都与磁盘的转速有关，且为线性关系，而转速是硬件的固有属性，因此操作系统无法优化延迟时间和传输时间。 先来先服务算法（FCFS) 根据进程请求访问磁盘的先后顺序进行调度。例如：假设磁头的初始位置为100号磁道，有多个进程先后陆续地请求访问55、58、39、18、90、160、150、38、184号磁道，那么按照FCFS的规则，按照请求的顺序，磁头需要一次移动到55、58、39、18、90、160、150、38、184号磁道。 磁头一共移动了45+3+19+21+72+70+10+112+146=498个磁道。响应一个请求平均需要移动489/9=55.3个磁道。（平均寻找长度）。 优点：公平，如果请求访问的磁道比较集中的话，算法还可以。 缺点：如果有大量进程竞争使用磁盘，请求访问的磁道很分散，那么FCFS在性能上很差，寻到时间长。 最短寻找时间优先（SSTF） SSTF算法优先处理的是离当前磁头最近的磁道，可以保证每次的寻道时间最短，但是并不能保证总的寻道时间最短。其实就是贪心思想，贪心解未必是最优解。 例如：假设磁头的初始位置为100号磁道，有多个进程先后陆续地请求访问55、58、39、18、90、160、150、38、184号磁道： 磁头总共移动了(100-18)+(184-18)=248个磁道，响应一个请求平均移动248/9=27.5个磁道（平均寻道长度） 优点：性能好，平均寻道时间短 缺点：可能产生饥饿现象 扫描算法（SCAN） SSTF产生饥饿的原因是磁头有可能会在一个小区域内来回的移动，为了防止这个问题，可以规定，只有磁头移动到最外侧的磁道的时候才能往内移动，移动到最内侧磁道的时候才能往外移动。这就是扫描算法（SCAN）的思想。由于磁头移动方式很像电梯，因此也叫电梯算法。 假设某磁道为0~200号，磁头的初始位置是100号，此时磁头正在往磁道号增大的方向移动，那么此时有多个进程的请求访问：55、58、39、18、90、160、150、38、184号磁道。 一定要注意必须移动到磁道最边缘处才可以更改移动方向即使没有请求访问最边缘磁道也要经过。 磁头总共移动了（200-100）+（200-18）=282个磁道，响应一个请求平均需要282/9=31.3个磁道（平均寻道长度）。 优点：性能好，平均寻道时间短，不会产生饥饿现象 缺点：①只有到达最边上的磁道时才能改变磁头移动方向，事实上，处理了184号磁道的访问请求之后就不需要再往右移动磁头了。②SCAN算法对于各个位置磁道的响应频率不均匀（如：假设此时磁头正在向右移动，且刚处理过90号磁道，那么下次处理90号磁道的请求就需要等磁头移动很长一段距离，而相应了184号磁道的请求之后，很快又可以再次相应184号磁道的请求了） LOOK调度算法 扫描算法（SCAN）：只有到达最边上的磁道时才能改变磁头移动方向，事实上，处理了184号磁道的访问请求之后就不需要再往右移动磁头了。LOOK算法就是为了解决这个问题，如果在磁头移动方向上没有别的请求，就可以立即改变磁头移动方向。（边移动边观察，因此叫LOOK） 假设某磁道的磁盘为0~200号，磁头的初始位置为100号磁道，且此时磁头正在往磁道号增大的方向移动，有多个进程先后陆续的请求访问55、58、39、18、90、160、150、38、184号磁道。 那么响应一个请求平均寻道长度为250/9=27.5磁道 优点：比起SCAN算法来，不需要每次都移动到最外侧或最内侧时才改变磁头方向，使寻道时间进一步缩短。 缺点：只解决了SCAN算法的缺点1，响应频率还是不均匀。 循环扫描算法（C-SCAN） SCAN算法对于各个位置磁道的响应频率不平均，而C-SCAN就是解决了这个问题。规定只有磁头向右移动或者向左移动时才可以处理磁道访问请求，而返回时直接快速移动到起始端中间返回过程不做任何请求任务。 假设某磁盘的磁道为0~200号，磁头的初始位置为100号磁道，且此时磁头正在向磁道号增大的方向移动，那么有多个进程陆续的请求访问55、58、39、18、90、160、150、38、184号磁道。 优点：比起SCAN算法，对于各个位置的响应频率很平均 缺点：只解决了SCAN算法的缺点2，但是还是只有到达最边缘的磁道才可以返回到起始端。 C-LOOK调度算法 C-SCAN算法的主要缺点是只有到达最边缘的磁道才可以返回到起始端，但是我们也可以模仿LOOK算法边移动边观察，当后面没有更大的请求磁道号时就不用再移动到最边缘了，直接返回到起始端节省开销。 假设某磁盘的磁道为0~200号，磁头的初始位置为100号磁道，且此时磁头正在向磁道号增大的方向移动，那么有多个进程陆续的请求访问55、58、39、18、90、160、150、38、184号磁道。 优点：比起C-SCAN算法来，不需要每次移动到最外侧或者最内侧才改变刺头方向，同时响应频率也很均匀。 缺点：也不算是缺点，就是没必要，因为边移动边观察看似节省开销了实际上实现起来开销也不必C-SCAN小多少。 总结 减少延迟时间的方法 我们知道延迟时间就是磁头等待到目标扇区的时间，那么磁盘扇区的不同排列方式也会对延迟时间造成影响。 如果排列如下： 那么假设现在要连续读取橙色区域的2,3,4区域，那么磁头读取一块的内容（也就是一个扇区的内容后）需要一小段的处理时间，而此时盘片还在不停地旋转。因此如果2,3号扇区相邻着排列，则读完2号扇区后无法连续不断的读入3号扇区。必须等待盘片继续旋转，3号扇区再次滑过磁头，才可以完成扇区读入。所以我们可以得到如下结论：磁头读入一个扇区数据后需要一小段时间处理，如果逻辑上相邻的扇区在物理上也相邻，那么读入几个连续的逻辑扇区，可能需要很长的延迟时间。 减少延迟的方法：交替编号 很明显，此时采用交替编号后，逻辑相邻的磁盘块物理结构上并不是相邻的，这样可以使读取连续的逻辑扇区所需要的延迟时间更小。 磁盘结构的设计 我们思考一个问题，为什么在设计磁盘的物理地址时使用的表示方法为（柱面号，盘面号，扇区号）而不是（盘面号，柱面号，扇区号）？这里的原因如下： 假设现在某个磁盘有8个柱面即8个磁道（且最内侧磁道编号为0），4个盘面，8个扇区。那么可以用3个二进制位表示柱面，2个二进制位表示盘面，3个二进制位表示扇区。 那么如果物理地址是（盘面号，柱面号，扇区号）来表示，那么如果现在需要连续读入物理地址（00,000,000）~（00,001,111）的扇区。那么（00,000，000）~（00,000,111）转两圈即可读完，之后在读取物理地址相邻的区域即（00,001,000）~（00,001,111）的时候需要启动磁头臂，将磁头移动到下一个磁道。 而如果是物理地址结构为（柱面号，盘面号，扇区号），且需要连续读入物理地址为（000,00,000）~（000,01,111）的扇区时，由于都在柱面为000的位置，所以不需要移动磁臂，只是在读入(000,01,000)~(000,01,111)时需要激活1号盘面的磁头即可。所以如果是（盘面，柱面，扇区）这种物理地址结构读入连续的物理地址时也需要不断的移动磁头，但是如果是（柱面，盘面，扇区）时就只需要激活不同盘面的磁头即可，无需移动磁臂，这样可以减少磁头移动消耗的时间。 减少延迟的方法：错位命名 如果按照上面这样命名，即不同盘面的相对位置处编号相同，那么假设要连续读入物理地址为(000,00,000)~(000,01,111)时当读取完磁盘块（000,00,111）之后需要短暂的时间处理，而盘面又在不停地旋转，那么当（000,01,000）第一次滑过1号盘面的磁头下方时，并不能读取数据，只能再等扇区再次滑过磁头。所以我们可以错位命名如下： 即此时的两个盘面相对位置处的编号都有错位。那么就可以做到当读取完磁盘块（000,00,111）之后，还有一段时间处理，当（000,01,000）第一次滑过1号盘面的磁头下方时，就可以直接读取数据了，从而减少了延迟时间。 思考：交替编号和错位命名的区别？ 首先交替编号和错位命名是两种策略，他们相互配合减少了磁盘读/写的时间开销。交替编号是针对某一个盘面的编号来说的，使得每一个盘面的编号的交替的。而错位命名是针对的不同盘面之间编号的，使得每一个盘面编号相同的扇区在不同的相对位置，使得切换盘面有一定的时间缓冲可以立刻读取下一个相邻的物理地址。 总结 磁盘的管理 磁盘初始化 分为如下几个步骤： 进行低级格式化（物理格式化），将磁盘的各个磁道划分为扇区。一个扇区通常可分为头、数据区域（如512B大小）、尾三个部分组成。管理扇区包括各种数据结构一般存放在头、尾两个部分，包括扇区校验码（如奇偶校验码，CRC循环冗长验证码等，校验码用于校验扇区中的数据是否发生错误） 将磁盘分区，每个分区由若干柱面组成（即分为C,D,E盘等） 我们可以看出越靠近里面的盘数据密度也就越大。 进行逻辑格式化，创建文件系统。包括创建文件系统的根目录，初始化存储空间管理所用的数据结构（如位示图法、空闲分区表等） 引导块 计算机在开机时需要进行一系列初始化工作，这些初始化工作通过执行初始化程序（自举程序）完成的。 初始化程序可以放在ROM中（只读存储器）中，ROM中的数据在出厂时就写入了，并且以后就不可以修改了（ROM一般是出厂时就集成在主板上）。 思考：自举程序放在ROM中存在什么问题？ 我们发现当需要更新自举程序时就会很不方便，因为ROM中的数据结构无法更改。所以我们需要解决此问题。 我们可以考虑不将自举程序放入ROM，而是在ROM中存放很小的“自举装入程序”，开机时计算机先运行“自举装入程序”，通过执行该程序就可以找到引导块，并将完整的“自举程序”读入内存，完成初始化。而完整的自举程序放在了磁盘的启动块（即引导块/启动分区）上，启动块位于磁盘的固定位置。拥有启动分区的磁盘称为启动磁盘或系统磁盘（一般我们的计算机中都是C盘）。 坏块的管理 对于简单的磁盘，可以在逻辑格式化时（建立文件系统时）对整个磁盘进行坏块检查，标明那些是坏扇区比如在FAT表上标明（在这种方式中，坏块对操作系统不透明）。对于复杂的磁盘，磁盘控制器（磁盘设备内部的一个硬件部位）会维持一个坏块链表。在磁盘出厂前进行低级格式化（物理格式化）时就将坏块链进行初始化。当然也可以保留一些备用扇区用于替换坏块。这种方案称为扇区备用，且这种处理方式中，坏块对操作系统透明。我们这里介绍几种策略： RAID0 RAID0又称为Stripe或者Striping,他代表着所有RAID级别中最高性能的存储性能。RAID0的原理就是把连续的数据分散到多个磁盘上存取，这样当系统有数据请求时就可以多个磁盘并行的执行，每一个磁盘运行属于它自己的那部分数据请求。这种并行操作请求的方法显著提高了存储性能。并且磁盘的读/写操作也会提高，假设RAID0将某一个请求分成了三个部分，那么每个磁盘只运行自己的那部分任务，那么理论上运行速度会提升为原来的3倍，但是由于总线带宽等影响，会低于理论值，但是也明显提升了速度。 虽然优点显著：读写，存储性能极高，但是缺点是不提供数据冗余，一旦用户数据损坏，损坏的数据将不能再恢复。所以RAID0中只要有一个硬盘损坏，整个RAID0设备都不能使用，所以可维护性极差。 磁盘空间使用率：100%，故成本最低。 读性能：N*单块磁盘的读性能 写性能：N*单块磁盘的写性能 冗余：无，任何一块磁盘损坏都将导致数据不可用 RAID1 其实就是镜像备份了，这样就实现了数据冗余，在成对的独立磁盘上产生互相备份的数据。当原始数据繁忙时，可以镜像拷贝读取数据，所以读取性能提高了。并且由于每一个盘都有镜像备份，所以磁盘阵列中单位成本很高。但是也提供了数据的安全性和可用性，当一个磁盘损坏失效，系统可以快速切换到备份的镜像磁盘上读写，不会立刻停止工作。当然两个都损坏了也是无法工作的，但概率太小。所以RAID1中总是有一个保持完整数据的备份盘，可靠性更好。 细节：读写数据的区别？ 一定要注意RAID1中读只能在一个磁盘上进行，即要不在DRIVE1BlockX读，要不在DRIVE2BlockX读,只是在DRIVE1BlockX忙碌时暂时无法提供读数据的时候，DRIVE2BlockX可以替DRIVE1BlockX提供读，但是总体上看只能一个盘提供，所以读的时候是不能并行执行的。而写磁盘的时候可以并行的对两个磁盘进行写，毕竟他们的数据应该是一样的（备份盘和原盘数据必须一致），但是虽然是并行写操作，但是因为要比较硬盘中的数据，所以写数据性能还是比单块磁盘慢。 磁盘空间使用率：50%，故成本最高。 读性能：只能在一个磁盘上读取，取决于磁盘中较快的那块盘 写性能：两块磁盘都要写入，虽然是并行写入，但因为要比对，故性能单块磁盘慢 冗余：只要系统中任何一对镜像盘中有一块磁盘可以使用，甚至可以在一半数量的硬盘出现问题时系统都可以正常运行 RAID10 其实可以看出特点，就是RAID1和RAID0的组合，对于整体来看组合是RAID0而局部看来每一个不分都是RAID1这样的好处是，整体上的读写性能很好，并且也不容易损坏因为每一个部分都有备份盘即使损坏了也可以立刻用备份盘替换。 磁盘空间利用率：50% 读性能：N/2*单块硬盘的读性能 写性能：N/2*单块硬盘的写性能 冗余：只要一对镜像盘中有一块磁盘可以使用就没问题 思考：为什么不是RAID01组合？ 即整体看来是RAID1，而局部看来每个部分都是RAID0我们发现整体看性能写很慢，读还可以，但是只要有2个局部损坏任意一个分盘都是整体都不能在使用了，性能一般且成本昂贵不易于维护😅。所以这个组合不适用。 RAID5 RAID 5是RAID 0和RAID 1的折中方案。RAID 5具有和RAID0相近似的数据读取速度，只是多了一个奇偶校验信息，写入数据的速度比对单个磁盘进行写入操作稍慢。同时由于多个数据对应一个奇偶校验信息，RAID5的磁盘空间利用率要比RAID 1高，存储成本相对较低，是目前运用较多的一种解决方案。当然我们发现每一组类型的盘都有一个备份盘随时准备顶替损坏的磁盘工作，并且备份盘每一个都分布在不同的disk上，而相应的有备份盘的disk就么有哪一种类的工作原盘。这样既便于维护整体性能也还不错，当有一个盘损坏时也可以继续工作，当然仅限于坏掉一个，当再坏掉一个或者备份盘先坏掉此时又有盘坏掉时也是会停止工作的。 磁盘空间利用率：(N-1)/N，即只浪费一块磁盘用于奇偶校验 读性能：(n-1)*单块磁盘的读性能，接近RAID0的读性能。 写性能：比单块磁盘的写性能要差 冗余：只允许一块磁盘损坏 以上内容来自大佬博客 总结"},{"title":"管程与死锁","path":"/wiki/操作系统笔记/管程与死锁/index.html","content":"管程 这部分仅是了解内容，当做拓展就好 为什么引入管程 在提出信号量后我们确实可以借用信号量+PV操作实现进程互斥关系但是我们发现这对编程人员极其不友好，编写程序困难，易出错。如下图： 我们知道这种P操作顺序错误会造成死锁，但是在编程中我们确实需要时刻注意P,V操作的顺序，这非常困难，所以能不能设计一个机制，让程序猿写程序时不需要关心复杂的PV操作，让写代码更轻松呢？可以，1973年，Brinch Hanson首次在程序设计语言（Pascal)中引入了&quot;管程&quot;的概念–一种高级同步机制，自此我们不需要在关心复杂P,V操作了，而是由编译器负责实现各进程的互斥进入管程操作。 管程的定义和基本特征 管程实际上就是一种特殊地软件模块，由这些部分组成： 局部于管程的共享数据结构说明 对该数据结构进行操作的一组过程(实际上就是函数) 对局部于管程的共享数据设置初始值的语句 管程有一个名字 管程的基本特征： 局部于管程的数据只能被局部于管程的过程(就是函数)所访问 一个进程只有通过调用管程内的过程(实际上就是函数)才能进入管程访问共享数据 每次仅允许一个进程在管程内执行某个内部过程(就是函数) 所以我们知道管程有自己的内部局部变量和函数以及一个管程名字，管程一次性只允许一个进程执行管程内函数并且管程内的数据只能被局部于管程的函数访问这样就实现了进程之间的互斥，即管程的函数封装了具体的操作，并且凭借每次只有一个进程能够调用管程函数来修改管程内的数据(实际就是信号量)。这样我们就不需要经常关注PV操作了。 如下是一个应用： 用管程解决生产者-消费者问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//定义管程数据结构为ProducerConsumermonitor ProducerConsumer //管程内数据实际上就是信号量 //解决同步问题 condition full,empty;\tint count=0;//缓冲区内产品数\t//第一个管程内部函数\t//把产品放入到缓冲区\tvoid insert(Item item)&#123; if(count==N)//缓冲区已满 wait(full)//阻塞等待 count++;//否则产品数加一 insert_item(item);//缓冲区放进一个产品 if(count==1) //此时缓冲区不空了唤醒wait(empty) signal(empty) &#125;\t//第二个管程内函数\t//从缓冲区取产品\tItem remove()&#123; //缓冲区是空的 if(count==0) wait(empty)//阻塞等待 count--;//否则产品数减一 if(count==N-1) //此时缓冲区不满了唤醒wait(full) signal(full); return remove_item();//取出产品 &#125;end monitor;//结束管程定义//生产者进程producer()&#123; while(1)&#123; item=生产一个产品; //调用管程函数进行放入产品操作 ProducerConsumer.insert(item); &#125;&#125;//消费者进程consumer()&#123; while(1)&#123; //调用管程函数进行取出产品操作 ProducerConsumer.remove(); //消费产品item &#125;&#125; 我们发现管程只是保证每次都只有一个进程进行操作的互斥关系，具体的同步关系还是需要我们在管程内自己实现代码逻辑，并且这种封装管程内函数然后暴露给其他进程调用来操作内部数据的方式就是典型的“封装”思想。 引入管程的目的无非就是要更方便的实现进程互斥和同步。所以原理如下： 需要在管程中定义共享数据(如生产者-消费者问题中的缓冲区) 需要在管程中定义用于访问这些共享数据的&quot;入口&quot;–其实就是一些封装函数(如生产者-消费者问题中，可以定义一个函数用于将产品放入缓冲区，再定义一个函数用于从缓冲区取出产品) 只有通过这些特定的&quot;入口&quot;才能访问共享数据 管程中有很多&quot;入口&quot;，但是每次只能开放一个&quot;入口&quot;，并且只能让一个进程或线程进入(如生产者-消费者问题中，各进程需要互斥的访问共享缓冲区，管程的这种特性即可保证一个时间段内最多只会有一个进程在访问缓冲区。注意：这种互斥特性是由编译器负责实现的，程序猿不用关心，但是互斥关系还是需要程序猿自己实现) 可在管程中设置条件变量(实际上就是信号量)+等待/唤醒操作以解决同步问题，可以让一个进程或者线程在条件变量上等待(此时，该进程应先释放管程的使用权，也就是让出&quot;入口&quot;)，可以通过唤醒操作将等待在条件变量上的进程或线程唤醒。 程序猿可以用某种特殊的语法定义一个管程（比如：monitor ProducerConsumer…end monitor)之后其他程序猿就可以使用这个管程提供的特定&quot;入口&quot;很方便的使用实现进程同步/互斥了。 JAVA中类似于管程的机制 JAVA中，如果使用synchronized来描述一个函数，那么这个函数同一时间段内只能被一个线程调用。 如下： 12345678static class monitor&#123; private Item buffer[]=new Item[N]; private int count=0; public synchronized void insert(Item item)&#123; .... &#125;&#125; 如上每次都只允许一个线程进入insert函数，如果多个线程同时调用insert函数则后来者需要排队等待。 总结 死锁 什么是死锁 其实我们在前面已经不止一次提到“死锁”的概念了，例如刚刚讲到的哲学家问题中同时都先拿左筷子再拿右筷子就会导致死锁现象出现。这里每个人都占有一个资源同时又在等待另一个人手里的资源的情况就是“死锁”这里我们给出严格的定义：在并发环境下，各种进程因争夺资源而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象，就是&quot;死锁&quot;。发生死锁后若无外力干涉，这些进程都将无法向前推进。 思考：死锁，饥饿，死循环有什么区别？ 死锁：各进程互相等待对方手里的资源，导致各进程都阻塞，无法向前推进的现象。 饥饿：由于长期得不到想要的资源，某进程无法向前推进的现象。比如：SPF算法中，若有源源不断的短进程到来，则长进程一直得不到处理机，从而发生长进程“饥饿”。 死循环：某进程执行过程中一直跳不出某个循环的现象。有时是因为程序逻辑bug导致的，有时是程序猿故意设计的（比如while(1)）。 共同点 区别 死锁 都是进程无法顺利向前推进的现象（故意设计的死循环除外） 死锁一定是“循环等待对方手里的资源”而导致的，因此如果有死锁现象，那么至少有两个或两个以上的进程同时发生死锁。另外，发生死锁的进程一定处于阻塞态。 饥饿 都是进程无法顺利向前推进的现象（故意设计的死循环除外） 可能只有一个进程发生饥饿，发生饥饿的进程既可能是阻塞态（如长期得不到需要的I/O设备），也可能是就绪态（长期得不到处理机）。 死循环 都是进程无法顺利向前推进的现象（故意设计的死循环除外） 可能只有一个进程发生死循环，死循环的进程可以上处理机运行（可以是运行态），只不过无法像期待的那样顺利推进，思索和饥饿问题是由于操作系统分配资源的策略不合理导致的，而死循环是由代码逻辑的错误导致的。死锁和饥饿是管理者（操作系统）的问题，死循环是被管理者的问题。 死锁产生的必要条件 产生死锁必须同时满足以下四个条件，只要其中任意一个条件不成立，死锁就不会发生。 互斥条件：只有对必须互斥使用的资源的争抢才会导致死锁（如哲学家问题的筷子，打印机等I/O设备）。像内存，扬声器这样可以同时让多个进程使用的资源是不会导致死锁的（因为进程不用阻塞等待这种资源）。 不剥夺条件：进程所获得的资源在未使用完之前，不能由其他进程强行夺走，只能主动释放。 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源的请求，而该资源又被其他进程占有，此时请求进程被阻塞，但又对自己已有的资源保持不放。 循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求。 所以我们完全可以参照哲学家问题的死锁情况推出这四个条件，并且还可以知道发生死锁时一定是有循环等待，但是发生循环等待时未必死锁（循环等待是死锁的必要不充分条件）。当然如果同类资源数大于1，则即使有循环等待，也未必发生死锁，但如果系统中每类资源都只有一个，那循环等待就是死锁的充要条件了。 什么时候发生死锁 对系统资源的竞争，各进程对不可剥夺的资源（如打印机）的竞争可能引起死锁，对可剥夺的资源(cpu)的竞争是不会引起死锁的。 进程推进顺序非法。请求和释放资源的顺序不当，也同样会导致死锁。例如：并发执行的进程P1，P2分别申请占有了资源R1,R2，之后进程P1有紧接着申请资源R2，而进程P2又申请资源R1，两者会因为申请的资源被对方占有而阻塞，从而发生死锁。 信号量的使用不当也会造成死锁，如生产者-消费者问题中实现互斥的P操作在实现同步操作P之前，就有可能会发生死锁。（我们可以把互斥信号量和同步信号量也看做是一种抽象的系统资源） 总之对不可剥夺的资源的不合理分配就可能会导致死锁。 死锁的处理策略 预防死锁。破坏死锁产生的四个必要条件中的一个或多个 避免死锁。用某种方法防止系统进入不安全状态，从而避免死锁（银行家算法） 死锁的检测和接触、允许死锁的发生，不过操作系统会负责检测出死锁的发生，然后才去某种措施解除死锁。 前面的两种方法都是不允许死锁发生，最后一种是允许死锁发生。 总结 死锁的处理策略 那么接下来我们就逐一讲解一下死锁处理的三条策略。 预防死锁 破坏互斥条件 互斥条件：只有对必须互斥使用的资源的争抢才会导致死锁 如果我们把只能互斥使用的资源改造为允许共享使用，那么系统就不会再进入死锁状态了，比如：SPOOLing技术。操作系统可以采用SPOOLing技术把独占设备在逻辑上改造成共享设备，比如用SPOOLing技术将打印机改造成共享设备。 这个策略的缺点是并不是所有的资源都可以改造成共享使用的设备，并且为了系统的安全，很多地方还必须保护这种互斥性，因此很多时候无法破坏互斥条件。 破坏不剥夺条件 不剥夺条件：进程所获得的资源在未使用完之前，不能由其他进程强行夺走，只能主动释放。 我们可以采用以下两种方案破坏不剥夺条件 方案1：当某个进程请求新的资源得不到满足时，他必须立即释放保持所有的资源，待以后需要时再重新申请。也就是说，即使某些资源尚未使用完，也需要主动释放，从而破坏了不可剥夺条件。 方案2：当某个进程需要的资源被其他进程所占有的时候，可以由操作系统协助，将想要的资源强行剥夺。这种方式一般需要考虑各进程的优先级（比如：剥夺调度方式，就是将处理机资源强行剥夺给优先级更高的进程使用） 这种策略的缺点是： 实现起来复杂无论是方案1还是方案2 释放已获得的资源可能会造成前一阶段的工作的失效，因此这种方法一般适用于易保存和恢复状态的资源，如cpu。（有PCB记录信息的好处） 反复地申请和释放资源会增加系统开销，降低系统吞吐量。 如果采用方案1，意味着只要暂时得不到某个资源，之前获得的那些资源都要放弃，以后再重新申请，如果一直放生这样的情况，就会导致进程饥饿。 破坏请求和保持条件 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，此时请求进程被阻塞，但又对自己已有的资源保持不放。 可以采用静态分配方法，即进程在运行前一次申请完他所需要的全部资源，在它的资源未满足之前，不让他投入运行，一旦投入运行，这个资源就一直归他所有，该进程就不会再请求别的任何资源了。 该策略实现起来简单，但也有明显的缺点：有些资源可能只需要很短的时间，因此如果进程的整个运行期间都一直保持着所有资源，就会造成严重的资源浪费，资源利用率低，另外该策略也会导致某些进程饥饿。 破坏循环等待条件 循环等待条件：存在一种进程资源的循环等待链，链中的每一个进程已获得的资源同时被下一个进程所请求。 我们可以采用顺序资源分配法，首先给系统中的资源编号，规定每个进程必须按编号递增的顺序请求资源，同类资源（即编号相同的资源）一次申请完。 思考：什么原理破坏了循环等待条件？ 一个进程只有已经占有小编号的资源时，才有资格申请更大编号的资源，按照此规则，已持有大编号资源的进程不可能逆向地回来申请小编号的资源，从而就破坏了循环等待链也就不会再出现循环等待的现象了。 我们假设现在有10个资源，编号1~10。 该策略的缺点： 不方便增加新的设备，因为可能需要重新分配所有的编号 进程实际使用资源的顺序可能和编号递增顺序不一致，可能会导致资源浪费 必须按照规定次序申请资源，用户编程麻烦 总结 避免死锁 避免死锁就是利用银行家算法避免系统处于不安全状态，那么首先我们先了解一下一个定义 安全序列，不安全序列，安全状态，不安全状态 我们以一个投资的例子来分析介绍，假设你是一位成功的银行家，手里拥有100亿资金，有三个企业借贷，分别是B,A,T三个公司： B表示最多会借70亿 A表示最多会借40亿 T表示最多会借50亿 然而有个不成文规定，如果你借给企业的钱总数达不到企业提出的最大要求那么值钱借给企业的钱就都拿不回来了。刚开始B,A,T三个企业分别借了20,10,30亿，如下： 此时我们手里还剩下40亿，此时A又提出要借款20亿，那么我们能否借给A呢？ 思考：如果借给A会有三家公司的钱都要不回来的风险吗？ 我们思考，此时借给A20亿，如下: 那么此时我们手中还剩下20亿，此时如果按照T-&gt;B-&gt;A或者A-&gt;T-&gt;B的顺序追债是可以把之前借的钱都要回来的，所以此时没有三家公司的钱都要不回来的风险，所以此时是安全的，我们可以借给A20亿。 思考：能否举出一个三家公司的钱都要不回来的情况吗？ 很简单，假设此时我们手上还有40亿，此时B也要借钱借30亿，那么此时如果我们借出去，如下图： 那么此时我们手中还剩下10亿，我们发现此时就不安全了，因为三家公司的钱我们都要不回来了。所以此时就是不安全的，我们不能再借给B30亿了。 根据上面的例子我们知道所谓安全序列，就是指如果系统按照这种序列分配资源，则每个进程都能顺利完成，只要能找出一个安全序列，系统就是安全的，当然安全序列可能有多个。 如果分配了资源之后，系统找不出任何一个安全序列，系统就进入了不安全状态，这也就意味着之后可能所有进程都无法顺利的执行下去了，当然如果有进程提前归还了一些资源（比如A先归还了10亿，那么手里有20亿按照T-&gt;B-&gt;A），那么系统也有可能重新回到安全状态，不过我们在分配资源之前总是要考虑到最坏的情况。 如果系统处于安全状态，就一定不会发生死锁。如果系统进入了不安全状态也未必就一定发生死锁，只是有死锁的风险，但是如果死锁了那么一定是在不安全状态下发生的。因此可以在资源分配之前预先判断这次分配是否会导致系统进入不安全状态，以此决定是否答应资源分配请求，这也是“银行家算法”的核心思想。 银行家算法 银行家算法是荷兰学者Dijkstra为银行设计的，以确保银行在发放现金贷款时，不会发生不能满足所有客户需要的情况。后来这种算法被用在操作系统中用于避免死锁。 核心思想和刚刚的借贷案例相同就是在进程提出资源申请时，先预判此次分配是否会导致系统进入不安全状态，如果会进入不安全状态，就暂时不答应这次请求，让该进程先阻塞等待。 思考：对于多种资源的情况，如何实现银行家算法？ 我们思考在BAT借贷的例子中只有一种类型的资源–钱，但是在实际的计算机系统中会有多种多样的资源，应该怎么把算法拓展为多种资源的情况呢？这里我们可以把单维的数字拓展为多维的向量，比如：系统中有5个进程P0~P4，3中资源R0~R2，初始数量为(10,5,7)，则某一时刻的情况可用下表方式表示： 对于上面的例子我们分析一下能否安全。首先第一次分配后剩余的资源数如下表： 进程 最大需求 已分配 最多还会需求 P0 (7,5,3) (0,1,0) (7,4,3) P1 (3,2,2) (2,0,0) (1,2,2) P2 (9,0,2) (3,0,2) (6,0,0) P3 (2,2,2) (2,1,1) (0,1,1) P4 (4,3,3) (0,0,2) (4,3,1) 此时我们还剩下（3,3,2）的资源在手里，那么此时系统是否处于安全状态？ 我们先检查（3,3,2）此时可以满足那些进程的需求，很明显现在满足P1,P3： 假设我们先把P1收回，此时应该是按照下面的公式更新已有资源 if(已有的资源&gt;最多还会需求)then{已有资源=已有资源+已分配资源}if(已有的资源&gt;最多还会需求)\\\\ then\\{ 已有资源=已有资源+已分配资源 \\} if(已有的资源&gt;最多还会需求)then{已有资源=已有资源+已分配资源} 所以我们收回P1后，已有资源更新为(3,3,2)+(2,0,0)=(5,3,2),剩余的进程资源表变为 进程 最大需求 已分配 最多还会需求 P0 (7,5,3) (0,1,0) (7,4,3) P2 (9,0,2) (3,0,2) (6,0,0) P3 (2,2,2) (2,1,1) (0,1,1) P4 (4,3,3) (0,0,2) (4,3,1) 此时满足收回条件的有P3,P1，我们假设先收回P3，那么现有资源为(5,3,2)+(2,1,1)=(7,4,3)，表更新为 进程 最大需求 已分配 最多还会需求 P0 (7,5,3) (0,1,0) (7,4,3) P2 (9,0,2) (3,0,2) (6,0,0) P4 (4,3,3) (0,0,2) (4,3,1) 此时全部进程都满足收回条件了，那肯定是先收回那个都可以了，所以只要是P1-&gt;P3开头的序列就一定是安全序列，所以操作系统处于安全状态，当然也不是只有P1-&gt;P3开头的是安全序列，同理P3-&gt;P1同样是安全序列，总之只要找到一条安全序列就是安全状态所以此时不会发生死锁。 以此类推，共5次循环检查即可将5个进程都加入安全序列，最终得到一个安全序列。这种算法成为安全性算法，可以很方便的使用代码实现以上流程，每一轮都从编号较小的进程开始检查。这里主要是要牢记已有资源的更新公式！ 思考:银行家算法的定义？ 假设系统有n个进程，m中资源，每个进程在运行前先声明对各种资源的最大需求数，则可以用一个n*m的矩阵（可以用二维数组实现）表示所有进程对各种资源的最大需求数。不妨称为最大需求矩阵Max,Max[i,j]=K表示进程Pi最多需要K个资源Rj,同理，系统可以使用一个n*m的分配矩阵Allocation表示对所有进程的资源分配情况，Max-Allocation=Need矩阵，表示各进程最多还需要多少各类资源。另外还要用一个长度为m的一维数组Available表示当前系统还有多少可用资源。某进程Pi向系统申请资源，可用一个长度为m的一维数组Requesti表示本次申请的各种资源量。 可用银行家算法预判本次分配是否会导致系统进入不安全状态： 如果 Requesti[j]&lt;=Need[i,j](0&lt;=j&lt;=m)Request_i[j]&lt;=Need[i,j](0&lt;=j&lt;=m) Requesti​[j]&lt;=Need[i,j](0&lt;=j&lt;=m) 便转向2，否则认为出错 如果 Requesti[j]&lt;=Available[j](0&lt;=j&lt;=m)Request_i[j]&lt;=Available[j](0&lt;=j&lt;=m) Requesti​[j]&lt;=Available[j](0&lt;=j&lt;=m) 便转向3，否则表示尚无足够资源，Pi必须等待 系统试探着把资源分配给Pi,并修改相应的数据（并非真的分配，修改数值只是为了做预判）： Available=Available−RequestiAvailable=Available-Request_i Available=Available−Requesti​ Allocation[i,j]=Allocation[i,j]+Requesti[j]Allocation[i,j]=Allocation[i,j]+Request_i[j] Allocation[i,j]=Allocation[i,j]+Requesti​[j] Need[i,j]=Need[i,j]−Requesti[j]Need[i,j]=Need[i,j]-Request_i[j] Need[i,j]=Need[i,j]−Requesti​[j] 操作系统执行安全性算法，检查此次资源分配后，系统是否处于安全状态。若安全，才正式分配，否则，恢复相应数据，让进程阻塞等待。 总结 死锁的检测和解除 死锁的检测和解除一大特点就是他允许死锁的发生然后检测到死锁后用一些方法解除死锁。所以首先我们需要能够检测出死锁。 死锁的检测 为了能够对系统是否已发生了死锁进行检测，必须： 用某种数据结构来保存资源的请求和分配信息 提供一种算法，利用上述信息来检测系统是否已进入死锁状态 我们一般可以用资源分配图来保存资源的请求和分配信息，有以下两点两边的定义： 如果系统中剩余的可用资源数足够满足进程的需求，那么这个进程暂时是不用阻塞的，可以顺利执行，如果这个进程执行结束了把资源归还给系统，就可能使某些正在等待资源的进程被激活，并顺利的执行下去。相应的，这些被激活的进程执行完了之后又会归还一些资源，这样可能又会激活另外一些阻塞的进程。 按照上面的过程叙述，我们知道对于资源分配图，每一个边对应一个圆的资源节点，如果最终能够消除所有边（优先消除绿边然后再消除蓝边），就称这个图是可完全简化的，此时一定没有发生死锁（相当于能找到一个安全序列）。如果最终不能消除所有变，那么此时就是发生了死锁，最终还连着的边的那些进程就是处于死锁状态的进程。 检测算法： 在资源分配图中，找出既不阻塞又不是孤点的进程Pi(即找出一条有向边与它相连，且该有向边对应资源的申请数量小于等于系统中已有空闲资源数量。如上图中R1没有空闲资源，R2有一个空闲资源。若所有的连接该进程的边均满足上述条件，则这个进程能继续运行直至完成，然后释放它所占有的所有资源)。消去它所有的请求边和分配边，使之成为孤立的节点。在上图中P1是满足这一条件的进程节点，于是P1的所有边消去。 进程Pi所释放的资源，可以唤醒某些因等待这些资源而阻塞的进程，原来的阻塞进程可能变为非阻塞进程，在下图中，P2就满足这样的条件，根据1的方法进行一系列简化后，若能消去图中所有的边，则称该图是可完全简化的。 死锁的解除 一旦检测出死锁的发生，就应该立即解除死锁，注意并不是系统中的所有进程都是死锁状态，而是用死锁检测算法化简资源分配图后，还连着边的那些进程就是死锁进程。解除死锁的方法有： 资源剥夺法：挂起（暂时放到外存）某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程，但是应防止被挂起的进程长时间得不到资源而饥饿。 撤销进程法：也叫终止进程法，顾名思义，强制撤销部分、甚至全部死锁进程，并剥夺这些进程的资源。这种方式的优点是实现简单，但所付出的代价可能会很大，因为有些进程可能已经运行了很长时间，已经接近结束了，一旦被终止可谓功亏一篑还得从头再来。 进程回退法：让一个或多个死锁进程回退到足以避免死锁的地步，这就要求系统要记录进程的历时信息，设置还原点。 总结"},{"title":"设备独立性软件","path":"/wiki/操作系统笔记/设备独立性软件/index.html","content":"I/O软件层次结构 从上图我们可以从总体上看出一个I/O设备相应请求时的全过程，分别经过了以下几个过程。 用户层软件 用户层软件实现了与用户交互的接口，用户可以直接使用该层提供的、与I/O操作相关的库函数对设备进行操作。比如库函数提供的printf()函数，他会被翻译成等价的write系统调用，用户层软件会在系统调用时填入相应参数，这样就可以通过系统调用的方式实现I/O请求。 windows操作系统会向外提供一系列系统调用，但是由于系统调用的格式严格，使用麻烦，所以在用户层上封装了一系列更加方便的库函数接口供用户使用就比如C库等。 设备独立性软件 又称为设备无关性软件，因为与设备的硬件特性无关的功能几乎都在这一层实现。他有以下几个功能： 向上层提供一些统一的调用接口（如read/write系统调用） 原理类似于文件保护，设备被看成是一种特殊的文件，不同用户对各个文件的访问权限不一样。同理也就实现了对设备的访问权限不同，保护设备不会被恶意文件修改 差错处理，设备独立性软件需要对一些设备的错误进行处理。 设备的分配与回收 数据缓冲区的管理，可以通过缓冲技术屏蔽设备之间数据交换单位大小和传输速度的差异 建立逻辑设备名到物理设备名的映射关系，根据设备类型选择调用相应的驱动程序。用户和用户层软件发出I/O操作相关系统调用的时，需要指明此次要操作的I/O设备的逻辑设备名。设备独立性软件通过“逻辑设备表（LUT，Logical Unit Table）”来确定逻辑设备对应的物理设备，并且找到该设备对应的设备驱动程序。 操作系统可以采用两种方式管理逻辑设备表LUT： ①整个系统就设置一张LUT，这就意味着所有用户不能使用相同的逻辑设备名，因此这种方式只适用于单用户操作系统。 ②为每一个用户都设置一个LUT，这样各个用户使用的逻辑设备名可以重复，适用于多用户操作系统，系统在用户登录时为其建立一个用户管理进程，而LUT就存放在用户管理进程的PCB中。 思考：为什么不同的设备需要不同的设备驱动程序？ 我们前面提到过，不同的厂商提供的设备信号规则不同，有的厂商对于生产的设备0代表空闲1代表忙碌，但是有的厂商生产的设备0代表忙碌1代表空闲。所以根据不同的信号需要正确识别信息，而这些不同设备的内部硬件特性只有厂商才知道，所以厂商需要提供与设备的对应的驱动程序，这样cpu才能够正确执行驱动程序的指令序列，来完成设置设备寄存器，检查设备状态等工作。（如果你不能够理解，那么就以这个例子为比喻：你通过介绍人录用了一个外国小伙当你的公司总监，但是你们之间的语言不通所以你无法知道他所返还的信息，所以介绍人在介绍外国小伙的同时还需要提供一个中间翻译即设备驱动程序，他能够为你们两个之间的信息交流提供翻译桥梁）。 设备驱动程序 设备驱动程序主要负责对硬件设备的具体控制，将上层发出的一系列命令（如read/write)转换成特定设备“能够听懂”的一系列指令操作，包括设置设备寄存器，检查设备状态等。不同的I/O设备有不同的硬件特性，具体细节只有设备的厂家才知道，因此厂家需要根据设备的硬件特性设计并提供相应的设备驱动程序。 中断处理程序 当I/O任务完成后，I/O控制器会发送一个中断信号，系统会根据中断信号类型找到相对应的中断处理程序并执行。中断处理程序的流程如下： 所以我们以一个I/O请求任务为例分别经过一下啊几个阶段才能够完成这次任务相应： 用户通过用户层软件提供的库函数发出的I/O请求 用户层软件通过“系统调用”请求设备独立性软件层的服务 驱动程序向I/O控制器发出具体命令 等待I/O设备完成的进程应该被阻塞，因此需要进程切换，而进程切换必然需要中断处理 总结 I/O软件各个层次之间的顺序要理解，要能够推理判断出某个处理属于哪个层次，通常直接涉及到硬件具体细节。且和中断无关的操作肯定是在设备驱动程序层完成的，没有涉及到硬件。对各个设备都需要进行的管理工作都是在设备独立性软件层完成的。 I/O核心子系统 I/O核心子系统是属于操作系统内核的一部分，所以肯定涉及到了调度，设备保护还有互斥等问题，下面就详细介绍这几种功能的具体实现 功能所属层次 我们知道这些功能都是由I/O核心子系统实现的，并且I/O核心子系统是由设备独立性软件、设备驱动程序、中断处理程序三个层次组成的，所以理论上这些功能肯定都是属于这三个层次。 但是实际上假脱机技术即SPOOLING技术（前面讲过是一种解决死锁的方法，即让个进程都产生这个临界区是自己的从而实现临界资源共享打破互斥条件来解决死锁）需要请求“磁盘设备”的设备独立性软件的服务，因此一般来说假脱机技术是在用户软件成实现的，但是408大纲又将假脱机技术归为了“I/O核心子系统”的功能，所以这里我们也认为假脱机技术是I/O核心子系统的功能。 I/O调度 与进程调度类似，I/O之间肯定也是需要有调度算法的，毕竟I/O设备就那么多，进程之间肯定是需要等待轮流使用I/O设备的。我们这里实际上已经学过了一些I/O调度算法，比如磁盘调度算法(FIFO算法，最短寻道优先算法，SCAN算法，C-SCAN算法， LOOK算法，C-LOOK算法等)。当多个磁盘I/O请求到来时，用某种调度算法确定满足I/O请求的顺序。同理打印机等设备肯定也有类似的FIFO，优先级算法，短作业优先等算法来确定I/O调度顺序。 设备保护 操作系统需要实现文件保护功能，不同的用户对各个文件有不同的访问权限（如：只读、读和写等）在UNIX系统中，设备被看作是一种特殊的文件，每个设备也会有对应的FCB（文件控制块，存储文件在磁盘中的相关信息）。当影虎情趣访问某个设备时，系统会根据FCB中记录的信息来判断该用户是否有相关的访问权限，以此实现“设备保护”的功能。 总结 这里只是介绍了部分简单的功能，下面将逐一介绍比较复杂的功能。 假脱机技术（SPOOLING技术） 产生的原因 在手工操作阶段主机直接从I/O设备获得数据，由于设备速度慢，主机速度快，人际速度矛盾明显，主机需要浪费很多时间来等待设备。而在批处理阶段，就引入了假脱机技术，缓解了cpu与慢速I/O设备之间的速度矛盾，另一方面，即使cpu在忙碌，也可以提前将数据输入到磁带，技术速度慢的输出设备正在忙碌，也可以提前将数据输出到磁带。 输入井和输出井 假脱机技术又称为&quot;SPOOLING技术&quot;，使用软件方式模拟脱机技术，SPOOLING系统的组成如下： 这样主机就不在需要长时间等待输入了，而是直接从磁盘即输入井中拿取数据，而慢速的技术输入就是将数据放入到磁盘中，这样就类似于吃自助餐，服务员（用户层软件）将数据直接提前放到餐台（磁盘）上，而餐客（设备）需要数据时就直接从餐台上拿取相应的数据了，输出亦是如此，这样就减少了长时间的数据等待时间了。 输入/输出缓冲区 实际上就是上面所讲的餐台。 共享打印机原理 这个就是我们之前讲的实现数据区共享以防止死锁的技术应用。 独占式设备：只允许各个进程串行使用设备，一段时间内只可以满足一个进程的请求。 共享设备：允许多个进程“同时”使用设备（宏观上是同时使用，实际上微观上是交替使用，即并发使用）可以满足多个进程的使用请求。 当多个用户进程提出要输出打印的请求时，系统会答应他们的请求，但是并不是真正把打印机分配给他们，而是由假脱机管理进程为每个进程做两件事： 在磁盘输出井中为每一个进程分配一个空闲缓冲区（也就是说，这个缓冲区是在磁盘上的），并将要打印的数据送入其中。 为用户进程申请一张空白的打印请求表，并将用户的打印请求填入表中（其实就是用来说明用户的答应数据存放位置等信息的），再将该表挂载到假脱机文件队列上。 当打印机空闲时，输出进程会从文件队列的队头取出一张打印请求表，并且根据表中的要求将要打印的数据从输出井传送到输出缓冲区，在输出到打印机进行打印。用这种方式可依次处理完全部的打印任务。 因此假脱机文件队列实际上就是打印任务队列，假脱机技术(SPOOLING技术)可以把一台物理设备虚拟成逻辑上的多台设备，可将独占式设备改造成共享设备。 总结 设备的分配与回收 设备分配时应考虑的因素 设备的固有属性可以分为三种： 独占设备：一个时段只能分配给一个进程（如打印机） 共享设备：可同时分配给多个进程使用（如磁盘），各进程往往是宏观上同时共享使用设备，而微观上是交替使用，即并发性的特点。 虚拟设备：采用SPOOLING技术将独占设备改造成虚拟的共享设备，可同时分配给多个进程使用（如采用SPOOLING技术实现的共享打印机） 设备的分配算法：FIFO,优先级算法，短作业优先SJF等。 从进程的安全性上考虑，设备分配有两种方式： 安全分配方式：为进程分配一个设备后就将进程阻塞，本次I/O完成后才将进程唤醒。这样一个时间段内每个进程只能使用一个设备，优点是破坏了请求和保持条件，不会死锁，缺点是对于一个进程来说，cpu和I/O设备只能串行工作，效率较低。 不安全分配方式:进程发出I/O请求后，系统为其分配I/O设备，进程可以继续执行，之后还可以发出新的I/O请求，只要某个I/O请求得不到满足时才将进程阻塞。这样一个进程可以同时使用多个不同的I/O设备，优点是进程的计算任务和I/O任务可以并行处理，是进程迅速推进，缺点是有可能发生死锁（死锁避免，死锁的检测和解除来解决此问题）。 静态分配和动态分配 静态分配：进程运行前为其分配全部所需资源，运行结束后归还资源，破坏了请求和保持条件，不会发生死锁。 动态分配：进程运行期间动态申请设备资源，但是可能会发生死锁。 设备分配管理中的数据结构 一个管道可以控制多个设备控制器，每个设备控制器可以控制多个设备。其中每个层次都会有自己的信息表如下： 设备控制表（DCT） 在进程管理中我们知道系统会根据阻塞原因的不同，将进程PCB挂到不同的阻塞队列中。因此设备队列的队首指针指向的一定是因为等待这个设备而导致阻塞的PCB队列。 控制器控制表（COCT） 每个设备控制器都会对应着一张COCT，操作系统会根据COCT的信息对控制器进行操作和管理。 通道控制表（CHCT） 每一个通道也都会对应着一个CHCT，操作系统会根据CHCT的信息对通道进行操作和管理。 系统设备表（SDT） 记录了系统中全部设备的情况，每一个设备对应一个表目。 设备分配的步骤 根据进程请求的物理设备名查找SDT（注意物理设备名是进程请求分配设备时提供的参数） 根据SDT找到DCT，若设备忙碌则将进程挂到设备等待队列中，不忙碌则将设备分配给进程 根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列，不忙碌则将控制器分配给进程。 根据COCT找到CHCT，若通道忙碌则将进程PCB挂到控制器等待队列，不忙碌则将通道分配给进程。 只有设备、控制器、通道三者都分配成功时，这次设备分配才算成功，之后便可以启动I/O设备进行数据传送了。 有没有什么缺陷？如何解决？ 有，我们发现进程请求时需要提供“物理设备名”，但是底层细节对用户不透明，不方便编程，因此如果一旦换了物理设备，那么这个程序就无法运行了，并且如果进程请求的物理设备正在忙碌，那么即使系统中还有同类型的设备，这个进程也会阻塞等待。改进方法就是建立逻辑设备和物理设备之间的映射机制，用户编程时只需提供逻辑设备名。 设备分配步骤的改进 根据进程请求的逻辑设备名查找SDT（注意物理设备名是进程请求分配设备时提供的参数） 根据SDT找到DCT，若设备忙碌则将进程挂到设备等待队列中，不忙碌则将设备分配给进程 根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列，不忙碌则将控制器分配给进程。 根据COCT找到CHCT，若通道忙碌则将进程PCB挂到控制器等待队列，不忙碌则将通道分配给进程。 逻辑设备表LUT建立了逻辑设备名和物理设备名之间的映射关系，当某个用户进程第一次使用设备时使用逻辑设备名向操作系统发出请求，操作系统会根据用户进程指定的设备类型（逻辑设备名）查找系统设备表，找到一个空闲设备分配给进程，并在LUT中增加相应表项。如果之后用户进程再次通过相同的逻辑设备名请求使用设备，则操作系统会通过LUT表即可知道用户进程实际要使用的是哪个物理设备了，并且也能知道该设备的驱动程序入口地址了。但是我们前面也讨论过：如果整个系统就一张LUT，那么各个用户所使用的逻辑设备名不允许重复，适用于单用户操作系统，而每个用户都拥有一种LUT，那么不同用户的设备逻辑名可以重复，使用于多用户操作系统。 总结"},{"title":"经典同步问题(2)","path":"/wiki/操作系统笔记/经典同步问题(2)/index.html","content":"吸烟者问题 问题描述 假设有一个系统有三个抽烟者进程和一个供应者进程，每个抽烟者不停地卷烟并抽掉他，但是要卷烟起并抽掉一只烟需要有三种材料：烟草，纸和胶水。三个抽烟者中，第一个拥有烟草，第二个拥有纸，第三个拥有胶水。供应者进程无限的提供这三种材料但是每一次供应者只在桌子上放两种材料，而拥有剩余那种材料的抽烟者才能卷一根并抽掉他，并给供应者一个信号告诉完成了，此时供应者就会将另外两种材料放在桌子上一直重复使得三个抽烟者进程可以轮流的抽烟。 问题分析 我们分析一下题意不难发现这是一个生产者对应多个消费者的问题，并且特殊地是这个生产者可以生产多种产品，如上图： 生产者可以生产三种产品分别是： 纸+胶水 烟草+胶水 烟草+纸 而消费者各自有不同的产品需求分别对应消费生产者的三种产品同时这样看来实际上还是缓冲区只能容下一个产品，并且一次性只能有一个进程访问缓冲区，所以事件关系如下： 桌子上有组合1的产品-&gt;第一个抽烟者取走东西抽烟(同步关系) 桌子上有组合2的产品-&gt;第二个抽烟者取走东西抽烟(同步关系) 桌子上有组合3的产品-&gt;第三个抽烟者取走东西抽烟(同步关系) 抽烟者发出完成信号-&gt;供应者将下一种产品组合放到桌子上(同步关系) 各进程互斥访问临界资源–桌子(互斥关系) 思考：需要设置，互斥信号量吗？ 前面我们提到过对于一个临界资源并且每次都只有一个同步信号量为1的情况可以不设置互斥信号量，此问题也可以不设置互斥信号量。 代码如下： 首先声明信号量其次声明索引值i用来判断供应者该提供那种产品了 12345semaphore offer1=0;//桌子上组合1的数量semaphore offer2=0;//桌子上组合2的数量semaphore offer3=0;//桌子上组合3的数量semaphore finish=0;//抽烟是否完成int i=0;//用于实现&quot;三个抽烟者轮流抽烟&quot; 供应者代码： 123456789101112131415161718provider()&#123; while(1)&#123; if(i==0)&#123; //将组合1的产品放到桌子上 V(offer1); &#125; else if(i==1)&#123; //将组合2的产品放到桌子上 V(offer2); &#125; else if(i==2)&#123; //将组合3的产品放到桌子上 V(offer3); &#125; i=(i+1)%3; P(finish); &#125;&#125; 抽烟者代码： 思考：为什么这次供应者的P操作放到了后面？ 因为初始化时finishi为0，而桌子上一开始是没有组合产品的，所以供应者现需要进行一次产品放置，所以此时P操作放在了最后面实际上就等于放在了下一次循环的开头，如果初始化时finish为1那么P操作就应该放在最前面。并且对于这种可以生产多个产品的供应者一定注意V操作对应着不同事件下。 读者-写者问题 问题描述 有读者和写者两组并发进程，共享一个文件，当两个或两个以上的读进程用时访问共享数据时不会产生副作用，但若某个写进程和其他进程(读进程或者写进程)同时访问共享数据时则可能导致数据不一致的错误，因此要求： 允许多个读者进程同时对文件进行读操作 只允许一个写者往文件中写信息 任一一个写者在完成写操作之前不允许其他的读者或者写者工作 写者操作执行写操作之前需要保证已有的读者和写着进程全部已经退出共享文件 注意：这里和生产-消费者最大的不同是共享数据不会被取走所以共享资源不会减少甚至清空，因此多个读者可以同时访问共享数据。 问题分析 首先我们知道无论是写进程-写进程之间还是写进程-读进程之间都是互斥访问共享文件的。而读进程-读进程之间是不互斥的。所以我们首先肯定是需要设置一个互斥变量rw用来保证写-写，写-读之前互斥，同时为了记录此时正在有几个读进程执行我们设置一个参量count用来记录读进程个数，这样我们知道只有count==0时此时写进程才可以进行写操作，当count&gt;0时说明还有读操作进行，每次新加入一个读进程count就++，每次退出一个读进程count–，只有count==0时说明此时没有读进程才能进行写操作。所以我们的代码如下： 首先声明一个互斥信号量和一个记录参量： 12semaphore rw=1;//实现对共享文件的互斥访问int count=0;//记录当前有几个进程在访问文件 那么写进程代码： 1234567writer()&#123; while(1)&#123; P(rw);//写之前“加锁” write....//进入共享区域进行写操作 V(rw);//退出共享区域并“解锁” &#125;&#125; 一个读进程 12345678910111213reader()&#123; while(1)&#123; if(count==0)&#123;//如果是第一个读进程那么需要进行判断&quot;上锁&quot; P(rw);//&quot;上锁&quot; &#125; count++;//读进程个数加一 read....//进入共享区域进行读操作 count--;//读完，退出读进程个数减一 if(count==0)&#123;//如果是最后一个读进程那么退出前解锁 V(rw);//&quot;解锁&quot; &#125; &#125;&#125; 思考：为什么对于读进程只有count==0时进行PV操作？ 我们思考，rw实际上是一个用来使得写-写和写-读之间互斥的信号量，所以对于每一个写进程都需要进行P,V操作保证每次共享区域都只有自己一个写进程。而对于读者进程来说，如果他是第一个要进入共享区域的进程，那么他唯一需要做的就是保证此时共享区域里面没有写进程即可，即使里面已经有读进程了没关系因为他们之间不是互斥的。所以对于count!=的读进程来说他不需要进行P操作检验就可以直接进入共享区域因为此时说明共享区域里面有读进程且没有写进程，同理对于退出也是，只有自己是最后一个读进程退出时才需要解锁否则其他读进程直接退出即可。 思考：上面的代码有没有什么问题？ 我们想一下，如果此时已经有一个读进程进入共享文件区域了，那么毫无疑问rw=0已经上锁了，此时如果又来了一个读进程我们知道由于count!=0他是不用经过P操作检查就可以直接进入共享区域的，但是我们现在就想对这个后来的读进程进行P操作检查，那么肯定他是通不过的，所以对于第一个if语句的作用除了要做到让第一个读进程上锁同时还要做到避免让其他后来的读进程被P操作检查因为一旦检查他们都是不可能进入的，但是现在上面的if语句操作代码有bug做不到第二个作用。原因如下：我们考虑现在有两个读进程在并发执行此时count==0，好巧不巧第一个读进程刚刚通过if(count==0)的检验还没来得及进行P上锁时间片用完了切换到了第二个读进程他也恰巧刚刚通过if(count==0)的检验也要进行P操作，此时说明这两个读进程都要进行P操作检查此时第二个进程的时间片也用完了有切换到了第一个进程第一个进程此时rw=1可以进入他通过了P操作检查并将rw设置为了0，此时他又用完时间片了切换到进程2PCB状态信息记录着他也要经过P检查，但是此时rw=0!完了第二个度进程是进不去的他过不了P进程的检查所以就一直阻塞到本轮所有的读进程结束才能进入。此时很明显是有问题的，if语句没有做到后来的读进程不用P操作检查。同理对于第二个if操作也会有问题，仔细想想就知道会造成最后一个进程还没有离开共享资源区，倒数第二个离开的进程就已经解锁了，这也很致命，所以我们需要对两个If语句进行优化。 思考：如何优化两个if语句？ 分析易知此时会发生这种bug是因为此时的if判断语句和后面的操作语句不想P,V操作可以做到检查和操作一气呵成的原子性特点，所以我们需要借助P,V做到if语句判断和操作一气呵成，因此我们可以再设置一个互斥信号量来实现各读进程对count的访问是互斥的。所以优化后的代码如下： 我们需要再声明一个互斥信号量： 1semaphore mutex=1;//用于保证对count变量的互斥访问 读进程的代码： 1234567891011121314151617reader()&#123;\twhile(1)&#123; P(mutex);//对count访问上锁 if(count==0)&#123;//如果是第一个读进程那么需要进行判断&quot;上锁&quot; P(rw);//&quot;上锁&quot; &#125; count++;//访问的进程加一 V(mutex);//对count访问解锁 read... P(mutex);//对count访问上锁 count--;//对count访问解锁 if(count==0)&#123;//如果是最后一个读进程退出那么退出前需要解锁 V(rw);//“解锁” &#125; V(mutex);//对count访问解锁 &#125;&#125; 思考：上面的代码还有什么问题？ 我们再仔细想一想，貌似这种代码还是有一个缺陷，我们想每次中途都可以来新的读进程并且如果已知读进程个数不是0那么rw就一直不解锁，也就意味着只要有读进程正在读并且此时又来了新的读进程，那么写进程就得已知阻塞等到所有的读进程全部读完并且没有再来新的读进程，此时rw解锁后写进程才能继续写，这很容易就造成写进程饥饿（即已知有新的读进程源源不断的断断续续的来）。所以上面这种算法是读进程优先的，所以我们需要优化一下代码，即不允许读进程一直插队，当写进程正在等待前面的读进程时新来的读进程只能排在正在的写进程后面等待下一轮的读进程。所以事先代码如下，可能有点不太好理解： 首先还是需要声明信号量，此时还要在上面的基础上在新添加一个信号量w用于实现“写优先”如下： 这样上面加粗部分的就是新家的互斥信号锁，我们来分析以下为什么加上了这个信号量就避免了写进程饥饿。假设现在来了一个读进程R1，他将w上锁然后进行count++和rw上锁等功能，然后在解锁w,此时来了一个写进程W1，他可以通过w的P检验并上锁，此时他还不能通过rw的P操作，因为R1还没有解锁，此时W1在阻塞等待，此时又来了一个读进程R2如果按照上面原先的读者优先算法读进程可以立即进入共享区域执行读操作，但是此时他因为不能通过w的P操作（因为此时W1没有解w锁）所以R2也只能阻塞等待，当R1读完退出临界资源后将rw解锁此时W1可以进行写操作了，此时R2正在阻塞等待W1完成，只有当写进程完成并解锁w后R2才可以开始访问共享文件。我们发现从原来的R1-W1-&gt;R1-R2-W1变成了R1-W1-&gt;R1-W1-R2即读进程不能随意插队了也就是读写进程公平等待了避免了写进程饥饿的风险。 总结一下我们发现实际上上面这种算法并不是&quot;写优先&quot;算法，他只是做到了保证读进程和写进程公平排队而已。所以有的教材也把这种算法叫做&quot;读写公平法&quot;。 总结：各个信号量的作用？ 我们一定要理解上面的代码衍生过程这样才可以深刻记忆各个信号量之间的作用。 rw-保证写写，写读的互斥访问 metux-保证读进程互斥访问count w-保证读写公平排队 十分注意count不是信号量他只是一个记录读进程数量的参量。 哲学家进餐问题 问题描述 一张圆桌上面坐着5名哲学家，每两个哲学家之间的桌子上摆着一个筷子，桌子的中间是一碗米饭，哲学家们倾注毕生精力用于思考和进餐，哲学家在思考时，并不影响他人，只有当哲学家饥饿时，才试图拿起左、右两根筷子（一根一根拿起）。如果筷子已经在他人手上时，则需要等待。解饿的哲学家只有同时拿起两根筷子才可以开始进餐，当进餐完毕后，放下筷子继续思考。 问题分析 从上面的描述中我们可以知道其实筷子就好像临界资源，每次都只允许一位哲学家进程访问，所以毋庸置疑这是互斥关系。5位哲学家与左右相邻对其中间的筷子是互斥访问的，但是这个不同于之前的生产者-消费者问题或者多生产者-多消费者问题亦或是吸烟者问题，此时一个进程需要同时访问两个临界资源。如何避免临界资源分配不当造成的死锁现象是哲学家问题的关键所在。 这里我们先设置互斥信号量，定义互斥信号量数组chopsticks[5]={1,1,1,1,1}（互斥信号量初始化为1）用于实现对5个筷子的互斥访问。并对哲学家按照0~4编号，同时哲学家i左边的筷子编号为i，右边的筷子编号为(i+1)%5。 思考：同时先左后右可取吗？ 即每一个哲学家都是先尝试拿左边的筷子然后在尝试拿到右边的筷子，如果拿不到就放下筷子。此时代码如下： 123456789101112semaphore chopsticks[5]=&#123;1,1,1,1,1&#125;;pi()&#123; while(1)&#123; P(chopsticks[i])//拿左 P(chopsticks[(i+1)%5])//拿右 eat... V(chopsticks[i])//放左 V(chopsticks[(i+1)%5])//放右 think... &#125;&#125; 很明显这种方法不妥当，会造成死锁最终谁也吃不上饭。因为当所有人都拿起左边的筷子时所有哲学家都不可能能拿到右边的筷子，所以所有哲学家最终都放下筷子重新再按照此方法尝试下去，最终谁也吃不上饭。这种循环等待右边的人放下筷子(阻塞)就是造成&quot;死锁&quot;的原因。 思考：加上某些条件可以避免死锁吗？ 可以我们尝试每次都只限制至多4名哲学家同时吃饭，这样就会由5双筷子4个人分，至少保证了有一名哲学家可以吃饭，不会在造成死锁现象。但是貌似效率太低，究其原因是虽然某些哲学家不能吃上饭但是还是会拿起一根筷子占为己有进行尝试。即某些进程明明已经不可能访问到临界资源了却还是占用了一部分临界资源。我们最好能够避免不能立刻执行的进程占用临界资源。 思考：加上某些条件可以避免进程占用不必要的临界资源？ 我们可以要求奇数号哲学家先拿左边的筷子，然后再拿右边的筷子，而偶数号哲学家正相反。这样可以保证当相邻的奇偶数号哲学家都想吃饭时，只会有一个哲学家获得第一个筷子，而另一名哲学家连第一个临街资源都没有获得就阻塞了，这样就避免了占有一支后再等待另一支的情况了。 思考：还有没有其他方法？ 归根结底上面的方法都是在还没能确保能获得全部临界资源时就拿起了部分临界资源然后再尝试获取另一部分临界资源，这样就可能会造成大家都拿到了一部分临界资源然后等待。所以我们可以规定只有进程一次性可以获得全部临界资源才执行即仅当一个哲学家左右两支筷子都可以使用时才允许他抓起来。这种方法貌似最合适代码如下： 123456789101112131415semaphore chopsticks[5]=&#123;1,1,1,1,1&#125;;semaphore mutex=1;//互斥的拿筷子pi()&#123; while(1)&#123; P(mutex); P(chopsticks[i])//拿左 P(chopsticks[(i+1)%5])//拿右 V(mutex); eat... V(chopsticks[i])//放左 V(chopsticks[(i+1)%5])//放右 think... &#125;&#125; 我们对比之前的发现只是在取筷子时加上了互斥锁，这样各个哲学家拿筷子这件事必须是互斥进行的，这样就保证了即使一个哲学家在拿筷子时如果拿到一半被阻塞了，也不会有别的哲学家会继续尝试拿筷子，这样的话，当前正在吃饭的哲学家放下筷子后，被阻塞的哲学家就可以获得等待的筷子了。我们发现这种方法虽然可以避免死锁，但是貌似和上面的思路不太一样，实际上他并没有真正的实现满足有两个筷子的哲学家尝试吃饭，而是保证了每次都只允许一名哲学家尝试拿筷子，如果他能一次性拿齐两双就吃饭如果拿不齐就阻塞等待，并且在他等待期间其他哲学家也禁止尝试拿筷子，必须等到这个阻塞的哲学家能够拿齐筷子吃饭后其他哲学家才可以尝试。这样的方法可以至少保证有一个哲学家能够进餐同时最好情况还可以有两名哲学家同时进餐。 思考：上面的方法有没有什么瑕疵？ 我们发现上面这种方法并不能保证只有两边的筷子都可用湿才允许哲学家拿起筷子。例如： 此时1号哲学家已经尝试拿齐了右边筷子(2号筷子)，但是由于0号此时在吃饭所以1号筷子不能拿齐，所以此时1号哲学家不拿起筷子进入阻塞等待，而此时虽然2号哲学家可以同时拿齐2,3号筷子，但是由于mutex此时在1号筷子处为1，其他哲学家此时都不能拿筷子，所以2号哲学家此时虽然可以同时拿齐两双筷子但是却没有资格去尝试拿，而1号哲学家虽然不能用时拿齐两双筷子但是他却可以等待0号进程吃完然后拿齐1,2号筷子吃饭。 思考：三种方法哪种更好? 对于上面所说的三种方法： 每次最多允许4名哲学家拿筷子 奇偶号哲学家反方向拿筷子 互斥锁保证每次一个哲学家拿筷子(两个筷子都能拿才有资格拿筷子) 实际上没有好坏之分，都是最好情况为同时2名哲学家进餐，对于多个进程访问临界资源并且一个进程需要同时访问两个临界资源的变式题参考哲学家问题。 各种问题总结 问题类型 生产者-消费者问题：一个临界资源，两个进程互斥访问，互斥+同步关系 多生产者-多消费者问题：一个临界资源，多个进程互斥访问，互斥+同步关系 吸烟者问题：一个临界资源，一个生产者-多个消费者问题，互斥+同步关系 读者-写者问题:一个临界资源，部分进程互斥访问，互斥+同步关系同时有优先级问题 哲学家进餐问题：多个临界资源，进程需要两个临界资源，纯互斥关系"},{"title":"虚拟内存管理","path":"/wiki/操作系统笔记/虚拟内存管理/index.html","content":"请求式分页管理方式 前面我们介绍了虚拟内存，并且介绍了请求式管理的由来，那么接下来就详细介绍一下请求式分页管理方式。请求分页存储管理与基本分页存储管理的主要区别是在程序执行过程中，当所访问的信息不在内存时，由操作系统负责将所需信息调入内存（这里操作系统要提供请求调页功能把缺失页面从外存调入内存）后继续执行程序。如果内存空间不足，由操作系统负责将内存中暂时用不到的信息换出到外存。（操作系统要提供页面置换的功能，将暂时用不到的页面换出外存).这里会涉及到页面机制，缺页中断机构和地址变换机构，我们在下面一一进行介绍。 页表机构 与基本分页管理相比，请求分页管理中，为了实现“请求调页”，操作系统需要知道每个页面是够已经掉入内存，如果还没有调入内存，那么也需要知道该页面在外存中存放的位置。并且当内存空间不够时，要实现“页面置换”，操作系统需要通过某些指标来决定到底换出哪个页面（页面置换算法），有的页面没有被修改过，就不用再浪费时间写回外存。有的页面修改过，需要将外存中的旧数据覆盖，因此操作系统需要记录各个页面是否被修改的信息。如下： 从上图我们可以看出请求分页存储管理的页表中存储了所有的页表，即使没有放入到内存中页记录在一个页表项。例如x现在就没有在内存中。 缺页中断机构 假设现在某进程要使访问的逻辑地址为（页号，页内偏移量）=（0，1024），那么经过查表发现此时0号页不在页表中，所以产生一个缺页中断，然后然后由操作系统对缺页中断进行处理。首先是将缺页的进程阻塞，然后放入阻塞队列，调页完成后再将其唤醒，放回就绪队列。如果内存中还有空闲块，那么就为进程分配一个空闲块，将所缺页面装入该快，并修改页表中相应的页表项。 如上图就是将0号页表项内存块号修改为a并且状态为1，并且还要将x号块内的页面装入内存中去： 思考：如果内存块此时是满的怎么办？ 那么就会调用页面置换算法将一些符合调出条件的页面写回外存，所以只有内存满的时候才会触发调换算法（其实很容易理解，写回外存肯定是有时间开销的，所以只有满的时候迫不得已了才会增加时间开销为新来的页腾地儿），类似的实际上TLB等快表也有这个调出暂时长时间不命中的页表项的算法。并且如果内存满了，调出某个页面时，如果这个页面在内存期间被修改过，那么需要将其写回外存覆盖旧数据，否则未修改过的页面就不用了写回外存了，直接淘汰掉就好。毕竟外存块x处还存有旧数据的页，所以我们也可以看出调入是从外存快复制一份页面进入内存块，调出的意思是从内存淘汰的意思，当修改过的时候，这个被淘汰的页面还肩负着通知外存块更新数据的使命所以还需要写回外存（当然肯定是有额外的时间开销的），当没有被修改过（不意味着没被使用，可能在内存期间一直在在被读也发挥作用了）那么就直接扔出内存即可。 思考：缺页中断属于内中断的哪一类？ 我们知道缺页中断是因为当前执行的指令想要访问的目标页面未调入内存而产生的，因此属于内中断。一条指令在执行期间，可能会产生多次缺页中断（如copy A to B,即将逻辑地址A的数据复制到逻辑地址B，而A,B属于不同的页面，就可能产生两次中断，即A的页不在内存中，B的页也不再内存中，可能会产生两次中断）。 我们可以看出缺页中断属于内中断中的故障，但是是可以被程序处理恢复的。 地址变换机构 因为不能保证逻辑地址访问的页在内存中，所以我们首先是需要确定页是否在页表中，如果不在还需要调入页面并修改表项，当然如果内存满了，那么还需要页面置换。所以新增的步骤有： 当然后面的步骤就是根据页号找到内存块号了，然后拼接物理地址最后再访问目标内存单元。 这里我们尤其要注意TLB的机制，他只存放现在在内存中的刚刚被访问过得页表项，所以TLB里的页一定是存在的。 这里面的一些小细节直接用图片给出，这里我们一定要注意绿框中的提示要点。我们可以看出当产生缺页中断时换出旧页面并调入新的页面到内存块后发生了几个重要的事件： 快表直接加上新的表项 不是直接通过慢表拼接出了物理地址然后访存，而是又重新来了一遍这次快表命中了，然后通过快表拼接出了物理地址进行访存。 思考：为什么没有查慢表和查快表一起进行？ 可以，但是没必要，因为此时慢表大概率会慢于快表（如果不是还要TLB作甚）并且查慢表还会出现缺页中断，并行查询也快不了多少。 思考：为什么当有缺页中断时会通过快表命中？ 这就是进程中断的原理了，当在某一个指令处中断时如果进程阻塞了PCB会记录上次停止的位置，然后当进程再次执行时PCB会恢复到上一次停止的指令处然后重新执行中断的指令（大部分情况下），所以此时还会再执行一遍这个指令的逻辑地址但是此时就会通过快表命中了。 总结 页面置换算法 这部分超级重要，我会做适当的扩充，毕竟王道讲的实在是太少了。一定要透彻了解并且会计算。首先我们明确一个目的，由于页面的换入换出需要磁盘I/O，会有较大的开销，因此优秀的页面置换算法应该追求更少的缺页率。 最佳置换算法（OPT） 也叫作最优置换算法（OPT,Optimal):每次选择淘汰的页面将是以后永不使用，或者在最长时间内不再被访问的页面，这样就可以保证最低的缺页率。这里我们知道肯定是最理想的情况，因为这个算法要求我们需要提前知道未来这个所要被调出的页面就是最长时间或者永久不可能再被使用的页面，但是未来不可预测，所以这个算法实际中不可能实现，但是我们需要学习算法思想（毕竟万一未来我们量子预测到未来这个算法那不就可以实现了吗😝） 例题：假设系统为某进程分配了三个内存块，并考虑有以下页面号引用串（会依次访问这些页面）：7,0,1,2,0,3,0,4,2,3,0,3,2,1,2,0,1,7,0,1 那么最终的过程如下： 首先不用想，第一次填入时肯定都是缺页的（这个很重要容易被忽视）所以内存块填入7,0,1就先缺页3次，然后接下来到2，我们需要调出一个页面，此时我们看一下未来的访页顺序发现7最长时间不会被访问了，所以调出7接入2又缺页1次，继续执行到3发现又该调出了，还是看未来的顺序，调出1,…一直这样看未来顺序调出页面最终缺页率还是可观的才45%。这里我们可以看出缺页率的计算公式： 缺页率=缺页中断次数/页面引用次数缺页率=缺页中断次数/页面引用次数 缺页率=缺页中断次数/页面引用次数 最近未使用页面置换算法（NRU） 最近未使用页面置换算法（Not Recently Used)和OPT很相似，既然我不能知道未来的顺序，那么我就往回看，根据经验分析和局部性原理我们知道如果一个页面很久没有被是用来，那么大概率他就会很长时间或者不会再被使用了（根据历史经验推测未来），所以我们每次都选择调出最近未使用的页面。这里我们的做法如下： 当一个页面被访问(读)时设置R(read)位，页面被写入(修改)时设置M(modificate)位。 当启动一个进程时，它的所有页面的两个位都由操作系统初始化为0，R会被定期地（比如在每次时钟中断时）清零以区别最近没有被访问和被访问的页面。 那么当发生缺页中段时就会检查页面，其中页面可以分为4类： 第0类：没有被访问也没有被修改过的页面（R=M=0) 第1类：没有被访问但是已被修改过的页面（R=0,M=1） 第2类：已被访问过但是没有被修改过的页面（R=1,M=0） 第3类：已经被访问过并且也被修改过的页面（R=M=1） 每次都是从类编号小的非空类中随机挑选一个页面淘汰。 思考：问什么第2类比第3类优先被淘汰？ 首先请读一下算法名字，他强调的就是最近未使用，所以重在根据是否最近被访问过来决定页面的重要性，所以2类先被淘汰，毕竟在一个时间嘀嗒中（大约20ms）淘汰一个没有被访问过的已被修改过的页面比淘汰一个被频繁使用的“干净”（没被修改过）的页面好，所以是否“干净”（即是够被修改过）只是一个次级判断条件。其实NRU这种算法优点就是易于理解和有效实现并且虽然性能不是最好的但是已经够用了。 先进先出页面置换算法（FIFO） 同样借鉴了NRU的思路，既然每次都淘汰最近未被使用的页面，那么大多数情况先来的一般会在内存中待较长的时间，根据时间局部性原理，一般他就是那个最近未被使用的页面。所以FIFO算法就是每次选择淘汰的页面是最早进入内存的页面。 实现方法：把调入内存的页面根据调入的先后顺序排成一个队列（FIFO队列），需要换出页面时就选择队头页面即可。所以队列的最大长度取决于操作系统为进程分配了多少个内存块。 例题： 缺页率=9/12=75%,说实话优点小高。那么你一定想到了如果多分几个内存块是不是缺页次数会变得更少，答案是未必，如下： 缺页率=10/12=83%缺页率反而更大了。只有FIFO会产生这种Belady异常现象，所以FIFO算法虽然实现简单，但是该算法与进程实际运行时的规律是不适应的，因为先进入的页面也有可能经常被访问，所以算法性能差，不推荐使用。 第二次机会页面置换算法（SC） 第二次机会页面置换算法（Second Chance)是对FIFO算法进行的一种优化改进。修改的思路其实很简单，就是避免将最先进来的页面却被访问的页面优先调出，所以只需要设置一位R位，如果是0那么这个页面就是既老还没有被使用，可以直接置换掉。如果是1那么就说明这个页面虽然老但是被访问过，所以将R为置为0然后把这个页面放到队尾修改装入的时间就好像它刚被装入一样（即拥有了第二次机会），然后继续搜索队头直至R=0的页面换出。 第二次机会算法就是在寻找一个最近的时钟间隔内没有被访问过的页面。如果所有的页面都被访问过了，即队列中所有的页面R都是1了，那么这个算法就是纯FIFO算法了，所以为了避免这种情况此时操作系统会一个接一个地将每一个页面都移动到队尾并将R设置为0。最后又回到原来的表头页面并且此时R位都是0，因此这个页面会被淘汰，所以这个算法总是可以结束不会出现死循环的。 思考：NRU和SC都有R位有什么区别？ NRU和SC的R位都是被访问的意思，但是NRU的R位是最近被访问的概念，而SC的R为只是表示被访问过的意思，所以NRU需要一个时间嘀嗒来设置R在一个时间段后清零，而SC就不需要只是当队列全是1时所有页面都在绕一圈然后R都变为0。 时钟页面置换算法（CLOCK） 对第二次机会算法的改进，我们发现第二次机会算法总是需要在链表中移动页面，这很低效没必要。所以更好的做法是把所有的页面都保存在一个类似钟面的环形链表中，一个表指针指向最老的页面（即最先进入内存的页面）。当发生缺页中断时，首先检查指针指向的页面，如果R位是0，那么就淘汰该页面，并把新的页面插入到这个位置，然后把表指针移到下一个页面，如果R位是1就将R位置为0然后检验下一个位置，重复这个过程一直到找到一个R位为0的页面为止。当所有的R位都是1时，则指针转一圈将所有的页面的R位都清为0。 我们发现实际上CLOCK算法和SC算法思想一模一样，只不过是换了一个数据结构来减少操作的开销。并且我们发现简单的CLOCK算法选择淘汰一个页面最多经过两轮扫描。 思考：能否进一步优化CLOCK算法？ 我们发现NRU不止讨论了是否最近被访问过的问题，还加了一个是否被修改过的判断指标，当都没有被访问时优先会淘汰没有被修改过的页面，这是因为毕竟修改过的页面被淘汰时还需要写回外存有更大的开销不如再等一等万一他一会被访问了不就不用写回外存了嘛。所以优先淘汰的是R=M=0的，那么CLOCK算法也可以借鉴这种思想。 改进型的时钟置换算法（CLOCK v2.0) 因此，除了考虑一个页面最近有没有被访问过之外，操作系统还应考虑页面有没有被修改过。在其他条件均相同时，应该优先淘汰没有被修改过得页面，避免I/O操作。这就是改进型的时钟置换算法的思想。所以也是有下面这四类： 第0类：没有被访问也没有被修改过的页面（R=M=0) 第1类：没有被访问但是已被修改过的页面（R=0,M=1） 第2类：已被访问过但是没有被修改过的页面（R=1,M=0） 第3类：已经被访问过并且也被修改过的页面（R=M=1） 页面的状态用（R,M）表示，所以（1,1）表示最近被访问过且被修改过。 算法规则：将所有可能被置换的页面排成一个钟面型的循环队列。 第一轮：从当前位置开始扫描到第一个（0,0)的帧用于替换，本轮扫描结束。 第二轮：前提是第一轮扫描失败（即没有(0,0)），那么重新扫描，查找第一个（0,1)的帧用于替换。本轮会将所有扫描过得帧访问位设置为0（即第二轮扫描后（1,0)-&gt;(0,0),(1,1)-&gt;(0,1)。 第三轮：前提是第二轮扫描失败（即没有(0,0)和（0,1)），那么重新扫描，查找第一个（0,0）（此时的(0,0)是原先的(1,0)）的帧用于替换。本轮不修改任何标志位。 第四轮：前提是第三轮扫描失败（即没有(0,0),(0,1)和(1,0)），那么重新扫描，查找第一个（0,1)（此时的(0,1)是原先的(1,1)）的帧用于替换。并且此轮一定会扫描成功。 由于第二轮已经将所有的访问位设置为0，因此经过第三轮、第四轮扫描一定会有一个帧被选中，因此改进型CLOCK算法选择一个淘汰页面最多会进行四轮扫描。 思考：改进型CLOCK和CLOCK最大的区别是什么？ 我们仔细对比一下两者的算法思想，我们发现虽然都是使用类似钟的循环队列数据结构，但是算法思想却截然不同，对于简单的CLOCK使用的是SC的思想，而改进型的时钟页面置换算法使用的是NRU+CS的算法思想但是更贴向NRU。 最近最少使用页面置换算法（LRU） 最近最少使用页面置换算法（LRU，Least Recently Used）：每次淘汰的页面是最近最久未使用的页面。 实现方法：赋予每个页面对应的页表项中，用访问字段记录该页面自上次被访问以来所经历的时间t，当需要淘汰一个页面时，选择现有页面中t值最大的，即最近最久未使用的页面。很明显这个非常的科学。 我们以一道例题讲解，假设某系统为某进程分配了4个内存块，并考虑到有以下页面号引用串：1,8,1,7,8,2,7,1,8,3,8,2,1,3,1,7,1,3,7 缺页率=6/20=33%很小。在手动做题时，若需要淘汰页面，可以逆向检查此时在内存中的几个页面号。在逆向扫描过程中最后一个出现的页号就是要淘汰的页面。我们发现这个方法太好啦，就用这个吧，但是实际上这个算法不常见，因为需要专门的硬件支持且实现困难，开销极大。 最不常用页面置换算法（NFU） 最不常用页面置换算法（NFU,Not Frequently Used）:用一个软件模拟LRU，该算法将每个页面与一个软件计数器相关联，计数器的初始值为0，每次时钟中断时，由操作系统扫描内存中的所有页面，将每个页面的R位（他是0或1）加到计数器上。这个计数器大体上跟踪了各个页面被访问的频繁程度。当发生缺页中断时，则置换计数器上数值最小的页面。 我们发现NFU不忘记任何事情，比如一个页面之前被频繁访问，导致这个计数器很大，但是后来不访问他了，但是由于计数器的值太大，他也一直不会被置换出去，这个缺点太严重，所以也不推荐。 老化算法 老化算法是对NFU算法的修改，其修改包括两个部分，首先，在R位被加进之前将计数器（二进制数）右移一位（相当于除以2）然后将新来的R位的数加在计数器的最左端的位（即次数最大最优决定权）。这样老化算法的计数器中只有有限位数，如果时钟滴答是20ms,8位一般足够了，加入一个页面160ms都没有被访问过，那么他很有可能就不重要了。 工作集页面置换算法(WS) 在讲解算法的实现之前，我们先了解一下几个概念： 思考：什么是工作集？ 工作集：一个进程当前正在使用的页面的集合称为工作集 思考：什么是颠簸现象？ 颠簸现象：程序每执行几条指令就产生一次缺页中断 思考：什么是请求调页？颠簸现象什么时候频繁出现？ 请求调页：在单纯的分页系统中，刚启动进程时，在内存中是没有页面的，所以当cpu尝试读取第一条指令时就会产生一次缺页中断，使操作系统装入含有第一条指令的页面，其他由访问全局数据和堆栈引起的缺页中断通常会紧接着发生。一段时间后，该进程需要的大部分页面都已经在内存中了，进程开始在较少缺页中断的情况下运行 思考：怎么解决程序初期运行的颠簸现象？ 有不少分页系统会设法跟踪进程的工作集，以确保进程运行以前，他的工作集就已经在内存中了，这样运行初期就不会频繁发生颠簸了。这种方法就叫做工作集模型，大大减少了缺页中断率。在进程前装入其工作集页面也称为预先调页。所以工作集随着时间变化的。 实际上大多数的程序会任意访问一小部分页面，工作集缓慢变化。当程序重新开始时，就有可能根据它上次结束时的工作集对要用到的页面作一个合理的推测，预先调页就是在程序IXUS运行之前预先装入推测的工作集的页面。 思考：纯分页式管理的工作集和请求式分页管理的工作集的区别？ 那么按照以前的方法定义工作集为前1000万次内存访问所使用过的页面的集合，那么现在在请求式分页管理中就应该定义为过去10ms中的内存访问所用到的页面的集合。这样的模型更合适和容易实现。并且要注意每个进程都只会计算自己执行的时间，所以当一个进程在T时刻开始然后在T+100ms的时间段内使用了40ms的CPU，那么对于工作集来说就是40ms。一个程序从他开始执行到当前所实际使用处理机的时间总数就是当前实际运行时间。我们通过这个近似的方法定义进程的工作集就是在过去的t秒实际运行时间中他所访问过的页面的集合。 那么现在我们再来探讨下基于工作集的页面置换算法：就是找出一个不在进程工作集中的页面淘汰他。 每个表项至少要包含两条信息： 上次使用该页面的近似时间 最近是否访问过的R位 过程如下： 扫描所有的页面检查R位。 如果R==1：那么设置上次使用时间为当前实际时间，以表示缺页中断时该页面正在被使用 如果R==0&amp;&amp;生存时间&gt;t:那么表示最近没有被访问过且已经不再工作集了，那么就移除这个页面，用新的页面置换它。扫描会继续进行以更新剩余的表项。所以这次扫描后所有不在工作集的页面都会被淘汰掉。 如果R==0&amp;&amp;生存时间&lt;=t:那么表示这个页面没有被访问过但是却还在工作集中，那么就记录下最长生存时间（就是当前时间-最早被使用时间即已经在呆工作集中的时间）。如果最后没有找到任何一个可以淘汰的即所有页面都是1情况除了现在被扫描的这个页面那么就淘汰这个页面，如果有多个3这种情况的即（1,3都有的情况）那么就淘汰生存时间最长的。如果最终所有页面都是1的情况（包括现在被扫描的这个也是1的情况）那么虽然都满足无需淘汰的条件，但是总是得出去一个，那么就尽可能随机淘汰一个“干净”（没有被修改过的）页面这样就无需进行I/O操作了节省开销。 总结 以上涵盖了大部分置换算法，这里列表总结 页面置换算法 算法规则 优点 缺点 最佳置换算法（OPT） 优先淘汰最长时间内不会被访问到的页面 缺页率小，性能最好 预测未来，无法实现 最近未使用置换算法（NRU） 优先淘汰最近未被访问且干净的页面（需要R,M） 性能优秀，实现简便 ———— 先进先出置换算法（FIFO） 优先淘汰最先进入内存的页面 实现简单 性能差，与规律相悖，可能出现Belady异常 第二次机会置换算法（SC) FIFO改良版，对于最近被访问过得放到队尾获得第二次机会 合理，性能适中 链表操作复杂 时钟置换算法（CLOCK） SC的时钟循环链表形式，规则同上 合理，性能适中 未考虑干净页面的I/O开销 改进型的时钟置换算法（CLOCK v2.0) 和NRU思路规则相似使用的是时钟循环链表形式 合理，性能适中，考虑了I/O开销 有时候扫描次数有点多 最近最少使用页面置换算法（LRU） 每次都淘汰上一次被访问时间最早的页 性能好，科学合理 实现复杂，需要特殊地硬件支持，开销大 最不常用页面置换算法（NFU） 计数器记录R的和来表示被访问频率，每次淘汰访问频率小的页面 实现简单 之前访问频率大但是最近不怎么访问的页面迟迟不能被置换 老化算法 NFU改良版 实现适中，性能适中 ———— 工作集算法（WS） 每次都淘汰不在工作集的或者在工作集时间最长的干净的页面 实现适中，性能适中 ———— 页面分配策略 页面分配、置换策略 驻留集 指请求分页存储管理中给进程分配的物理块的集合。在采用了虚拟存储技术的系统中，驻留集大小一般小于进程的总大小。如果驻留集太大，就失去了虚拟存储技术的应用意义，导致多道程序并发度下降，资源利用率降低。如果驻留集太小，会导致缺页颠簸，系统需要花费大量时间处理缺页。所以驻留集的大小要合适。 我们考虑一个极端的情况，如果一个进程共有100个页，那么如果驻留集大小为100，那么进程可以全部放入内存运行期间也就不会再发生缺页了，如果驻留集为1，则进程运行期间必定会频繁的缺页。 页面分配策略 固定分配：操作系统为每个进程分配一组固定数目的物理块，在运行期间各个进程的驻留集大小不变。 可变分配：先为每个进程分配一定数目的物理块，在进程运行期间，可根据情况做适当的增加或减少。即驻留集大小动态变化。 页面置换策略 局部置换：发生缺页时只能选进程自己的物理块进行置换。 全局置换：可以将操作系统保留的空闲物理块分配给缺页进程，也可以将别的进程持有的物理块置换到外存，再分配给缺页进程。 思考：分配策略和置换策略的关系？ 固定分配局部置换：系统为每个进程分配一定数量的物理块，在整个运行期间都不改变。若进程在运行中发生缺页，则只能从该进程在内存中的页面中选出一页换出，然后再调入需要的页面。这种策略缺点是很难在刚开始就确定应该为每个进程分配多少个物理块才算合理。（采用这种策略的系统可以根据进程大小，优先级，或是根据程序猿给出的参数来确定为一个进程分配的内存块数） 可变分配全局置换：刚开始为每个进程分配一定数量的物理块。操作系统会保持一个空闲物理块队列。当某个进程发生缺页时，从空闲物理块中取出一块分配给该进程，如果已经没有空闲物理块了，则可以选择一个未锁定的页面换出外存（注意，并不是所有的页面都可以换出外存，比如系统会锁定一些页面，这些页面中的内容不能置出外存比如重要的内核数据等），再将物理块分配给缺页的进程。如果采取这种策略，那么只要进程发生缺页，都将先获得空闲的物理块，只有空闲物理块也没有的时候系统会调出一些其他进程未锁定的页面（这个页可能是任何一个进程的页），然后将腾出的物理块分配给这个缺页的进程。因此这个被选中的进程拥有的物理块会减少，缺页率会增加。 可变分配局部置换：刚开始会为每个进程分配一定数量的物理块。当某进程发生缺页时，只允许从该进程自己的物理块中选出一个进行换出外存。如果进程在运行中频繁地缺页，系统会为该进程多分配几个物理块，直至该进程缺页率趋势适当程度；反之，如果进程在运行中缺页率特别低，则可适当减少分配给该进程的物理块。 思考：可变分配全局置换和可变分配局部置换的区别？ 可变分配全局置换是只要缺页系统就会给他分配新的物理块。 可变分配局部置换是根据发生缺页的频率动态增加或减少进程的物理块直至频率趋于稳定。 调入页面的时机 预调页策略 根据局部性原理（主要是空间局部性原理），一次调入若干个相邻的页面可能比一次调入一个页面更加高效。但是如果预先调入的页面大多数没有被访问，那么就会低效。因此可以预测不久之后可能访问到的页面，将他们预先调入内存，但是目前预测成功概率为50%。所以这种策略主要用于进程的首次调入，由程序猿指出应该调入那些部分。 请求调页策略 进程在运行期间发现缺页时才将页面调入内存。这种策略调入的页面一定会被访问，但是每次只能调入一页，而且每次调入都要磁盘I/O操作，所以开销大。 调入页面的区域 当系统拥有足够的对换区空间： 那么页面的调入和调出都是内存和对换区之间进行，这样可以保证页面的调入和调出速度很快，在进程运行前，需要将进程相关的数据从文件区复制到对换区。 当系统缺少足够的对换区空间： 凡是不会被修改的数据都直接从文件区调入，由于这些页面不会被修改，因此换出时不必写回磁盘，下次需要时再从文件区调入即可。对于可能被修改的 部分，换出时需写回磁盘对换区，下次需要时再从对换区调入。 独特的UNIX方式： 运行之前进程有关的数据全部放在文件区，故未使用过的页面，都可从文件区调入。若被使用过的页面需要换出，则写回对换区，下次需要时从对换区调入。 颠簸(抖动)现象 刚刚换出的页面马上又换入内存，刚刚换入的内存又要换出内存，这种频繁的页面调度行为就是颠簸或抖动。产生的原因是划分给进程的驻留集太小。 工作集 驻留集：在请求分页存储管理中给进程分配的物理块的集合。 工作集：在某段时间内，进程实际访问页面的集合。 所以工作集大小可能会小于窗口尺寸，系统会根据工作集大小和窗口尺寸的关系动态更改驻留集。比如某个进程的窗口尺寸为5，但是一段时间的检测发现进程的工作集一般最大就是3，那么物理块大小更改为3即可满足需要。所以一般驻留集的大小不能小于工作集的大小，否则就会导致进程运行过程中频繁缺页。 总结"},{"title":"缓冲区&结束语","path":"/wiki/操作系统笔记/缓冲区&结束语/index.html","content":"缓冲区管理 这节接上一张仍然是核心子系统的功能实现，本节是缓冲区管理。 什么是缓冲区 缓冲区我们并不陌生，机组原理中讲过的cache还有操作系统中讲述的高速缓冲tlb都是以中国缓冲区，他们都是一个存储区域，可以由专门的硬件寄存器组成，也可以利用内存作为缓冲区。使用硬件作为缓冲区的成本较高，容量也较小，一般仅用于对速度要求非常高的场合（如存储器管理中的联想寄存器TLB,由于对页表的访问频繁，因此使用速度很快的联想寄存器来存放页表项的副本）。一般情况下，更过的是利用内存部分空间作为缓冲区，“设备独立性软件的缓冲区管理就是要组织管理好这些缓冲区。 这里我们将详细讲述“内存作为缓冲区”的知识点。首先我们先要了解一下缓冲区的作用： 缓和cpu和I/O设备之间速度不匹配的矛盾。 减少对cpu的中断频率，放宽对cpu中断响应时间的限制。 解决数据颗粒度不匹配的问题，例如输出进程每次可以生成一块数据，但是I/O设备每次只能输出一个字符。 提高cpu和I/O设备之间的并行性。 这里我们介绍几种缓冲区管理策略 单缓冲 假设某用户进程请求某种块设备读入若干块的数据，如果采用单缓冲的策略，操作系统会在主存中为其分配一个缓冲区（如果题目中没有特别说明，一个缓冲区的大小就是一个块）。 此时当缓冲区数据非空时，不能往缓冲区冲入数据，只能从缓冲区把数据传出，当缓冲区为空时，可以往缓冲区冲入数据，但必须把缓冲区充满以后，才可以把缓冲区数据传出。其实特别类似于管道机制 如上图，块设备数据-&gt;缓冲区用时短于cpu处理数据的时间，因此读入的时间更快，那么一段时间后缓冲区就会被充满数据，此时就不能再继续输入数据了，需要等待cpu一直工作到缓冲区为空时才可以继续块设备数据-&gt;缓冲区。所以处理一块数据的平均用时=C+M。 当T&gt;C时，那么cpu处理速度更快，反而不会是的缓冲区被充满，所以此时处理一块数据的平均用时=T+M。所以无论是哪种情况，永远是取速度慢的，所以采用单缓冲策略时，处理一块数据平均耗时Mx(C,T)+M。 双缓冲 假设某用户进程请求某块设备读入若干块的数据。如果采用双缓冲的策略，操作系统会在主存中为其分配两个缓冲区（如果题目中没有说明，一个缓冲区的大小就是一块）。那么此时设备对于将数据写入缓冲区1,2的速度相同并且缓冲区-&gt;工作区的速度相同的。那么此时假设初始状态为：工作区空，其中的一个缓冲区满，另一个缓冲区空。 那么如果T&gt;M+C,此时即设备将数据填满空数据区2时，另一个数据区1已经全部移到工作区并且被cpu处理完了，那么每次都是设备-&gt;缓冲区的速度慢，所以处理一块数据的平均时间就是T。 如果此时T&lt;M=c，那么也就是当满缓冲区1数据移到工作区且被cpu处理完之前，另一个空的缓冲区2已经被填满了，那么此时处理一块数据的平均时间就是M+C。并且在双缓冲策略中，我们发现缓冲区1和缓冲区2是交替进行两个任务：①空的时候就是被设备数据填充②满的时候就是将数据转移到工作区。相应频率是相同的，不会出现一个缓冲区一直空，一个缓冲区一直满的情况，因为两个任务并行进行。所以在双缓冲策略中，处理一个数据块的平均时间为Max(T,M+C)。 使用单/双缓冲在通信时的区别 两台机器之间，可以配置缓冲区用于数据的发送和接受。 当采用单缓冲的时候，显然两个相互通信的机器只设置单缓冲区，那么在任一时刻只能实现数据的单向传输。显然效率并不高。所以一般使用双缓冲策略比较好： 此时两个相互通信的机器设置双缓冲时，则同一时刻可以实现双向的数据传输。我们对比发现实际上管道通信就是一种利用单缓冲区的方法，所以一个管道通信只能实现单一方向的数据传输，而如果想实现双向数据传输，就必须建立两个管道。 循环缓冲区 将过个大小相等的缓冲区链接成一个循环队列。下图中，绿色表示空缓冲区，橙色代表已充满数据的缓冲区。 缓冲池 缓冲池由系统中的共用的缓冲区组成，这些缓冲区按使用状况可以分为：空缓冲队列，装满输入数据的缓冲队列（输入队列），装满输出数据的缓冲队列（输出队列）。另外，根据一个缓冲区在实际运算中扮演的功能不同，又设置了四种工作缓冲区（全部都是以缓冲池的视角命名的）： 用于收容输入数据的工作缓冲区（hin）：存储的是要输入到用户进程的数据，但是要暂时存放到缓冲池，完成的是设备输入数据-&gt;缓冲区 用于提取输入数据的工作缓冲区（sin)：存储的是要输入到用户进程的数据，并且是要离开缓冲池，完成的是缓冲池的输入数据-&gt;用户进程 用于收容输出数据的工作缓冲区（hout)：存储的是要输出到设备的数据，但是要暂时存放到缓冲池，完成的是用户进程输出数据-&gt;缓冲区 用于提取输出数据的工作缓冲区（sout）：存储的是要输出到设备的数据，并且是要离开缓冲池，完成的是缓冲池的输出数据-&gt;设备 总结 结尾语 历时半个月，我终于完成了408–操作系统一周目的学习，20天的熬夜学习换来了丰富的回报，独自一人在图书馆中爆肝王道笔记的场景历历在目，相信经过这次学习更加坚定了长时间战线学习的信念👊，接下来敬请期待我的计算机组成原理学习笔记。–2021.1.18"},{"title":"什么是计算机图形学","path":"/wiki/GAMES191笔记/什么是图形学/index.html","content":"fff ggg"},{"title":"进程调度算法","path":"/wiki/操作系统笔记/进程调度算法/index.html","content":"调度算法 先来先服务（FCFS) 先来先服务算法（First Come First Serve)强调公平性，按照进程先来先服务的思想调度，在作业调度时，考虑的是那个作业先到达后备队列，用于进程调度的时候，考虑的是哪个进程先到达就绪队列。是一种非抢占式的算法，即不会有进程中途插队的情况出现。 例如各进程到达就绪队列的时间、需要的运行时间如下表所示，如果使用FCFS算法来调度进程，那么下列的各进程的等待时间，平均等待时间，周转时间，平均周转时间和带权周转时间与平均带权周转时间各是多少？ 首先周转时间=完成时间-到达时间，带权周转时间=周转时间/运行时间，等待时间=周转时间-运行时间。按照先来先服务的算法调度，那么就是根据到达的先后顺序调度，当然也就是等待时间越久（说明来的越早）的进程优先得到服务。所以调度的顺序就是P1-&gt;P2-&gt;P3-&gt;P4。如下图： P1先执行，即到达就先运行，运行7个时间单位，在P1运行途中P2，P3和P4实际上已经都到达了就绪队列了，但是P1执行完，P2等待时间肯定是最久的，所以他执行，然后P3,P4。所以周转时间=完成时间-到达时间可以算出各个进程的周转时间如下表： 进程 到达时间 完成时间 周转时间 1 0 7 7 2 2 11 9 3 4 12 8 4 5 16 11 然后计算带权周转时间如下表： 进程 周转时间 运行时间 带权周转时间 1 7 7 1 2 9 4 2.25 3 8 1 8 4 11 4 2.75 等待时间如下表: 进程 周转时间 运行时间 等待时间 1 7 7 0 2 9 4 5 3 8 1 7 4 11 4 7 所以平均周转时间=(7+8+9+11)/4=8.75，平均带权周转时间=(1+2.25+8+2.75)/4=3.5,平均等待时间=(0+5+7+7)/4=4.75，注意本题的进程都是纯计算的进程，一个进程到达要么在等待，要么在运行，如果是又有计算，又有I/O操作的进程，那么其等待时间就是周转时间-运行时间-I/O操作时间（本题不考虑这种情况）。我们通过上面的3个表可以看出当带权周转时间大的时候说明等待时间所占整个的周准事件比例也就越大，所以使用户的满意度也就降低了，并且这种FCFS算法很明显很不合理，对于某些运行时间非常短的且来的较晚的进程，如果其前面具有一个周转时间非常大的进程时，就会出现长时间的等待从而造成带权周转时间很大，从整体来看，也会影响到平均带权周转时间较大（当然FCFS不会造成平均周转时间大），并且这种算法当面对突发紧迫重要的进程任务时也不能及时处理，所以现代的操作系统是不采取这种调度算法的，这种算法对长作业有利，但是对于短作业不利，当然这种算法也不会造成饥饿现象。 思考：什么是饥饿？ 可以理解为排队买东西，老是有人中间插队造成后面的排队的人迟迟无法得到需求。进程亦是如此，当前面的进程总是出现插队现象，就会造成后面的进程一直长时间无法得到相应造成饥饿现象，当然FCFS虽然不合理，但是总是能得到服务的，所以不会造成饥饿现象，而接下来介绍的算法就有可能造成饥饿现象。 短作业优先（SJF） 短作业优先（Shortest Job Fiirst)追求最少的等待时间，最少的平均周转时间，最少的平均带权周转时间，既然FCFS会造成运行时间的短作业长时间等待，那么我就每次都让运行时间短的短作业先进行服务，长时间的大作业多等待一会也不会造成非常离谱的带权周转时间。这种短作业进程优先服务的进程调度算法也是非抢占式的算法，但是也有抢占式的算法例如最短剩余时间优先算法（SRTN,Shortest Remaining Time Next),但是SJF算法有可能会导致饥饿现象。 思考：SJF和SPF的区别？ SJF是对于作业调度（即高级调度）的短作业优先算法，所以叫Shortest Job First而SPF是对于进程调度（低级调度）的短作业优先算法，所以叫做Shortest Process First。 思考:什么是抢占式算法？什么是非抢占式算法？ 你可能会疑惑到SJF里可能会出现晚到但是先执行的情况出现，难道还不是抢占式算法？这里的抢占式是指某个任务在运行过程中还没有运行完时被剥夺cpu占用权，使另一个进程开始在cpu上运行。而SJF虽然会每次都挑选就绪队列中运行时间最短的短作业，但也是一定保证这个任务一次性执行完。所以SJF和FCFS都是非抢占式，而SRTN就是抢占式了，他每次都会实时监视计算那个任务有最短剩余时时间谁就上cpu及时前一个任务还没有执行完也要下cpu等待，当然当这个任务又可以上cpu时还是可以继续执行没完成的部分，不需要重新开始，即PCB会帮助记录状态信息以便恢复，一般SRTN又称作具有抢占式的短作业优先进程调度算法。 接下来，我们也计算一个SJF的题，为了更好的理解抢占式，我们做的是具有抢占式的短作业优先算法题：各进程到达就绪队列的时间、需要的运行时间如下表所示。使用抢占式的短作业优先调度算法， 计算各进程的等待时间、平均等待时间、周转时间、平均周转时间、带权周转时间、平均带权周转时间。 所以实际上做的是SRTN的算法题。 最短剩余时间算法：每当有进程加入到进程就绪队列时就需要进行调度即使现在还有任务在cpu上运行，如果新到达的进程剩余时间比当前运行的进程剩余时间更短，则新进程抢占cpu，当前运行进程在PCB记录相关信息后让出cpu重新回到就绪队列等待直至其又是最短剩余时间的作业时在上cpu。同时，当然运行任务结束后还要执行调度。 如下图： 需要注意的是，每次当有新进程到达时就绪队列都会改变，按照上述的规则进行检查。所以每次到达新进程时都要格外注意计算剩余时间。如上，在0-2时只有进程1，所以其先执行，但是当来到时刻2，插入一个新的作业2，他的剩余时间为4(因为还没有执行过，所以剩余运行时间=运行时间)，而此时作业1还有5的运行时间比作业2长，所以虽然作业1没有运行完也要下cpu,作业2抢占cpu。 所以可以用下表表示整个过程Pi(remain time): 0时刻（P1到达）：P1（7），7上cpu执行。 2时刻（P2到达）：P1（5），P2（4），2上cpu执行 4时刻（P3到达）：P1（5），P2（2），P3（1），3上cpu执行 5时刻（P3完成且P4到达）：P1（5），P2（2），P4（4），2上cpu执行 7时刻（P2完成）：P1（5），P4（4），4上cpu执行 11时刻（P4完成且只剩下1）：P1（5），1上cpu执行 16时刻，所有进程完成，调度算法结束。 我们同样计算一下周转时间： 进程 到达时间 完成时间 周转时间 1 0 16 16 2 2 7 5 3 4 5 1 4 5 11 6 带权周转时间： 进程 周转时间 运行时间 带权周转时间 1 16 7 2.28 2 5 4 1.25 3 1 1 1 4 6 4 1.5 等待时间： 进程 周转时间 运行时间 等待时间 1 16 7 9 2 5 4 1 3 1 1 0 4 6 4 2 所以平均周转时间=(16+5+1+6)/4=7,平均带权周转时间=(2.28+1.25+1+1.5)/4=1.50,平均等待时间=(9+1+0+2)/4=3。我们发现对于短作业优先，其平均的指标要明显低于FCFS算法，同时我们这里给出非抢占式的SJF算法的平均指标： 调度算法 平均周转时间 平均带权周转时间 平均等待时间 SJF 8 2.56 4 SRNT 7 1.5 3 我们会发现抢占式的短作业优先算法比非抢占式的短作业优先算法的平均指标更小，也就意味着平均性能更好。 注意：小细节 如果题目中并未特别说明，所提到的“短作业/进程优先算法”默认都是非抢占式的即SJF。 很多的教材上都会说“SJF调度算法的平均等待时间，平均周转时间”最少，严格来说，这个表述是错误的，不严谨的。之前的例子已经表明，最短剩余时间优先算法（即抢占式的短作业优先算法）还要更少。应该再加上“在所有进程同时可运行时，采用SJF调度算法的平均等待时间，平均周转时间最少”或者“在所有进程几乎同时到达时，采用SJF调度算法的平均等待时间和平均周转时间最少”。否则在判断题中如果未加上上面的条件，那么SRNT是平均等待时间，平均周转时间最少的。 平均等待时间最短未必平均带权周转时间最短，毕竟后者同时由运行时间和周转时间决定。但是一般来说平均等待时间和平均带权周转时间正相关，即平均等待时间较小一般平均带权周转时间也不会太大。 短作业优先调度平均等待时间和平均周转时间短显而易见，但是却不公平，这种进程调度算法对短作业有利，但是对长作业不利，很有可能造成饥饿现象，即在排队过程中总是出现短作业，这样就会一直插队造成长作业一直等待不能得到相应的饥饿现象。 思考：有没有一种较为中和的算法，不会产生过于极端的情况？ 仔细对比发现FCFS和SJF都是很大几率出现较为极端的情况的，前者会对短作业不友好且平均性能不好，后者对长作业不友好，虽然平均性能不错，但是却会造成饥饿现象，那么我们可以发明一种更平和的算法，牺牲部分平均性能，但是对于长短作业都有所考虑，且不会造成饥饿的算法–高响应比优先算法。 高响应比优先算法（HRRN） 高响应比优先算法（Highest Response Ratio Next)要综合考虑作业/进程的等待时间和要求服务的时间，这里我们首先需要引入一个新的概念–响应比 响应比=等待时间+要求服务时间/要求服务时间=等待时间/要求服务时间+1响应比=等待时间+要求服务时间/要求服务时间=等待时间/要求服务时间+1 响应比=等待时间+要求服务时间/要求服务时间=等待时间/要求服务时间+1 要求服务时间就是运行时间，所以可以看出响应比一定是&gt;=1的，自HRRN算法中每次调度时都会计算各个作业/进程的响应比并且选择响应比最高的作业/进程服务。仔细思考响应比我们会发现等待时间更长且运行时间更短的作业/进程会优先选择，这样就实现了既不会让短作业等待也太长时间也不会让短作业永远最先被服务，相应的，也就实现了不会让长作业等待太长时间，折中了SJF和FCFS的优点，并且这种算法也是非抢占式的，只有在该作业/进程运行完毕或者中途主动放弃时才会触发调度，才需要即需要计算响应比，很显然这种算法不会产生饥饿现象。 下面我们还是对于上面的那4个任务按照HRRN算法调度计算： 每次调度时我们都计算响应比，并选择响应比高的上cpu 0时刻：只有P1到达了就绪队列，P1上处理机 7时刻：P1完成，就绪队列中有P2，P3,P4，P2的响应比为((5+4)/4=2.25),P3的响应比为((3+1)/1=4),P4的响应比为((2+4)/4=1.5)，显然选择3，虽然2和4都是一样的运行时间，但是2等待时间更长响应比也就越高。 8时刻：P3完成，此时剩下P2和P4，刚刚就算过两个任务的运行时间一样，经过相同的等待时间，P2还是比P4等待的时间长，所以2上cpu（不信可以计算响应比）。 12时刻：P2完成，还剩下P4，4上cpu 16时刻：所有任务完成，调度算法结束。 我们同样计算一下平均性能，首先计算周转时间 进程 完成时间 到达时间 周转时间 1 7 0 7 2 12 2 10 3 8 4 4 4 16 5 11 带权周转时间 进程 周转时间 运行时间 带权周转时间 1 7 7 1 2 10 4 2.5 3 4 1 4 4 11 4 2.75 等待时间 进程 周转时间 运行时间 等待时间 1 7 7 0 2 10 4 6 3 4 1 3 4 11 4 7 所以平均周转时间=(7+10+4+11)/4=8,平均带权周转时间=(1+2.5+4+2.75)/4=2.56，平均等待时间=(0+6+3+7)/4=4 前三种算法的总结 首先我们对比一下平均性能 算法 平均周转时间 平均带权周转时间 平均等待时间 FCFS 8.75 3.5 4.75 SJF 8 2.56 4 SRNT 7 1.5 3 RHHN 8 2.56 4 我们发现FCFS确实平均性能有点拉胯，而SJF和SRNT虽然平均性能优秀但是饥饿现象导致也不太好，而RHHN不但没有饥饿现象，而且平均性能也较好甚至这题的情况下平均性能和SJF一样优秀，所以RHHN整体应该较为出色，但是每次都要计算响应比又加大了计算开销。 这几种算法主要关心的是对用户的公平性，平均周转时间和平均等待时间等平均性能的指标，但是并不关心响应时间，前面我们也提高到过平均性能一般是操作系统关心的，但是用户关心的是自己的任务能否更快完成，所以上面这几种方法对于用户来说交互性很糟糕，因此这三种算法一般适用于早期的批处理系统，当然FCFS现在也扮演着某些情况的重要角色。但是接下来我们在介绍几种更适合于交互式系统的调度算法。并且要注意上面的这几种算法对于高级调用和低级调用均可以采用。 时间片轮转（RR） 不陌生呀，前面介绍操作系统发展史时分时系统就是时候用的这个时间片从而大幅推进了系统的发展，那么接下来我们就详细了解一下时间片轮转调度算法。时间片轮转（RR,Round-Robin)公平的，轮流的为各个进程服务，让每一个进程在一定的时间间隔内都可以得到相应。按照各进程到达就绪队列的顺序，轮流的让各个进程执行一个时间片（如100ms）。若进程未能在一个时间片内执行完，则剥夺处理机（外中断），将进程重新放到就绪队列队尾重新排列。并且注意此时的时间片轮转只能适用于低级调度（进程调度），因为作业只有放入内存建立了相应的进程后才能分配给处理机时间片，所以高级调度不适用。很明显，RR是一种抢占式的进程调度算法，计时装备由时钟装置完成，到达一个时间片后，就由时钟中断来通知cpu时间已到。因为各个进程都会得到相应，所以不会造成饥饿现象。 例题：各进程到达就绪队列的时间、需要的运行时间如下表所示。使用时间片轮转调度算法，分析时间片大小分别为2,5时的进程情况。 时间片轮转算法轮流让就绪队列中的进程依次执行一个时间片（每次选择的都是排在就绪队列队头的进程）按照上表，就绪队列如下： 假设现在时间片为2那么 0时刻（P1(5)）：0时刻只有p1到达就绪队列，让P1上处理机运行一个时间片。 2时刻（P2(4)-&gt;P1(3）:2时刻P2到达就绪队列，P1运行完一个时间片，被剥夺处理机，重新放到队尾，此时2排在了队头，因此2上处理机（注意：此时P1由于运行完一个时间片刚下处理机，然后此时插进入了P2，那么默认他是排在刚刚完成的P1的前面即P2插入队尾紧接着P1插入队尾）。 4时刻（P1(3)-&gt;P3(1)-&gt;P2(2)）：4时刻，P3到达，先插到队尾，紧接着P2下处理机也插到队尾，此时又轮到P1上处理机。 5时刻（P3(1)-&gt;P2(2)-&gt;P4(6)）：5时刻，时间片还没结束，此时P4先任务插到末尾，由于一个时间片还没结束，所以此时1任务还在cpu上执行。 6时刻（P3(1)-&gt;P2(2)-&gt;P4(6)-&gt;P1(1)）：6时刻，P1时间片用完，下处理机，重新回到就绪队列的末尾，发生调度，3上处理机。 7时刻（P2(2)-&gt;P4(6)-&gt;P1(1)）：虽然P3的时间片还没用完，但是由于此时P3只需要一个时间，所以7时刻它运行完主动放弃了cpu，因此也发生调度，队头进程2上处理机。 9时刻（P4(6)-&gt;P1(1)）：进程2时间片用完，并且刚好运行结束，发生调度，P4上处理机。 11时刻（P1(1)-&gt;P4(4)）：P4时间片用完，重新回到就读队列队尾，队头任务1上处理机。 12时刻（P4(4)）:此时虽然时间片还有，但是1已运行完，主动放弃处理机，此时只剩下了P4，4上处理机。 14时刻（）：就绪队列为空，P4接着上cpu执行。 16时刻：所有进程运行结束，调度算法结束。 对于时间片5和上面类似，可以自己尝试。 这种RR算法更注重的是响应时间，因而不计算周转时间，一般来说，设计RR算法目的就是要让响应时间合适，即时间片要让切换进程的开销占比不超过10%。比如一个系统中有10个进程在并发执行，如果时间片为1s,则一个进程被相应的时间可能至少需要9s,也就是说用户在自己进程的时间片外通过键盘发出调试命令，可能需要等待9秒才能被系统响应（当然，如果实在自己的时间片内就会被立即响应）。这样时间片的大小也要制定合适，如果太大了，使得每一个任务都可以在 一个时间片内就完成，则时间片轮转调度算法就退化为FCFS算法了，并且会增大进程的响应时间。如果太小的话，进程调度、切换有时间代价（保存、恢复运行环境），因此如果时间片太小会导致花费大量的时间来处理进程切换，从而导致实际用于进程执行的时间比例也减小了。 优先级调度算法（PSA) 优先级调度算法（PSA,Priority Scheduling Algorithm)的提出就是为了适应随着现代计算机的发展，越来越多的应用场景需要根据任务的紧急程度来决定处理顺序的情况。每个作业/进程都有各自的优先级，调度时永远选择优先级最高的作业/进程，这种算法即可用于作业调度，也可用于进程调度，甚至，还会用于之后学习的I/O调度。PSA同时具有抢占式和非抢占式的两种情况，但是面对实际情况，一般的PSA都是抢占式的，非抢占式的实现简单，但是实际应用意义不太大。 思考:抢占式和非抢占式的PSA区别？ 非抢占式的PSA需要在进程主动放弃处理机时进行调度，仍然没能有效解决实现紧急重要任务的初衷问题，而抢占式就是可以在就绪队列变化时检查是否产生了更高的优先级的任务，则进行抢占式切换所以肯定也是外中断了。 我们先来看一下非抢占式的PSA：各进程到达就绪队列的时间，需要运行的时间，进程优先数如下表所示（优先数越大，优先级越高） 非抢占式PSA每次调度选择当前已经到达且优先级最高的进程，当前进程主动放弃处理机时发生调度。 0时刻（P1）：只有P1到达，P1上处理机。 7时刻（P2,P3,P4）：P1运行完成放弃处理机，其余的三个进程都已经到达，选择优先级最高的P3上处理机。 8时刻（P2,P4）：P3完成，P2,P4优先级相同，则等待时间更长的（更早到达就绪队列的）先上，所以P2上处理机。 12时刻（P4）：P2完成，就绪队列只剩下P4,P4上处理机。 16时刻（）：P4完成，没有任务了，算法结束。 同样的我们在采取抢占式的PSA对上面的进程表进行调度： 抢占式的PSA永远要保证运行着的是优先级最高的任务，如果新到的任务优先级比正在运行的优先级高，则抢占，如果相同，则仍然等待（毕竟人家先到的）。 0时刻（P1）：只有P1到达，P1上处理机。 2时刻（P2）：P2到达就绪队列，发现此时P2优先级更高，虽然P1还在运行，抢占，P2上处理机，P1回到就绪队列。 4时刻（P1,P3）：P3到达，优先级比P2还高，虽然P2还在运行，抢占，P3上处理机，P2回到就绪队列。 5时刻（P1,P2,P4）：P3完成了，主动释放处理机，同时，P4也到达，由于P2比P4更先进入就绪队列，所以2上处理机。 7时刻（P1,P4）：P2完成，就绪队列只剩下P1,P4且P4优先级高，P4上处理机。 11时刻（P1）：P4完成，P1上处理机。 16时刻（）：P1完成，所有进程均已运行完，算法结束。 并且在PSA中就绪队列未必就只有一个，可以按照不同的优先级来组织，另外，也可以吧优先级高的进程排在更靠近队头的位置（使用优先级队列）。并且我们又跟据优先级是否动态改变分为了静态优先级和动态优先级两种。 静态优先级：创建进程时优先级确定，之后不发生改变。动态优先级：创建进程时有一个初始值，之后会根据情况动态地调整优先级。 一般系统进程优先级是高于用户进程的，前台进程优先级高于后台进程，操作系统更偏好I/O型进程（或者称为I/O繁忙性进程），这样I/O设备和cpu可以并行工作（注意不是并发）。如果优先让I/O繁忙型进程优先运行的话，则越有可能让I/O设备尽早的投入工作，则资源利用率、系统吞吐量也就会得到提升。因此与I/O型进程相对立的就是计算型进程（或者称为cpu繁忙型进程）。 思考：为什么要存在动态优先级PSA？ 可以从追求公平，提升资源利用率等角度考虑，如果某进程在就绪队列中等待了很长的时间则可以适当的提高其优先级，如果某进程长时间的占用处理机运行了很长时间，则可适当的降低其优先级，如果一个进程频繁的进行I/O操作，则可适当的提升其优先级。并且仔细思考，对于静态优先级的PSA，如果每次就绪队列中都会出现新的优先级高于进程P的进程，那么P就会长时间无法得到相应，造成饥饿现象的出现，所以动态优先级的PSA也可有效避免饥饿现象。 在PSA算法中用优先级区分紧急程度，重要程度，适用于实时操作系统，可灵活的调整对各个作业/进程的偏好程度。缺点是对于静态优先级的PSA可能会造成饥饿。 思考：有没有更好的算法? 思考我们已经介绍过得算法貌似都有自己的优缺点，FCFS公平但平均性能不好，SJF短作业永远优先平均性能优秀但是容易饥饿，高响应RHHN比虽然这种了FCFS和SJF但是对于用户交互糟糕且计算开销大，时间片RR虽然各进程相应但是应急能力一般，优先级PSA灵活调整各种进程服务的机会但是静态优先级易饥饿动态优先级实现较为复杂，所以有没有一种更好的这种考虑以上所有算法优点的同时缺点又不是那么明显的算法？有–多级反馈队列调度算法。 多级反馈队列调度算法（MFQSA） 多级队列反馈队列调度算法（Multilevel Feedback Queue Scheduling Algorithm）综合了上面算法的优点，他是设置多级就绪队列，各级队列优先级从高到低，时间片从小到大，新进程到达时先进入第1级队列队尾，按照FCFS原则排队等待被分配时间片，若用完时间片进程还未结束，则进入下一个队列的队尾，在等待FCFS分配时间片，如果已经到达了最下级的队列还没执行完就重新放回该队列的队尾（所以永远是以非递增的顺序向低级队列插入），只有当第k级队列为空时，才会为k+1级队头的进程分配时间片用于进程调度。所以从整体来看，各个队列之间有静态PSA的特点，对于某一个队列里的进程又有RR的特点，同时从某个进程来看又有动态PSA和FCFS的特点，真实太妙了。当然这个算法也是抢占式的算法，即在k即队列的进程运行过程中(此时1~k-1级队列应该都已经为空)，若更上级的队列（1~k-1级)中又进入了一个新进程，则由于新进程处于优先级更高的队列中，因此新进程会抢占处理机，原来运行的进程放回k级队列的队尾。 例题：各进程到达就绪队列的时间、需要的运行时间如下表所示。使用MFQSA算法，分析运行情况。 0时刻：只有P1，P1上处理机运行一个时间片，下处理机。 1时刻：P2到达，P1还未能在一个时间1的情况下运行完，P1移动至第二队列，开始执行P2，P2上处理机。 2时刻：P2执行完一个时间片1，也没能执行完，所以也插到队列2，此时队列1空了，开始给队列2分发时间片。此时1先到的队列2，所以P1先执行，再次上cpu 4时刻：P1又执行完一个时间片2，此时还是没能执行完，插到队列3队尾，发现队列2还没空还有P2，P2上处理机。 5时刻：此时P2在cpu上执行了时间片2的一半，还没运行完，但是来了新任务3插入到了队列1末尾，此时队列1不是空的，抢占，2下cpu重新插回到队列2末尾。 6时刻：P3执行了一个时间片1执行完了，下cpu，不用在插入到队列2了，此时队列2的P2重新上cpu。 8时刻：P2又运行完了一个时间片2，此时P2完成，不需要在插到队列3了，此时队列1,2都空了，只剩下了队列3的P1，P1上cpu 12时刻：P1又执行完了一个时间片4，此时已经完成了7/8，还差1，所以重新插回到队列3的队尾，此时队列3只有P1，所以P1又上cpu 13时刻：P1运行完成，所有进程都结束，算法结束。 MFQSA对各类的进程都相对公平（FCFS的优点），每个进程到达都可以很快得到相应（RR的优点），短进程只用较少的时间就可以完成（SPF的优点），不必事先估计进程的运行时间（避免用户作假），可灵活的调整各类进程的偏好程度，比如cpu密集型进程，I/O密集型进程（拓展：MFQSA可将因I/O而阻塞的进程重新放回到原队列，这样I/O型进程就可以保持较高的优先级），唯一的缺点就是还是有可能造成饥饿现象的，但是概率不会像SPF那么大。 后三种算法的总结 比起早期的批处理操作系统来说，由于计算机的造价大幅下降，因此之后的交互式的操作系统（包括分时操作系统和实时操作系统等）更注重系统的响应时间、公平性和平衡性等指标。而这后面的这几种算法能较好的满足交互式系统的需求，因此这三种算法适用于交互式系统。（比如UNIX使用的就是多级反馈队列调度算法）。 总结 经过上面的6个算法的介绍，我们对各个算法都有了一定的了解，并且最好是记住英文缩写名字，因为考试有时候只给英文要对其有印象。前面三种要熟练掌握计算指标的方法，后面的三种方法要可以熟练表述运行过程并且了解优缺点，尤其是最后的MFQSA。这里我列出以下注意点总结希望你可以有所收获。 抢占!=饥饿，对于抢占式动态PSA算法不会造成饥饿。 RR和MFQSA不适用于作业调度（高级调度），因为时间片的原因必须进入内存分配成进程后才能实现。 等待时间最大!=带权周转时间最大，只是成正相关。 有可能造成饥饿的算法：SJF,SPF,SRTN,抢占式的静态PSA和MFQSA。 交互式糟糕的算法：FCFS,SJF,HRRN，交互式好的算法：RR,PSA,MFQSA"},{"title":"I/O管理概述","path":"/wiki/操作系统笔记/I/O管理概述/index.html","content":"I/O设备的基本概念与分类 接下来我们介绍以下I/O设备管理的知识，首先我们学习认识一下I/O设备 什么是I/O设备 顾名思义，就是输入/输出设备（Input/Output)。I/O设备可以将数据传入到计算机，或者可以接受计算机输出数据的外部设备，属于计算机的硬件部分。 这里的输入和输出都是以计算机的视角来看的。所以显示器是计算机线束输出数据所以为输出设备。在UNIX系统中将外部设备抽象为了一种特殊的文件，用户可以使用与文件操作系统相同的方式对外部设备进行操作。例如write操作就是向外部设备输出数据，read操作就是从外部设备读入数据。 I/O设备根据使用特性的分类 可以分为： 人机交互类外部设备（数据传输速度慢） 存储设备（数据传输速度快） 网络通信设备（数据传输速度介于两者之间） I/O设备根据传输速度分类 低速设备 中速设备 高速设备 I/O设备根据信息交换的单位分类 块设备（传输速率较高，可以寻址，即对他可以随机的读/写任意一块） 字符设备（传输速率慢，不可寻址，在输入输出时常采用中断驱动方式） 总结 I/O控制器 I/O控制器主要由机械部件和电子部件组成，这里我们依次介绍。 I/O设备的机械部件 I/O的机械部件主要用于执行具体的I/O操作，如鼠标/键盘的按钮，显示器的LED屏，移动硬盘的磁臂，磁盘盘面等。 而I/O的电子部件通常是一块插入主板扩充槽的印刷电路板。 I/O设备的电子部件（I/O控制器） CPU无法直接控制I/O设备的机械部件，因此I/O设备还要有一个电子部件作为CPU和I/O设备机械部件之间的桥梁，勇于实现CPU对设备的控制，这个电子部件就是I/O控制器，又称为设备控制器。CPU可以控制I/O控制器，所以可以通过I/O控制器来控制设备的机械部件。 I/O控制器主要有以下功能： 接受和识别CPU发出的命令（如CPU发来的read/write命令，I/O控制器中会有相应的控制寄存器来存放命令与参数） 向cpu报告设备的状态（I/O控制器有相应的状态寄存器，用于记录I/O设备的当前状态，1表示空闲，0表示忙碌，当然也有的控制器为1表示忙碌，0表示空闲，这个看厂商的设定） 数据交换（I/O控制器会设置相应的数据寄存器。输出时，数据寄存器用于暂存CPU发来的数据，之后再由控制器传达给IO设备。输入时，数据寄存器暂存设备发来的数据，之后CPU从数据寄存器中取走数据） 地址识别（类似于内存的地址，为了区分不同设备控制器中的各个寄存器，也需要给各个寄存器设置一个特定的寄存器，I/O控制器通过CPU提供的地址来判断CPU要读/写的是哪一个寄存器） I/O控制器的组成 所以①一个I/O可能会对应多个设备②数据寄存器，控制寄存器，状态寄存器等可能会有多个（例如每一个控制/状态寄存器对应一个具体的设备），且这些寄存器都要有相应的地址，才能方便CPU的操作。有的计算器会让这些寄存器占用内存地址的一部分，称为内存映像I/O，另外一些计算机则采用I/O专用地址即寄存器独立编址。 思考：两种寄存器地址组成形式有什么区别？ 所以我们可以看出内存映像I/O貌似性能更好。 总结 I/O控制方式 那么I/O控制器具体通过什么方法来控制I/O设备呢？我们也会有多种形式其中主要会影响到读/写操作的流程，CPU的干预频率，数据传送的单位，数据流向等问题。 程序直接控制 我们以读操作为例 ①CPU向控制器发出读数据的命令。于是I/O控制器设备启动并且状态寄存器设置为1（未就绪）然后开始做准备工作让输入设备准备输入数据同时控制器自身准备接受数据到数据寄存器 ②CPU轮询检查控制器的状态是否就绪，即CPU时刻准备与控制器进行工作 ③输入设备准备好数据后将数据传给控制器同时报告自身状态 ④控制器将输入的数据放到数据寄存器，并且将自身的状态更改为0（表示已就绪和CPU进行交换工作） ⑤CPU发现控制器设备已经就绪，那么就将数据寄存器中的数据读入到CPU的寄存器中同时把CPU寄存器中的内容放到内存以便进行数据交换 ⑥如果还要继续读入数据，那么CPU继续发出读的指令 CPU干预频率：这样的方式CPU的干预频率很频繁，I/O操作开始之前，完成之后需要CPU介入，并且等待I/O完成的过程中需要不断地轮询检查。 数据传送单位：每次读/写一个字 数据流向： 读操作（数据输入）：I/O设备-&gt;CPU(包括CPU寄存器)-&gt;内存 写操作（数据输出）：内存-&gt;CPU(包括CPU寄存器)-&gt;I/O设备 每个字的读/写都需要CPU的帮助 优点：实现简单，在读/写指令后加上循环检查的一系列指令即可。 缺点：CPU和I/O设备只能串行工作，CPU需要一直轮询检查，长期处于忙碌状态，CPU利用率低。 中断驱动方式 引入中断机制，由于I/O设备很慢，因此CPU发出读/写命令以后可以将等待I/O的进程阻塞，先切换到其他进程。当I/O设备完成后，控制器会向CPU发送一个中断信号，CPU检测到中断信号后保存当前进程的运行环境信息，然后转去执行中断处理程序来处理中断。处理中断的过程中，CPU从I/O控制器中读一个字的数据传送到CPU寄存器，再写入主存。接着，CPU恢复等待I/O的进程（或其他进程）的运行环境，然后继续执行。 这样就不是cpu主动一直询问控制器设备是否就绪了，而是当控制器就绪后主动告诉CPU。这里我们要注意： ①CPU会在每个指令周期的末尾检查中断 ②中断处理过程中需要保存，恢复进程的运行环境，这个过程是需要一定的时间开销的。可见，如果中断发生的频率也会降低系统性能。 CPU干预频率：每次I/O操作开始之前，完成之后需要CPU的介入。等待I/O完成的过程中CPU可以切换到别的进程执行。 数据传送单位：每次读/写一个字 数据的流向： 读操作（数据输入）：I/O设备-&gt;CPU(包括CPU寄存器)-&gt;内存 写操作（数据输出）：内存-&gt;CPU(包括CPU寄存器)-&gt;I/O设备 每个字的读/写都需要CPU的帮助 优点：和程序直接控制方式相比，CPU不用一直不停的轮询，CPU和I/O设备可以并行工作，CPU利用率得到明显的提升 缺点：每个字在I/O设备和内存之间的传输，都需要经过CPU。并且频繁的中断也会消耗较多的CPU时间。 DMA方式 与“中断驱动方式”相比，DMA（Direct Memory Access,直接存储器存取，主要用于块设备的I/O控制）有这样几个改进： 数据的传送是“块”，不再是一个字，一个字的传送 数据流向是设备直接放到内存，或者内存到设备，不再需要CPU的帮助 仅在传送一个或多个数据块的开始和结束时，才需要CPU干预 DMA控制器 所以这个方法需要DMA控制器来服务。DMA控制器结构如下： DR(Data Register,数据寄存器)：暂存从设备到内存，或者从设备到内存的数据 MAR(Memory Address Register,内存地址寄存器)：在输入时，MAR表示数据应该放到内存中的什么位置，输出时MAR表示要输出的数据放在内存中的什么位置 DC(Date Counter,数据计数器)：表示剩余要读/写的字节数 CR（Command Register,命令/状态寄存器)：用于存放CPU发来的I/O命令，或设备的状态信息。 CPU干预频率：仅在传送一个或多个数据块的开始和结束时，才需要CPU干预 传送数据的单位：每次读/写一个或多个块（注意每次读写的都是连续的多个块，且这些块读入内存后在内存中也必须是连续的） 数据流向（不需要CPU帮助）： 读操作（数据输入）：I/O设备-&gt;内存 写操作（数据输出）：内存-&gt;I/O设备 优点：数据时以“块”为单位，CPU介入频率进一步降低。数据的传输不在需要经过CPU在写入内存，数据效率高。CPU和I/O设备的并行性也进一步提升。 缺点：CPU每发出一条I/O指令，只能读/写一个或多个连续的数据块。如果要读/写多个离散的存储块，或者将数据分别写到不同的内存区域时，CPU要分别发出多条I/O指令，进行多次中断处理才能完成。 通道控制方式 通道：一种硬件，可以理解为“弱鸡版CPU”，也是可以识别并执行一系列通道指令 和CPU相比，通道可以执行的指令很单一，并且通道程序是放在主机内存中的，也就是说通道与CPU共享内存。 CPU干预频率：极低，通道会根据CPU的知识执行相应的通道程序，只有完成一组数据块的读/写操作后才需要发出中断信号，请求CPU干预 数据传送的单位：每次读/写一组数据块 数据的流向（在通道的控制下进行）： 读操作（数据输入）：I/O设备-&gt;内存 写操作（数据输出）：内存-&gt;I/O设备 优点：CPU,通道，I/O设备并行工作，资源利用率很高 缺点：实现复杂，需要专门的通道硬件支持 总结 控制方式 完成一次读/写的过程 CPU干预频率 每次I/O的数据传输单位 数据流向 程序直接控制方式 CPU发出I/O命令后需要不断轮询 极高 字 设备-&gt;CPU-&gt;内存内存-&gt;CPU-&gt;设备 中断驱动方式 CPU发出I/O命令后可以做其他事，本次I/O完成后设备控制器发出中断信号 高 字 设备-&gt;CPU-&gt;内存内存-&gt;CPU-&gt;设备 DMA方式 CPU发出I/O命令后可以做其他事，本次I/O完成后，DMA控制器发出中断信号 中 块 设备-&gt;内存内存-&gt;设备 通道控制方式 CPU发出I/O命令后可以做其他事，通道会执行通道程序以完成I/O，完成后通道向cpu发出中断信号 低 一组块 设备-&gt;内存内存-&gt;设备 每一个阶段的优点都是上一个阶段的最大缺点，总体来看，整个发展过程就是尽量减少CPU干预，把CPU从繁杂的I/O控制事务中解脱出来，以便更多的完成数据处理任务。"}]